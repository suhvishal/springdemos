<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Troubleshooting application performance with Red Hat OpenShift metrics, Part 3: Collecting runtime metrics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ncoqSuhXI4A/troubleshooting-application-performance-red-hat-openshift-metrics-part-3" /><author><name>Pavel Macik</name></author><id>1144a785-8b85-4a04-b813-02b2ca9fc72b</id><updated>2021-07-22T07:00:00Z</updated><published>2021-07-22T07:00:00Z</published><summary type="html">&lt;p&gt;This is the third article in series showing how to use metrics from &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; to reveal application performance problems. In &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1"&gt;Part 1&lt;/a&gt;, I explained the environment and requirements for our application, the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator&lt;/a&gt;. In &lt;a href="https://developers.redhat.com/articles/2021/07/15/troubleshooting-application-performance-red-hat-openshift-metrics-part-2-test"&gt;Part 2&lt;/a&gt;, I showed you how to set up a test environment in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; and introduced the test scenarios. Now, we can start to focus on the metrics themselves.&lt;/p&gt; &lt;h2&gt;Collecting runtime metrics with the OpenShift monitoring tool&lt;/h2&gt; &lt;p&gt;To see what is going on with Service Binding Operator and the OpenShift cluster under heavy load, it is important to collect metrics for the duration of the test. OpenShift's &lt;a href="https://docs.openshift.com/container-platform/4.7/monitoring/understanding-the-monitoring-stack.html" target="_blank"&gt;monitoring tool&lt;/a&gt;, a combination of &lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; and &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; with predefined, out-of-the-box metrics, can collect the necessary data for the lifespan of the cluster. At first, it seems an ideal solution to simply use that feature, let the monitoring stack gather the data normally, and collect it after the test is done. The problem is that all of the monitoring tools—the Prometheus and Graphana instances—are deployed on the same cluster as the application they're monitoring, including the OpenShift cluster's own resources. So, if the cluster goes down (which we expect to happen while stress testing), the monitoring subsystem goes down along with it, and all the gathered data is lost. Keep in mind that we are using a temporary development cluster that is terminated after about 10 hours anyway.&lt;/p&gt; &lt;p&gt;To ensure results are preserved on a node that won't crash, I've created the following collector script. It uses the &lt;a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/" target="_blank"&gt;OpenShift Client tool&lt;/a&gt; (&lt;code&gt;oc&lt;/code&gt;) to pull the runtime metrics every 30 seconds or so for the duration of the test and stores it in a set of simple CSV files, one for nodes and one for each monitored pod. I start the script in the background before the user provisioning starts in order to catch the "before" state, and leave the script running for some time after the load generation ends to see the long-term behavior of the watched resources:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;PERIOD="${1:-30}" RESULTS=${2:-metrics-$(date "+%F_%T")} mkdir -p "$RESULTS" strip_unit(){ echo -n $1 | sed -e 's,\([0-9]\+\)m,\1,g' | sed -e 's,\([0-9]\+\)Mi,\1,g' | sed -e 's,\([0-9]\+\)%,\1,g' } # Nodes oc get nodes &gt; $RESULTS/nodes.yaml oc describe nodes &gt; $RESULTS/nodes.info node_info_file(){ readlink -m "$RESULTS/node-info.$1.csv" } node_line(){ node=$1 node_json="$(oc get node $node -o json)" echo -n "$(echo "$node_json" | jq -rc '.status.conditions[] | select(.type=="MemoryPressure").status');" echo -n "$(echo "$node_json" | jq -rc '.status.conditions[] | select(.type=="DiskPressure").status');" echo -n "$(echo "$node_json" | jq -rc '.status.conditions[] | select(.type=="PIDPressure").status');" echo -n "$(echo "$node_json" | jq -rc '.status.conditions[] | select(.type=="Ready").status');" node_info=($(oc adm top node $node --no-headers)) echo -n "$(strip_unit ${node_info[1]});" echo -n "$(strip_unit ${node_info[2]});" echo -n "$(strip_unit ${node_info[3]});" echo "$(strip_unit ${node_info[4]})" } NODES=($(oc get nodes -o json | jq -rc '.items[].metadata.name' | sort)) for node in "${NODES[@]}"; do echo "Time;MemoryPressure;DiskPressure;PIDPressure;Ready;CPU_millicores;CPU_percent;Memory_MiB;Memory_percent" &gt; $(node_info_file $node) done # Operator pods pod_info_file(){ readlink -m "$RESULTS/pod-info.$1.csv" } pod_line(){ pod=$1 ns=$2 pod_info=($(oc adm top pod $pod -n $ns --no-headers)) echo -n "$(strip_unit ${pod_info[1]});" echo "$(strip_unit ${pod_info[2]})" } for namespace in openshift-operators openshift-monitoring openshift-apiserver openshift-kube-apiserver openshift-sdn openshift-operator-lifecycle-manager service-binding-operator; do PODS=($(oc get pods -n $namespace -o json | jq -rc '.items[].metadata.name' | grep -E 'operator|prometheus|apiserver|sdn|ovs|olm|packageserver' | sort)) for pod in "${PODS[@]}"; do echo "Time;CPU_millicores;Memory_MiB" &gt; $(pod_info_file $pod) done done echo "Collecting metrics" # Periodical collection while true; do echo -n "." for namespace in openshift-operators openshift-monitoring openshift-apiserver openshift-kube-apiserver openshift-sdn openshift-operator-lifecycle-manager service-binding-operator; do PODS=($(oc get pods -n $namespace -o json | jq -rc '.items[].metadata.name' | grep -E 'operator|prometheus|apiserver|sdn|ovs|olm|packageserver' | sort)) for pod in "${PODS[@]}"; do pod_file=$(pod_info_file $pod) echo -n "$(date -u '+%F %T.%N');" &gt;&gt; $pod_file pod_line $pod $namespace &gt;&gt; $pod_file done done for node in ${NODES[@]}; do node_file=$(node_info_file $node) echo -n "$(date -u '+%F %T.%N');" &gt;&gt; $node_file node_line $node &gt;&gt; $node_file done sleep ${PERIOD}s done &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the cluster survives the entire duration of the stress test, we can use Grafana to download the collected data from Prometheus in a form of similar CSV files.&lt;/p&gt; &lt;h2&gt;Compiling a performance report&lt;/h2&gt; &lt;p&gt;Since these tests were arranged pretty quickly and the metrics were collected in the raw form of CSV files, I had to manually convert the data into charts to put them into perspective. I used &lt;a href="https://www.google.com/sheets/about/"&gt;Google Sheets&lt;/a&gt; for that purpose.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;Finally, we have all the infrastructure we need for performance testing. In Part 4, we will take our first look at the actual metrics.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3" title="Troubleshooting application performance with Red Hat OpenShift metrics, Part 3: Collecting runtime metrics"&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 3: Collecting runtime metrics&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ncoqSuhXI4A" height="1" width="1" alt=""/&gt;</summary><dc:creator>Pavel Macik</dc:creator><dc:date>2021-07-22T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3</feedburner:origLink></entry><entry><title type="html">Quarkus 2.0.3.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XyM3cPhK_3w/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-0-3-final-released/</id><updated>2021-07-22T00:00:00Z</updated><content type="html">We just released Quarkus 2.0.3.Final with a new round of bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.0. If you are not using 2.0 already, please refer to the 2.0 migration guide. Full changelog You can get the full changelog of 2.0.3.Final on GitHub....&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XyM3cPhK_3w" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-0-3-final-released/</feedburner:origLink></entry><entry><title>Red Hat JBoss Enterprise Application Platform 7.4 brings new developer and operations capabilities</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/yEw64ms3fXw/red-hat-jboss-enterprise-application-platform-74-brings-new-developer-and" /><author><name>James Falkner</name></author><id>21cdd3d4-e045-4d3a-9126-26349ea57f80</id><updated>2021-07-21T19:00:00Z</updated><published>2021-07-21T19:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) 7.4 is now in general availability (GA). JBoss EAP is an open source, &lt;a href="https://jakarta.ee/compatibility/"&gt;Jakarta Enterprise Edition (Jakarta EE) 8-compliant&lt;/a&gt; application server that enables organizations to deploy and manage enterprise &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; applications across hybrid IT environments, including bare-metal, virtualized, private, and public clouds. This release provides enhancements to operations on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; as well as several new improvements in &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt;, management, and developer productivity.&lt;/p&gt; &lt;p&gt;This article covers what's new in the JBoss EAP 7.4 GA. With this release, Red Hat continues its commitment to Jakarta EE support and enabling developers to extend existing application investments as they transition to emerging architectures and programming paradigms that require a lightweight, highly modular, cloud-native platform.&lt;/p&gt; &lt;h2&gt;Self-signed certificates and runtime data&lt;/h2&gt; &lt;p&gt;This release features the ability to automatically generate self-signed certificates, which makes it easier for developers to verify the security capabilities of their applications before moving to production.&lt;/p&gt; &lt;p&gt;JBoss EAP 7.4 also publishes runtime metrics for managed executor services for developers using managed threads that want to observe the state of the service and threads it is managing. Developers using Jakarta Enterprise Beans (stateful session beans, stateless session beans, and singleton beans) can now access runtime data about their beans using the JBoss CLI. These beans can also be dynamically discovered over HTTP as part of this release. They can optionally use globally-defined compression on both clients (using the &lt;code&gt;default.compression&lt;/code&gt; property) and servers (using the &lt;code&gt;default-compression&lt;/code&gt; attribute in the server configuration).&lt;/p&gt; &lt;h2&gt;Elytron enhancements and TLS support&lt;/h2&gt; &lt;p&gt;Security is perhaps the most important consideration for any organization. The Elytron subsystem, &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.1/html/migration_guide/migrating_to_elytron"&gt;introduced&lt;/a&gt; in JBoss EAP 7.1, has been enhanced in this release with support for auto-updating of credentials, reducing the number of steps to add new credentials to existing stores. In addition, administrators can now use regular expressions to translate security role names (avoiding the creation and maintenance of custom translators).&lt;/p&gt; &lt;p&gt;JBoss EAP 7.4 also now supports Transport Layer Security (TLS) 1.3. It is disabled by default in this release, but easily enabled via the &lt;code&gt;cipher-suite-names&lt;/code&gt; attribute. For websites that use cookies, the Undertow subsystem now supports the use of &lt;a href="https://datatracker.ietf.org/doc/html/draft-ietf-httpbis-cookie-same-site-00"&gt;SameSite cookie&lt;/a&gt; handling via the &lt;code&gt;samesite-cookie&lt;/code&gt; handler declaration.&lt;/p&gt; &lt;p&gt;Finally, administrators can now use Git repositories to store and manage their server configuration, properties files, and deployments.&lt;/p&gt; &lt;p&gt;These are just a few of the many new security features of JBoss EAP 7.4. For more details, consult the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Management capabilities&lt;/h2&gt; &lt;p&gt;JBoss EAP solves many developers' hardest challenges, in large part due to its very flexible configuration and management capabilities. In this new release, administrators can now use global directories to distribute shared libraries, saving many steps when deploying new libraries across multiple applications. To control configuration drift, administrators can now declare the configuration directory as read-only to prevent unintended updates, which is especially useful in immutable &lt;a href="https://developers.redhat.com/topics/containers/"&gt;container&lt;/a&gt; deployments.&lt;/p&gt; &lt;p&gt;Finally, the management console (and the JBoss command-line interface) gains the ability to add new role decoders to the Elytron subsystem. Using this decoder, remote clients making calls to JBoss EAP can be mapped to Elytron roles based on the source address or regular expression and subsequently used during authorization decisions.&lt;/p&gt; &lt;h2&gt;ActiveMQ Artemis messaging capabilities&lt;/h2&gt; &lt;p&gt;Developers using the &lt;a href="https://activemq.apache.org/components/artemis/"&gt;Apache ActiveMQ Artemis&lt;/a&gt; messaging capabilities included in JBoss EAP 7.4 can now pause message topics (in addition to queues). This allows messages to be received into the queue or topic but held for delivery until the topic or queue is resumed, which is useful for testing failure scenarios or other maintenance tasks. JBoss EAP 7.4 also features a configurable list of hosts to periodically ping to detect broker network isolation, improving the resiliency of messaging applications.&lt;/p&gt; &lt;h2&gt;Support for Red Hat OpenShift and Galleon feature packs&lt;/h2&gt; &lt;p&gt;JBoss EAP 7.4 container images are available via the &lt;a href="https://catalog.redhat.com/"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;, which can be used to create and deploy JBoss EAP 7.4 applications on Red Hat OpenShift. With this new release, the Source-to-Image (S2I) builder images now support custom &lt;a href="https://developers.redhat.com/blog/2020/04/10/jboss-eap-7-3-brings-new-packaging-capabilities"&gt;Galleon&lt;/a&gt; feature pack configurations to specify the location of custom content and which feature packs to use when building custom images. A new Galleon layer, &lt;code&gt;web-passivation&lt;/code&gt;, is included to automatically include a local cache (based on &lt;a href="https://infinispan.org/"&gt;Infinispan&lt;/a&gt;) for data session handling.&lt;/p&gt; &lt;p&gt;Additionally, Red Hat OpenShift users can use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html/getting_started_with_jboss_eap_for_openshift_online/eap-operator-for-automating-application-deployment-on-openshift_default"&gt;JBoss EAP Operator&lt;/a&gt; to deploy and manage JBoss EAP applications using operator semantics.&lt;/p&gt; &lt;h2&gt;MicroProfile 4.0 support with JBoss EAP expansion pack&lt;/h2&gt; &lt;p&gt;With the &lt;a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/"&gt;JBoss Enterprise Application Platform expansion pack&lt;/a&gt; (EAP XP) 3.0 release, developers can use &lt;a href="https://microprofile.io/"&gt;Eclipse MicroProfile&lt;/a&gt; APIs to build and deploy microservices-based applications. This new JBoss EAP XP release brings support for the MicroProfile 4.0 specification, including bulk configuration property support, the ability to define cross-invocation life cycles for circuit breakers and bulkheads, and support for adding tags to individual metrics, which enhances the observability of JBoss EAP XP applications. JBoss EAP XP 3 also lets you specify runtime configuration via the &lt;code&gt;--cli-script&lt;/code&gt; argument when launching applications, which runs a custom script at startup.&lt;/p&gt; &lt;p&gt;The new release also includes a developer preview of &lt;a href="https://github.com/eclipse/microprofile-reactive-messaging"&gt; MicroProfile Reactive Messaging&lt;/a&gt;, allowing developers to integrate with &lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt; Red Hat AMQ Streams&lt;/a&gt; and work as a message relayer to consume, process, and produce messages. Note that the MicroProfile APIs that were included in earlier versions of JBoss EAP 7 have been moved to EAP XP. To use these MicroProfile APIs, developers should use JBoss EAP XP (and be aware of its &lt;a href="https://access.redhat.com/support/policy/updates/jboss_eap_xp_notes"&gt;different support life cycle&lt;/a&gt;). The JBoss EAP XP 3 release is forthcoming.&lt;/p&gt; &lt;h2&gt;Migration tooling updates&lt;/h2&gt; &lt;p&gt;As with any new JBoss EAP release, the popular &lt;a href="https://developers.redhat.com/products/mta/overview"&gt;Migration Toolkit for Applications&lt;/a&gt; (based on the &lt;a href="https://github.com/windup"&gt;Windup project&lt;/a&gt;) gets new updates in its upcoming 5.2 release, including new rules for JBoss EAP 7.3 to 7.4 and JBoss EAP XP 2 to XP 3, a new IntelliJ IDEA extension, and assistance for &lt;a href="https://access.redhat.com/products/thorntail"&gt;Thorntail&lt;/a&gt; users looking to migrate to JBoss EAP XP.&lt;/p&gt; &lt;h2&gt;Supported platforms&lt;/h2&gt; &lt;p&gt;As always, the list of supported platforms continues to evolve. In this new 7.4 release, several platforms and features have been deprecated in favor of better alternatives. For the full list of new, changed, or deprecated platforms and features, please refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/"&gt;release notes&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;JBoss EAP on Microsoft Azure&lt;/h2&gt; &lt;p&gt;Microsoft and Red Hat &lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-brings-jboss-enterprise-application-platform-microsoft-azure-easing-shift-cloud-traditional-java-applications"&gt;recently announced&lt;/a&gt; the availability of Red Hat JBoss Enterprise Application Platform as a native offering in Microsoft Azure, both for traditional VM-based deployments and a &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/application-platform/azure"&gt;managed offering through Azure App Service&lt;/a&gt;. Both of these offerings currently offer JBoss EAP 7.3, and work is underway to update these to use the new 7.4 release.&lt;/p&gt; &lt;h2&gt;Red Hat Runtimes&lt;/h2&gt; &lt;p&gt;JBoss EAP is included in &lt;a href="https://www.redhat.com/en/products/runtimes"&gt;Red Hat Runtimes&lt;/a&gt;, a Red Hat Application Services product that includes a set of cloud-native runtimes and capabilities such as the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Red Hat build of Quarkus&lt;/a&gt;, Red Hat support of &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt;, Red Hat build of &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, Red Hat's single sign-on technology, &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; broker, and &lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt;. These are all integrated and optimized for Red Hat OpenShift, offering a coherent hybrid cloud application platform on which they can optimize their existing Java applications while innovating with enterprise Java and non-Java &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;, DevOps, &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt;, and advanced deployment techniques. Check out the &lt;a href="https://access.redhat.com/support/policy/updates/jboss_notes#p_eap"&gt;JBoss EAP support life cycle&lt;/a&gt; for details on support levels and dates.&lt;/p&gt; &lt;h2&gt;JBoss EAP resources&lt;/h2&gt; &lt;p&gt;You can find the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/"&gt;JBoss Enterprise Application Platform 7.4 documentation&lt;/a&gt;, including release notes, the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html/getting_started_guide/"&gt;Getting Started Guide&lt;/a&gt;, and several other guides on &lt;a href="https://docs.redhat.com"&gt; docs.redhat.com&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/21/red-hat-jboss-enterprise-application-platform-74-brings-new-developer-and" title="Red Hat JBoss Enterprise Application Platform 7.4 brings new developer and operations capabilities"&gt;Red Hat JBoss Enterprise Application Platform 7.4 brings new developer and operations capabilities&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/yEw64ms3fXw" height="1" width="1" alt=""/&gt;</summary><dc:creator>James Falkner</dc:creator><dc:date>2021-07-21T19:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/21/red-hat-jboss-enterprise-application-platform-74-brings-new-developer-and</feedburner:origLink></entry><entry><title>Bootstrap GitOps with Red Hat OpenShift Pipelines and kam CLI</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cjmU1pPrHqI/bootstrap-gitops-red-hat-openshift-pipelines-and-kam-cli" /><author><name>Ishita Sequeira, William Tam</name></author><id>3060b156-f201-4d00-816f-6cf5753afe71</id><updated>2021-07-21T07:00:00Z</updated><published>2021-07-21T07:00:00Z</published><summary type="html">&lt;p&gt;The GitOps Application Manager command-line interface (CLI), &lt;code&gt;kam&lt;/code&gt;, simplifies &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; adoption by bootstrapping Git repositories with opinionated layouts for &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous delivery&lt;/a&gt;. It also configures Argo CD to sync configurations across multiple &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; environments. In this article, we show you how to generate a GitOps repository using the &lt;code&gt;kam&lt;/code&gt; CLI to streamline your application delivery.&lt;/p&gt; &lt;h2&gt;A GitOps approach to application deployment&lt;/h2&gt; &lt;p&gt;Developers can adopt a GitOps approach to managing deployments into one or more environments. An &lt;em&gt;environment&lt;/em&gt; is a namespace in a Kubernetes cluster that is used to perform quality, &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt;, and performance checks. To ensure quality and expected behavior, teams deploy applications across various environments like development, testing, and staging, before running on the production cluster.&lt;/p&gt; &lt;p&gt;Currently, the &lt;code&gt;kam&lt;/code&gt; CLI bootstrap command supports three fixed environments: &lt;code&gt;dev&lt;/code&gt;, &lt;code&gt;stage&lt;/code&gt;, and &lt;code&gt;cicd&lt;/code&gt;. The &lt;code&gt;cicd&lt;/code&gt; environment is where we keep our &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton&lt;/a&gt; pipelines and tasks. The commands enable the user to deploy the application to only two environments: &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;stage&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;One or more applications can be deployed into a given environment. An &lt;em&gt;application&lt;/em&gt; is an aggregation of one or more services. The source code for each service (or &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservice&lt;/a&gt;) is contained within a single Git repository. To minimize the risk of change after deployment, whether intended or not, we must maintain a reproducible, reliable, and auditable deployment process. The ultimate value of GitOps lies in its ability to simplify the way you manage infrastructure and application deployment, control change, and gain visibility into your environments.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/redhat-developer/kam"&gt;&lt;code&gt;kam&lt;/code&gt; CLI&lt;/a&gt; generates a GitOps repository that contains the Kubernetes YAML configuration files required to deploy one or more applications into an environment. We use &lt;a href="https://github.com/kubernetes-sigs/kustomize"&gt;Kustomize&lt;/a&gt; to customize and configure resources based on their dependencies.&lt;/p&gt; &lt;p&gt;In this tutorial, we'll show you how to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Bootstrap a GitOps repository.&lt;/li&gt; &lt;li&gt;Create two services in an environment and deploy them in a single application.&lt;/li&gt; &lt;li&gt;Trigger the pipeline by making a small change in the service repository.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To generate a GitOps repository using the &lt;code&gt;kam&lt;/code&gt; CLI, you need to have the &lt;a href="https://github.com/redhat-developer/kam/blob/4f3be956fadecec2367840d0079bf9be6d384483/docs/journey/day1/prerequisites/gitops_operator.md"&gt;OpenShift GitOps Operator&lt;/a&gt; and &lt;a href="https://github.com/redhat-developer/kam/blob/4f3be956fadecec2367840d0079bf9be6d384483/docs/journey/day1/prerequisites/pipelines_operator.md"&gt;OpenShift Pipelines Operator&lt;/a&gt; preinstalled on your OpenShift cluster. This tutorial also requires &lt;code&gt;kam&lt;/code&gt; version 0.0.36 or later. Download the &lt;code&gt;kam&lt;/code&gt; binary from &lt;a href="https://github.com/redhat-developer/kam/releases"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Create the GitOps repository with 'kam bootstrap'&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;kam bootstrap&lt;/code&gt; command generates the &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton and Kubernetes resources&lt;/a&gt; required to deploy an application across different environments. The command also generates event listeners that can trigger the pipelines on pull requests. You can apply these generated resources to a Kubernetes cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kam bootstrap --service-repo-url https://github.com/ishitasequeira/taxi.git --gitops-repo-url https://github.com/ishitasequeira/gitops.git --image-repo quay.io/isequeir/image-repo --dockercfgjson ~/Downloads/isequeir-robot-auth.json --git-host-access-token $TOKEN &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's break this down:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;--gitops-repo-url&lt;/code&gt; provides a Git repository containing the Kubernetes YAML configuration files required to deploy one or more applications into an environment.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--image-repo&lt;/code&gt; should specify either &lt;code&gt;registry/username/repository&lt;/code&gt; to use Red Hat Quay or Docker Hub, or &lt;code&gt;project/app&lt;/code&gt; to use the Red Hat OpenShift internal image registry.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--service-git-repo&lt;/code&gt; contains the service source code and the deployment path.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The command generates the repository layout and creates the required pipeline resources. The &lt;code&gt;cicd&lt;/code&gt; environment is also created within the layout. The &lt;code&gt;cicd&lt;/code&gt; environment is where we keep our &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton&lt;/a&gt; pipelines and tasks. Two pipelines are created in the &lt;code&gt;cicd&lt;/code&gt; environment: one for the GitOps repository and another for the applications that can be deployed in various environments.&lt;/p&gt; &lt;p&gt;An unencrypted secret is generated into the &lt;code&gt;secrets&lt;/code&gt; folder, which is a sibling of the folder that contains the generated repository layout. Deploying this GitOps configuration without encrypting the secrets is insecure and is not recommended. One way to encrypt the secrets is by using the &lt;a href="https://github.com/bitnami-labs/sealed-secrets"&gt;Sealed Secrets&lt;/a&gt; tool from Bitnami Labs. To encrypt and apply the generated secrets, follow the instructions linked &lt;a href="https://github.com/redhat-developer/kam/blob/4f3be956fadecec2367840d0079bf9be6d384483/docs/journey/day1/README.md#sealed-secrets"&gt;here&lt;/a&gt;. Make sure to apply the secrets (encrypted or unencrypted) to the cluster every time a secret is generated (when the &lt;code&gt;bootstrap&lt;/code&gt; or &lt;code&gt;service add&lt;/code&gt; command is executed).&lt;/p&gt; &lt;p&gt;Create a new GitHub or GitLab project to host the GitOps repository (for example, &lt;code&gt;https://github.com/ishitasequeira/gitops.git&lt;/code&gt;). Push the generated directory structure to the repository using the following commands (the directory structure is created in the current working directory):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git init . $ git add . $ git commit -m "Initial Commit" $ git remote add origin &lt;insert gitops repo&gt; $ git push -u origin main&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This initializes the GitOps repository and starts your journey to deploying applications via Git.&lt;/p&gt; &lt;p&gt;We'll now bring up our deployment infrastructure. You only need to do this once at the beginning; the configuration will be self-hosted after that:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -k config/argocd/&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Add services to the GitOps repository&lt;/h2&gt; &lt;p&gt;For this demo, we will use a &lt;a href="https://github.com/ishitasequeira/frontend-app"&gt;front-end service&lt;/a&gt; that deploys an Angular web application and a &lt;a href="https://github.com/ishitasequeira/backend-service"&gt;back-end service&lt;/a&gt; that deploys a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; REST API. The front-end service tries to connect to the back-end service via the REST APIs to fetch or update the data.&lt;/p&gt; &lt;p&gt;Let's add the services to our GitOps repository using the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kam service add --git-repo-url https://github.com/ishitasequeira/frontend-service.git --app-name full-stack --env-name stage --image-repo quay.io/isequeir/frontend-service --service-name frontend-service $ kam service add --git-repo-url https://github.com/ishitasequeira/backend-service.git --app-name full-stack --env-name stage --image-repo quay.io/isequeir/backend-service --service-name backend-service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To break it down:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;--git-repo-url&lt;/code&gt; contains the service source code and the deployment path.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--app-name&lt;/code&gt; is the name of the application folder that utilizes one or more services.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--env-name&lt;/code&gt; is the environment to which the service is being deployed.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For each service, an unencrypted secret will be generated into the &lt;code&gt;secrets&lt;/code&gt; folder. Make sure to apply these secrets to the cluster. To encrypt this secret, follow the same steps &lt;a href="https://github.com/redhat-developer/kam/tree/master/docs/journey/day1#secrets"&gt;here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt; Next, push the newly created services to the GitOps repository and sync the applications on the Argo CD user interface (UI).&lt;/p&gt; &lt;p&gt;At this point, the applications in Argo CD should be synced and healthy. You may need to manually &lt;a href="https://github.com/argoproj/argo-cd/blob/master/docs/getting_started.md#7-sync-deploy-the-application"&gt;sync applications&lt;/a&gt; from the Argo CD web UI if any of the applications are out of sync.&lt;/p&gt; &lt;p&gt;You can log into the Argo CD instance from the application launcher in the OpenShift console, as shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%203.39.28%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%203.39.28%20PM.png?itok=4U5m50Ql" width="1440" height="809" alt="A screenshot of the Applications menu showing where to log into the Argo CD instance." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Select &lt;strong&gt;Cluster ArgoCD&lt;/strong&gt; from the application launcher to log in to the Argo CD instance.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For the precreated Argo CD instance under the &lt;code&gt;openshift-gitops&lt;/code&gt; project, you'll find the password to log into the Argo CD UI by navigating to &lt;strong&gt;Workloads → Secrets&lt;/strong&gt; and selecting &lt;code&gt;openshift-gitops-cluster&lt;/code&gt;, as shown in Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-15%20at%2012.40.01%20AM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-15%20at%2012.40.01%20AM.png?itok=tLBAH9gW" width="1440" height="857" alt="A screenshot showing how to access the password to log into the Argo CD UI as described in the article." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Access the password to log into the Argo CD UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Copy the secret at the bottom of the &lt;strong&gt;Secrets details&lt;/strong&gt; page to your clipboard (see Figure 3) and paste it into the password field on the Argo CD login page. Use &lt;code&gt;admin&lt;/code&gt; as the username.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-15%20at%2012.43.35%20AM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-15%20at%2012.43.35%20AM.png?itok=_O4ihX8j" width="1440" height="853" alt="Screenshot of the Secret details page that shows where the user can copy the password." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Copying the password from the Secrets details page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Alternatively, you can fetch this password via the command line by running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl get secret openshift-gitops-cluster -n openshift-gitops -ojsonpath='{.data.admin\.password}' | base64 -d&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see in Figure 4, the applications have been synced.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.02.51%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.02.51%20PM.png?itok=4Lt0lzVs" width="1440" height="814" alt="Screenshot showing the list of apps deployed in the Argo CD UI." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The list of applications in the Argo CD UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;When you click the full-stack application (see Figure 5), there should be two services associated with it: the front-end service and the back-end service.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-15%20at%2012.49.40%20AM_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-15%20at%2012.49.40%20AM_0.png?itok=ou7W-PJ8" width="1440" height="756" alt="Screenshot of the full-stack application with the statuses Healthy and Synced." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Deploying the front-end and back-end services.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can launch the front-end service by navigating to &lt;strong&gt;stage project → Topology&lt;/strong&gt; and clicking the front-end service (see Figure 6).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.04.19%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.04.19%20PM.png?itok=Bt2E4Cg4" width="1440" height="758" alt="Accessing the front-end service from the Topology page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Topology view of services deployed in the project stage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Doing this creates a route, which will serve the default image. The route is automatically created based on the name of your application source repository. When you open the front-end service URL, you will see the web page, as shown in Figure 7.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.04.30%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.04.30%20PM.png?itok=A3nOFZ8D" width="1440" height="799" alt="Screenshot of the example web application home page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: The web application homepage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can access the list of products fetched from the back-end service using the REST API calls by clicking the &lt;strong&gt;Products&lt;/strong&gt; tab in the upper-right corner of the application (see Figure 8).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.04.43%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.04.43%20PM.png?itok=89LFZ8NY" width="1440" height="635" alt="Screenshot of the Product Inventory page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: Accessing the product database from the Products tab.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Add webhooks to the Git repository&lt;/h2&gt; &lt;p&gt;We need to add a webhook to the Git repository once the resources are applied. The webhook's payload URL is the &lt;code&gt;EventListener&lt;/code&gt; external address exposed by the OpenShift route. You can obtain the external address by displaying OpenShift routes in the &lt;code&gt;cicd&lt;/code&gt; namespace. Simply run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get route --namespace cicd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You also need a webhook for your Git hosting service. This is used to trigger continuous integration (CI) pipeline runs automatically on pushes to your repositories. The address (&lt;code&gt;host/port&lt;/code&gt;) retrieved from the command will be further added to the webhook secret of that repository. We can create webhooks for the added services and the GitOps repository.&lt;/p&gt; &lt;p&gt;For this demo, we will add a webhook to the front-end service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kam webhook create --env-name stage --service-name frontend-service --git-host-access-token $TOKEN&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Testing the pipeline&lt;/h2&gt; &lt;p&gt;After you add the webhook secret, trigger a small change in your front-end service repository to see if &lt;code&gt;app-ci-pipeline&lt;/code&gt; is working. For this example, we changed the content and the color of the homepage navigation bar in a new branch. Doing this created a pull request. To determine if the &lt;code&gt;app-ci-pipeline&lt;/code&gt; has been triggered, go to the Developer view on the OpenShift web console. The &lt;strong&gt;Pipelines&lt;/strong&gt; tab will indicate whether the pipelines are triggered and completed successfully (see Figure 9). Upon successfully completing the pipeline, a new image gets pushed to the Red Hat Quay repository that we specified while adding the service.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.04.54%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.04.54%20PM.png?itok=mWbL9o9T" width="1440" height="630" alt="Checking the status on the Pipelines detail page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: Checking the &lt;code&gt;app-ci-pipeline&lt;/code&gt; status.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For the changes to be reflected on the deployed application, we need to update the image tag for the front-end service in the &lt;code&gt;config&lt;/code&gt; folder. Once the image tag has been updated, we'll push the changes to the GitOps repository and delete the previous front-end service deployment. Doing so automatically updates the deployment with a new image, as shown in Figure 10.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.05.04%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.05.04%20PM.png?itok=lgfzihOT" width="1440" height="695" alt="Screenshot of the updated web application homepage." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: The updated web application homepage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article provided an overview of the CI/CD experience using the &lt;code&gt;kam&lt;/code&gt; CLI. To learn more about the &lt;code&gt;kam&lt;/code&gt; CLI, visit the &lt;a href="https://github.com/redhat-developer/kam"&gt;GitHub project&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/21/bootstrap-gitops-red-hat-openshift-pipelines-and-kam-cli" title="Bootstrap GitOps with Red Hat OpenShift Pipelines and kam CLI"&gt;Bootstrap GitOps with Red Hat OpenShift Pipelines and kam CLI&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cjmU1pPrHqI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ishita Sequeira, William Tam</dc:creator><dc:date>2021-07-21T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/21/bootstrap-gitops-red-hat-openshift-pipelines-and-kam-cli</feedburner:origLink></entry><entry><title type="html">How to use Long Running Actions between microservices</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6KP9A9oleNg/how-to-use-long-running-actions-between.html" /><author><name>Michael Musgrove</name></author><id>https://jbossts.blogspot.com/2021/07/how-to-use-long-running-actions-between.html</id><updated>2021-07-20T20:31:00Z</updated><content type="html">INTRODUCTION In my I showed how to run a Long Running Action (LRA) within a single JAX-RS resource method using quarkus features to build and run the application. I showed how to create and start an LRA coordinator and then generated a basic hello application, showing how to modify the application to run with a long running action (by adding dependencies on the org.eclipse.microprofile.lra:microprofile-lra-api and org.jboss.narayana.rts:narayana-lra artifacts, which together provide annotations for controlling the lifecycle of LRAs). That post also includes links to the and to the . In this follow up post I will indicate how to include a second resource in the LRA. To keep things interesting I’ll deploy the second resource to another microservice and use quarkus’s MicroProfile Rest Client support to implement the remote service invocations. The main difference between this example and the one I developed in the earlier post, apart from the technicalities of using Rest Client, is that we will set the attribute to false in the remote service so that the LRA will remain active when the call returns. In this way the initiating service method has the option of calling other microservices before ending the LRA. CREATING AND STARTING AN LRA COORDINATOR LRA relies on a coordinator to manage the lifecycle of LRAs so you will need one to be running for this demo to work successfully. The showed how to build and run coordinators. Alternatively, which execute all of the steps required in the current post and it includes a shell script called which will build a runnable coordinator jar (it’s fairly simple and short so you can just read it and create your own jar or just run it as is). GENERATE A PROJECT FOR BOOKING TICKETS Since the example will be REST based, include the resteasy and rest-client extensions (on line 6 next): 1: mvn io.quarkus:quarkus-maven-plugin:2.0.1.Final:create \ 2: -DprojectGroupId=org.acme \ 3: -DprojectArtifactId=ticket \ 4: -DclassName="org.acme.ticket.TicketResource" \ 5: -Dpath="/tickets" \ 6: -Dextensions="resteasy,rest-client" 7: cd ticket You will need the mvn program to run the plugin (but the generated projects will include the mvnw maven wrapper). Modify the generated TicketResource.java source file to add Microprofile LRA support. The changes that you will need for LRA are on lines 26 and 27. Line 26 says that the bookTicket method must run with an LRA (if one is not present when the method is invoked then one will be automatically created). Note that we have set the end attribute to false to stop the LRA from being automatically closed when the method finishes. By keeping the LRA active when the ticket is booked, the caller can invoke other services in the context of the same LRA. Most services will require the LRA context for tracking updates which typically will be useful for knowing which actions to compensate for if the LRA is later cancelled: the context is injected as a JAX-RS method parameter on line 27. You will also need to include callbacks for when the LRA is later closed or cancelled (the methods are defined on lines 37 and line 46, respectively). 1: package org.acme.ticket; 2: 3: import static javax.ws.rs.core.MediaType.APPLICATION_JSON; 4: 5: // import annotation definitions 6: import org.eclipse.microprofile.lra.annotation.ws.rs.LRA; 7: import org.eclipse.microprofile.lra.annotation.Compensate; 8: import org.eclipse.microprofile.lra.annotation.Complete; 9: // import the definition of the LRA context header 10: import static org.eclipse.microprofile.lra.annotation.ws.rs.LRA.LRA_HTTP_CONTEXT_HEADER; 11: 12: // import some JAX-RS types 13: import javax.ws.rs.GET; 14: import javax.ws.rs.PUT; 15: import javax.ws.rs.Path; 16: import javax.ws.rs.Produces; 17: import javax.ws.rs.core.Response; 18: import javax.ws.rs.HeaderParam; 19: 20: @Path("/tickets") 21: @Produces(APPLICATION_JSON) 22: public class TicketResource { 23: 24: @GET 25: @Path("/book") 26: @LRA(value = LRA.Type.REQUIRED, end = false) // an LRA will be started before method execution if none exists and will not be ended after method execution 27: public Response bookTicket(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 28: System.out.printf("TicketResource.bookTicket: %s%n", lraId); 29: String ticket = "1234" 30: return Response.ok(ticket).build(); 31: } 32: 33: // ask to be notified if the LRA closes: 34: @PUT // must be PUT 35: @Path("/complete") 36: @Complete 37: public Response completeWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 38: System.out.printf("TicketResource.completeWork: %s%n", lraId); 39: return Response.ok().build(); 40: } 41: 42: // ask to be notified if the LRA cancels: 43: @PUT // must be PUT 44: @Path("/compensate") 45: @Compensate 46: public Response compensateWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 47: System.out.printf("TicketResource.compensateWork: %s%n", lraId); 48: return Response.ok().build(); 49: } 50: } Skip the tests: rm src/test/java/org/acme/ticket/* Add dependencies on microprofile-lra-api and narayana-lra to the pom to include the MicroProfile LRA annotations and the narayana implementation of them so that the LRA context will be propagated during interservice communications: &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.microprofile.lra&lt;/groupId&gt; &lt;artifactId&gt;microprofile-lra-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.narayana.rts&lt;/groupId&gt; &lt;artifactId&gt;narayana-lra&lt;\/artifactId&gt; &lt;version&gt;5.12.0.Final&lt;/version&gt; &lt;/dependency&gt; We are creating ticket and trip microservices so they need to listen on different ports, configure the ticket service to run on port 8081: 1: quarkus.arc.exclude-types=io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantRegistry,io.narayana.lra.filter.ServerLRAFilter,io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantResource 2: quarkus.http.port=8081 3: quarkus.http.test-port=8081 The excludes are pulled in by the org.jboss.narayana.rts:narayana-lra maven dependency. As mentioned in my previous post this step will not be necessary when the pull request for the io.quarkus:quarkus-narayana-lra extension is approved. Now build and test the ticket service, making sure that you have already started a coordinator as described in the previous blog (or you can use the shell scripts ): ./mvnw clean package -DskipTests # skip tests java -jar target/quarkus-app/quarkus-run.jar &amp;amp; # run the application in the background curl http://localhost:8081/tickets/book TicketResource.bookTicket: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_2 1234 The bookTicket() method prints the method name and the id of the active LRA followed by the hard-coded booking id 1234. GENERATE A PROJECT FOR BOOKING TRIPS Now create a second microservice which will be used for booking trips. It will invoke other microservices to complete trip bookings. In order to simplify the example there is just the single remote ticket service involved in the booking process. First generate the project. Like the ticket service, the example will be REST based so include the resteasy and rest-client extensions: mvn io.quarkus:quarkus-maven-plugin:2.0.1.Final:create \ -DprojectGroupId=org.acme \ -DprojectArtifactId=trip \ -DclassName="org.acme.trip.TripResource" \ -Dpath="/trips" \ -Dextensions="resteasy,rest-client" cd trip The rest-client extension includes support for MicroProfile REST Client which we shall use to perform the remote REST invocations from the trip to the ticket service. For REST Client we need a TicketService and we need to register it as shown on line 12 of the following listing: 1: package org.acme.trip; 2: 3: import org.eclipse.microprofile.rest.client.inject.RegisterRestClient; 4: 5: import javax.ws.rs.GET; 6: import javax.ws.rs.Path; 7: import javax.ws.rs.Produces; 8: import javax.ws.rs.core.MediaType; 9: 10: @Path("/tickets") 11: @Produces(MediaType.APPLICATION_JSON) 12: @RegisterRestClient 13: public interface TicketService { 14: 15: @GET 16: @Path("/book") 17: String bookTicket(); 18: } Let’s also create a TripService and inject an instance of the TicketService into it, marking it with the @RestClient annotation on line 11. The quarkus rest client support will configure this injected instance such that it will perform remote REST calls to the ticket service (the remote endpoint for the ticket service will be configured below in the application.properties file): 1: package org.acme.trip; 2: 3: import org.eclipse.microprofile.rest.client.inject.RestClient; 4: import javax.enterprise.context.ApplicationScoped; 5: import javax.inject.Inject; 6: 7: @ApplicationScoped 8: public class TripService { 9: 10: @Inject 11: @RestClient 12: TicketService ticketService; 13: 14: String bookTrip() { 15: return ticketService.bookTicket(); // only one service will be used for the trip booking 16: 17: // if other services need to be part of the trip they would be called here 18: // and the TripService would associate each step of the booking with the id of the LRA 19: // (although I've not shown it being passed in this example) and that would form the 20: // basis of the ability to compensate or clean up depending upon the outcome. 21: // We may include a more comprehensive/realistic example in a later blog. 22: } 23: } And now we can inject an instance of this service into the generated TripResource (src/main/java/org/acme/trip/TripResource.java) on line 26. I have also annotated the bookTrip() method with an LRA annotation so that a new LRA will be started before the method is started (if one wasn’t already present) and I have added @Complete and @Compensate callback methods (these will be called when the LRA closes or cancels, respectively): 1: package org.acme.trip; 2: 3: import javax.inject.Inject; 4: import javax.ws.rs.GET; 5: import javax.ws.rs.Path; 6: import javax.ws.rs.Produces; 7: import javax.ws.rs.core.Response; 8: 9: import static javax.ws.rs.core.MediaType.APPLICATION_JSON; 10: 11: // import annotation definitions 12: import org.eclipse.microprofile.lra.annotation.ws.rs.LRA; 13: import org.eclipse.microprofile.lra.annotation.Compensate; 14: import org.eclipse.microprofile.lra.annotation.Complete; 15: // import the definition of the LRA context header 16: import static org.eclipse.microprofile.lra.annotation.ws.rs.LRA.LRA_HTTP_CONTEXT_HEADER; 17: 18: // import some JAX-RS types 19: import javax.ws.rs.PUT; 20: import javax.ws.rs.HeaderParam; 21: 22: @Path("/trips") 23: @Produces(APPLICATION_JSON) 24: public class TripResource { 25: 26: @Inject 27: TripService service; 28: 29: // annotate the hello method so that it will run in an LRA: 30: @GET 31: @LRA(LRA.Type.REQUIRED) // an LRA will be started before method execution and ended after method execution 32: @Path("/book") 33: public Response bookTrip(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 34: System.out.printf("TripResource.bookTrip: %s%n", lraId); 35: String ticket = service.bookTrip(); 36: return Response.ok(ticket).build(); 37: } 38: 39: // ask to be notified if the LRA closes: 40: @PUT // must be PUT 41: @Path("/complete") 42: @Complete 43: public Response completeWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 44: System.out.printf("TripResource.completeWork: %s%n", lraId); 45: return Response.ok().build(); 46: } 47: 48: // ask to be notified if the LRA cancels: 49: @PUT // must be PUT 50: @Path("/compensate") 51: @Compensate 52: public Response compensateWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 53: System.out.printf("TripResource.compensateWork: %s%n", lraId); 54: return Response.ok().build(); 55: } 56: } For the blog we can skip the tests: rm src/test/java/org/acme/trip/* Configure the trip service to listen on port 8082 (line 2). Also configure the remote ticket endpoint as required by the MicroProfile REST Client specification (line 5): 1: quarkus.arc.exclude-types=io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantRegistry,io.narayana.lra.filter.ServerLRAFilter,io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantResource 2: quarkus.http.port=8082 3: quarkus.http.test-port=8082 4: 5: org.acme.trip.TicketService/mp-rest/url=http://localhost:8081 6: org.acme.trip.TicketService/mp-rest/scope=javax.inject.Singleton Add dependencies on microprofile-lra-api and narayana-lra to the pom to include the MicroProfile LRA annotations and the narayana implementation of them so that the application can request that the LRA context be propagated during interservice communications: &lt;dependency&gt; &lt;groupId&gt;org.eclipse.microprofile.lra&lt;/groupId&gt; &lt;artifactId&gt;microprofile-lra-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.narayana.rts&lt;/groupId&gt; &lt;artifactId&gt;narayana-lra&lt;/artifactId&gt; &lt;version&gt;5.12.0.Final&lt;/version&gt; &lt;/dependency&gt; and finally, build and run the microservice: ./mvnw clean package -DskipTests java -jar target/quarkus-app/quarkus-run.jar &amp;amp; Use curl to book a trip. The HTTP GET request to the trips/book endpoint is handled by the trip service bookTrip() method and it then invokes the ticket service to book a ticket. When the bookTrip() method finishes the LRA will be closed (since the default value for the LRA.end attribute is true), triggering calls to the service @Complete methods of the two services: curl http://localhost:8082/trips/book TripResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TripResource.completeWork: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.completeWork: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c 1234 Notice the output shows the bookTrip and bookTicket methods being called and also notice that the @Complete methods of both services (completeWork()) were called. The id of the LRA on all calls should be the same value as shown in the example output, this is worthwhile noting since the completion and compensation methods will typically use it in order to determine which actions it should clean up for or compensate for when the LRA closes or cancels. Not shown here, but if there was a problem booking the ticket then the ticket service should return a JAX-RS status code (4xx and 5xx HTTP codes by default) that triggers the cancellation of the LRA, and this would then cause the @Compensate methods of all services involved in the LRA to be invoked. -------------------------------------------------------------------------------- Last updated 2021-07-20 21:25:50 BST&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6KP9A9oleNg" height="1" width="1" alt=""/&gt;</content><dc:creator>Michael Musgrove</dc:creator><feedburner:origLink>https://jbossts.blogspot.com/2021/07/how-to-use-long-running-actions-between.html</feedburner:origLink></entry><entry><title type="html">Error handling in Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JoZkCTo7ztw/error-handling-in-kogito.html" /><author><name>Tiago Dolphine</name></author><id>https://blog.kie.org/2021/07/error-handling-in-kogito.html</id><updated>2021-07-20T16:33:30Z</updated><content type="html">WHAT ARE ERRORS WITHIN PROCESSES? First, we need to define the most usual type of errors, they can be divided into two main categories: technical and business errors. Technical Errors could be defined as general errors that are not explicitly handled by the process, for instance, a database communication or constraint issue, a network connection that is down, or even an external service that is not responding. These errors are hidden and harmless for the process itself, which means the system and internal logic might have capabilities to handle them, like retries upon certain failures, or a more critical technical error may cause the transaction to fail, rolling back changes and keeping the process in a consistent state. Business Errors, on the other hand, are related to the domain of a given process and are handled in the process declaration with a specific business meaning, that triggers an alternative flow to properly handle that situation, for instance, a credit limit exceeded, an item out of stock or even fraud that was detected in the payment process. In this post, let’s focus on the business errors and the error events supported in Kogito after the ERROR EVENTS Error events are events that are triggered by a defined known error. In BPMN, an error is meant for business errors and allows us to explicitly model errors with an alternative flow for them; they are represented by a flash symbol. START ERROR EVENT A start error event indicates the point to start a flow to handle a situation where an error has happened, and can only be used to trigger an Event Sub-Process, it could not be used to start a new process instance, for example. The Event subprocess is the way to declare the error handling flow and has access to the parent process or subprocess variables. Process using Start and End Error Events with Event Sub-process END ERROR EVENT The end error events are used to stop the process execution flow throwing an error when it is reached. This error can be caught by an error boundary event or triggering a start event that matches the error by its error code. In case there is no matching, the execution is stopped and the process follows to an error state (semantics defaults to the none end event semantics). BOUNDARY ERROR EVENT A boundary error event is placed in the boundary of an activity, for instance, an embedded subprocess or a service task, and indicates a specific error defined by the error code, can be caught within the scope of the activity in which it is hooked to. When an end error event is achieved, its referred error is thrown and it will be propagated upwards to its parent until the point an error boundary event is found, as already mentioned, where it matches the given error code from the thrown error. The activity where a boundary error is defined and all the nested ones, like any other subprocess, are stopped. The process continues from the sequence flow attached to the boundary error event. Another point of observation might be that an event subprocess if added to an embedded subprocess, could become an alternative to the use of a boundary event. Process using a Boundary Error Event EXCEPTIONS AS ERRORS In Kogito it is provided a way to use Java Exceptions to be mapped to Errors, to use this feature, the FQN of the exception class should be used as the error code in the process, but keep in mind this is just an alternative in the case where the error could not be properly thrown as an Error Event in the process. The exception mapped to error works in some specific locations like in a Service Task implementation, in a custom work item handler, or even in a Script Task.  In general, this feature should only be used in some edge cases where you need to turn a technical error (exception in your code) into a business error, that is not the standard behavior, but it could be useful in some situations and in this case the process takes control of the alternative flow when this error happens. Setting the error code as the Java Exception FQN Seat reservation process using exception as an error in a service task RETRY USING ERROR EVENTS When designing a process it might be useful to explicitly declare a retry logic when some unexpected situation occurs, this is useful for long-running actions, and it is not recommended for short-lived ones that in general are technical errors like an HTTP request timeout, that could be handled by the HTTP client library instead of the process itself.  Error events could be an alternative to do retries inside the process flow. For example, we could consider a process as follows, where we retry to execute an action, declared in a sub-process, upon a condition where a certain error event is thrown,  in this situation the error is caught and a timer could be used to wait for some time before executing the same operation again. Retry process example CONCLUSIONS The business error handling in the process has advantages compared to the exception handling in the code itself, where we could consider technical errors. So this clear separation of errors types is important to simplify the complexity of a system. It is possible to see and easily monitor what happens in the process when unexpected situations happen, it allows us to easily measure unexpected situations and it makes the process easy to evolve and to be maintained like changing an alternative flow to another flow upon an error. Error handling capability provided by Kogito is important to be considered when designing processes and it brings many benefits, that’s why creating the business logic to handle business errors and isolating them from internal technical errors should be taken into account in the whole system architecture. SHOW ME THE CODE! In the next blog post, let’s use the Error Events that were covered here in concrete examples where you be able to run and test. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JoZkCTo7ztw" height="1" width="1" alt=""/&gt;</content><dc:creator>Tiago Dolphine</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/error-handling-in-kogito.html</feedburner:origLink></entry><entry><title type="html">Integrating Red Hat Process Automation Manager and Red Hat AMQ Streams on OpenShift in 4 steps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/kne3WIvD6EM/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/07/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html</id><updated>2021-07-20T13:44:25Z</updated><content type="html">An event-driven architecture is a model that allows communication between services in a decoupled fashion. This pattern has evolved into a powerful software paradigm and covers a wide array of use cases. Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. With this industry evolution, we can now create event-driven business processes which can work seamlessly in a microservices-based environment. With the latest release of Red Hat Process Automation Manager (7.11), you can work with business processes capable of interacting with external services via events, either by emitting or consuming them. Earlier this year,  wrote a detailed  about the integration between Red Hat Process Automation Manager and Kafka. In this article, we will look at the integration of Red Hat Process Automation with Red Hat AMQ Streams on OpenShift. Red Hat AMQ Streams is an enterprise grade Kubernetes-native Kafka solution. In IT today, it can be both challenging and time-consuming for operations and development teams to be experts in many different technologies knowing how to use them, while also knowing how to install, configure, and maintain them. Kubernetes operators help streamline the installation, configuration, and maintenance complexities. We will be using the AMQ Streams Operator and the Business Automation Operator to help simplify the process. In this article you’ll see how to deliver and test an event-driven process application on OpenShift in four steps: 1. Kafka deployment on OpenShift 2. Red Hat Process Automation Manager deployment on OpenShift 3. Creation and deployment of the business application 4. Test of the business application using events To learn more check with a detailed step-by-step guide. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/kne3WIvD6EM" height="1" width="1" alt=""/&gt;</content><dc:creator>Sadhana Nandakumar</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html</feedburner:origLink></entry><entry><title>Deploy Node.js applications to Red Hat OpenShift with Helm</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/EN_yHQH-Iz8/deploy-nodejs-applications-red-hat-openshift-helm" /><author><name>Ash Cripps</name></author><id>20cbf5cf-69e0-4305-9cd5-16cb7dfb1e46</id><updated>2021-07-20T07:00:00Z</updated><published>2021-07-20T07:00:00Z</published><summary type="html">&lt;p&gt;There are many different ways to deploy your &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; applications to &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. This article shows you how to deploy a Node.js application using Helm, along with some recent additions to OpenShift.&lt;/p&gt; &lt;h2&gt;What is Helm?&lt;/h2&gt; &lt;p&gt;&lt;a&gt;Helm&lt;/a&gt; is a package manager for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, which you can use to define, install, and upgrade all types of Kubernetes applications. You can think of Helm as an operating system packager (such as &lt;code&gt;apt&lt;/code&gt; or &lt;strong&gt;yum&lt;/strong&gt;) but for Kubernetes. With Helm, you package your Kubernetes application into a &lt;em&gt;chart&lt;/em&gt;, which is a series of files that define the Kubernetes resources for your deployment. You can use Helm for a variety of scenarios—from very simple applications to complex ones with many dependencies.&lt;/p&gt; &lt;p&gt;Helm offers a fast and effective way for you and your customers to automate Node.js application deployments. Helm also supports Go, which allows for greater chart customization depending on user-specified values. (You can turn certain features on or off depending on the values.) For more information, see the &lt;a href="https://helm.sh/docs/"&gt;Helm documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can use Helm to deploy applications to any Kubernetes environment through the command line. It’s often as easy as &lt;code&gt;helm install XYZ&lt;/code&gt;. However, in OpenShift we’ve worked to make it even easier. There are now two ways to deploy applications with Helm using the OpenShift user interface (UI).&lt;/p&gt; &lt;p&gt;We'll start with a Helm chart template that was recently made available on OpenShift. You can use the template to deploy your Node.js application to OpenShift via Helm as a starter, and then customize it to create your own Helm chart. While you can also use this template to deploy to Kubernetes, it includes OpenShift extensions that make deployments easier in that environment.&lt;/p&gt; &lt;p&gt;In the next sections, I will show you how to use the Helm chart template to deploy a Node.js application to OpenShift with just a few clicks. After that, we'll talk through the chart's implementation, and I'll show you how to package up your own Helm chart and add it to the OpenShift developer catalog.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can also add your own Helm charts to OpenShift by using a custom resource (CR) to &lt;a href="https://docs.openshift.com/container-platform/4.6/cli_reference/helm_cli/configuring-custom-helm-chart-repositories.html"&gt;create a new Helm chart repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Setting up the demonstration&lt;/h2&gt; &lt;p&gt;To follow this demonstration, you will need access to an OpenShift cluster. I'm using &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt;, which allows me to run a single-node OpenShift cluster locally. It doesn’t have all the features of an OpenShift cluster, but it has everything we need for this article. Alternatively, You could use the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which requires a Red Hat account.&lt;/p&gt; &lt;p&gt;You will also need a Node.js application that can be containerized. If you don't have one, you can use the sample program &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy a simple application with Helm on OpenShift&lt;/h2&gt; &lt;p&gt;The Helm chart that I added allows you to easily deploy a simple application with Helm through the OpenShift user interface, without having to write your own chart. Let's go through the steps together.&lt;/p&gt; &lt;h3&gt;Step 1: Select Helm Chart from the project's topology view&lt;/h3&gt; &lt;p&gt;First, you want to be inside the OpenShift console's developer view. From there, click on the topology view in the left-hand menu. You will be presented with the developer catalog, featuring a variety of deployment options. Go ahead and click on Helm Chart, as shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-1.png?itok=ztMJgYH2" width="600" height="316" alt="The OpenShift developer catalog with the Helm Charts option highlighted." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Select Helm Chart as your deployment option. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 2: Select the Node.js chart&lt;/h3&gt; &lt;p&gt;From within the Helm Chart section of the developer catalog, select the Node.js chart, which is highlighted in Figure 2. Then, click &lt;strong&gt;Install Helm Chart&lt;/strong&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-2.png?itok=vAU5HzGH" width="600" height="325" alt="The Helm Charts section of the developer catalog with the Node.js option highlighted." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Select the Node.js Helm chart from the developer catalog. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 3: Configure the Helm release&lt;/h3&gt; &lt;p&gt;Now, you can configure the values that will be implanted in your Helm release. OpenShift gives you two ways to input values, using either the user-friendly form view or the YAML view. For the purpose of this demonstration, we'll use the form view.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Not all of the available values are presented in the form view, so the YAML view gives you more control.&lt;/p&gt; &lt;p&gt;In the form shown in Figure 3, I name my release &lt;code&gt;nodejs-example&lt;/code&gt;, set my image name as &lt;code&gt;my-node-image&lt;/code&gt;, and input the URL of my source code’s Git repository. For this example, I'm using the &lt;code&gt;nodejs-http-rest&lt;/code&gt; example from &lt;code&gt;nodeshift-starters&lt;/code&gt;, but feel free to use your own repository if you wish.&lt;/p&gt; &lt;p&gt;I'll leave the rest of the options set to default (blank) for now, but you can change them. For example, you might want to pull from a different Node source image for the &lt;a href="https://docs.openshift.com/container-platform/4.7/openshift_images/using_images/using-s21-images.html"&gt;S2I&lt;/a&gt; builder. Figure 3 shows my completed form.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-3.png?itok=I9FbUWAF" width="600" height="326" alt="The completed Helm Chart form." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Example of how to fill out the Node.js Helm chart form. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 4: Install and track the build&lt;/h3&gt; &lt;p&gt;Once you have filled in the required values, go ahead and click &lt;strong&gt;Install&lt;/strong&gt;. At first, you will notice the pod within your deployment is reporting &lt;code&gt;ErrImagePull&lt;/code&gt;. This is &lt;em&gt;normal&lt;/em&gt;. The pod is unable to pull down your image because it hasn't been built yet! You can track your image's progress under the &lt;strong&gt;Builds&lt;/strong&gt; section of your deployments submenu, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-4.png?itok=bPujM4w2" width="600" height="312" alt="Pods spinning up from the Helm chart." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The OpenShift topology overview, showing the pods spinning up from the Helm chart deployment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Congratulations—you've just deployed your Node.js application to OpenShift via Helm! To view your application, click the &lt;strong&gt;open url&lt;/strong&gt; button in the top right corner of the deployment in the topology view.&lt;/p&gt; &lt;h2&gt;Customize your Helm chart&lt;/h2&gt; &lt;p&gt;For this section, I assume you already have a basic understanding of how Helm charts are structured, so we won't dig deep into the base files. Instead, we will explore the OpenShift-specific files you wouldn't normally find in a standard &lt;a href="https://helm.sh/docs/chart_template_guide/getting_started"&gt;Helm chart&lt;/a&gt;. We'll explore the following files, which you can use to develop a custom Helm chart:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;values.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;buildconfig.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;imagestream.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;route.yaml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Note that the &lt;code&gt;buildconfig.yaml&lt;/code&gt;, &lt;code&gt;imagestream.yaml&lt;/code&gt;, and &lt;code&gt;route.yaml&lt;/code&gt; files are all specific to OpenShift.&lt;/p&gt; &lt;h3&gt;values.yaml&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;values.yaml&lt;/code&gt; file is very important as it is the one your user will interact with, either directly or through the UI. So, you need to make sure it is easy to follow and that you have enough values to cover all the customization use cases you wish your user to have.&lt;/p&gt; &lt;h3&gt;buildconfig.yaml&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;buildconfig.yaml&lt;/code&gt; is the first OpenShift-specific file in the Helm chart. Let's have a look at the file for the sample Node.js chart:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;{{- if .Values.build.enabled }} apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: {{ include "nodejs.name" . }} labels: {{- include "nodejs.labels" . | nindent 4 }} spec: source: type: Git git: uri: {{ .Values.build.uri }} ref: {{ .Values.build.ref }} {{- if .Values.build.contextDir }} contextDir: {{ .Values.build.contextDir }} {{- end }} strategy: type: Source sourceStrategy: from: kind: ImageStreamTag namespace: {{ .Values.build.source.namespace }} name: {{ .Values.build.source.name }} {{- if .Values.build.pullSecret }} pullSecret: name: {{ .Values.build.pullSecret }} {{- end }} {{- if .Values.build.env }} env: {{- tpl (toYaml .Values.build.env) . | nindent 8 }} {{- end }} output: to: kind: {{ .Values.build.output.kind }} name: {{ include "nodejs.imageName" . }} {{- if and (eq .Values.build.output.kind "DockerImage") .Values.build.output.pushSecret }} pushSecret: name: {{ .Values.build.output.pushSecret }} {{- end }} {{- if .Values.build.resources }} resources: {{- toYaml .Values.build.resources | nindent 4 }} {{- end }} triggers: - type: ConfigChange {{- end }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing you'll notice is &lt;code&gt;{{- if .Values.build.enabled }}&lt;/code&gt;, which is an &lt;code&gt;if&lt;/code&gt; conditional in Golang templating. It allows the user to specify if they wish to build the image themselves using this config or use an image already in the OpenShift instance. You can also use this option to turn the OpenShift-specific features on or off. This option is useful if you want to be able to deploy your Helm chart to a vanilla Kubernetes environment.&lt;/p&gt; &lt;p&gt;The next line to look at is &lt;code&gt;apiVersion: build.openshift.io/v1&lt;/code&gt;. This line specifies that the file is OpenShift-specific and uses the OpenShift API to build a deployable image.&lt;/p&gt; &lt;p&gt;The next key section is the &lt;code&gt;source&lt;/code&gt; section under &lt;code&gt;spec&lt;/code&gt;. This section, as the name hints, is where you specify the source of the program: What Git repo and which reference should be checked out.&lt;/p&gt; &lt;p&gt;Next up, we specify what &lt;a href="https://docs.openshift.com/container-platform/4.7/cicd/builds/build-strategies.html"&gt;strategy&lt;/a&gt; we will use to build the image. For this Helm chart, I used the source-to-image (S2I) build strategy but you could choose to use a Docker strategy or a custom build. Inside the &lt;code&gt;strategy&lt;/code&gt; block, I specify that I want to build from an &lt;code&gt;ImageStreamTag&lt;/code&gt; and then have variables for the namespace, name, and if the source has a pull secret or not. The user can also use the &lt;code&gt;strategy&lt;/code&gt; block to specify if they have environment variables for the build.&lt;/p&gt; &lt;p&gt;Finally, we see the &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;resources&lt;/code&gt; blocks. The &lt;code&gt;resources&lt;/code&gt; block is where the user can specify if they want to limit the resources (such as CPU and memory) to be available to the pod once it is built. The &lt;code&gt;output&lt;/code&gt; block is where the user specifies what type of output they would like (I defaulted to &lt;code&gt;ImageStreamTag&lt;/code&gt; for this example), the name of the output, and the push secret (if needed) to upload the image.&lt;/p&gt; &lt;h3&gt;imagestream.yaml&lt;/h3&gt; &lt;p&gt;Now, let's have a look at the &lt;code&gt;imagestream.yaml&lt;/code&gt; file, which is another OpenShift-specific file. It's a fairly simple file where you just specify the kind of image stream and its name and labels. For more information about image streams please refer to the &lt;a href="https://docs.openshift.com/container-platform/4.7/openshift_images/image-streams-manage.html"&gt;OpenShift documentation for image streams&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;route.yaml&lt;/h3&gt; &lt;p&gt;Lastly, let's take a look at the &lt;code&gt;route.yaml&lt;/code&gt; file. This file is used to set up the routes for your application within OpenShift. Here is where you will do things like establishing a TLS connection, and specifying ports and certificates. You can use this file to expose your application in OpenShift without having to mess around with port forwarding like you would in standard Kubernetes.&lt;/p&gt; &lt;h2&gt;Package and deploy the Helm chart&lt;/h2&gt; &lt;p&gt;Once you have your Helm chart completed, you'll need to package it into a .tar file and upload it to your repository of choice. You can then deploy it to OpenShift via a custom resource definition (CRD), like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOF | oc apply -f - apiVersion: helm.openshift.io/v1beta1 kind: HelmChartRepository metadata: name: $name spec: name: $name connectionConfig: url: https://raw.githubusercontent.com/$org/$repo/$ref EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The CRD will show up under &lt;strong&gt;Custom Resource Definitions&lt;/strong&gt; in the OpenShift administrator view, and the chart itself will show up in the OpenShift developer catalog. You will be able to select the chart from the catalog to deploy your application.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has been an introduction to deploying Node.js applications to OpenShift via Helm. You saw how to use an example Helm chart or build your own chart to deploy a Node.js application.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/20/deploy-nodejs-applications-red-hat-openshift-helm" title="Deploy Node.js applications to Red Hat OpenShift with Helm"&gt;Deploy Node.js applications to Red Hat OpenShift with Helm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/EN_yHQH-Iz8" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ash Cripps</dc:creator><dc:date>2021-07-20T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/20/deploy-nodejs-applications-red-hat-openshift-helm</feedburner:origLink></entry><entry><title type="html">Eclipse Vert.x 4.1.2 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jrlC_Sar5RE/eclipse-vert-x-4-1-2" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-1-2</id><updated>2021-07-20T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.1.2 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jrlC_Sar5RE" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/eclipse-vert-x-4-1-2</feedburner:origLink></entry><entry><title type="html">Add SQL datasource for authoring dashboards</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cAw05RDXDds/add-sql-datasource-for-authoring-dashboards.html" /><author><name>Manaswini Das</name></author><id>https://blog.kie.org/2021/07/add-sql-datasource-for-authoring-dashboards.html</id><updated>2021-07-19T14:37:29Z</updated><content type="html">Dashboards are quintessential ways of gaining precise and accurate at-a-glance insights into complex data indicating key performance indicators(KPIs), metrics, and other key data points related to business or specific processes. is a one such standalone tool that is also integrated into Business Central and is used by the Datasets editor and Content Manager page to facilitate creating dashboards and reporting. You can get started by referring to the , if you are a first-time user. Refer to for configuring CSV datasets for authoring dashboards on DashBuilder. In the , we walked you through the process of adding Prometheus datasets for authoring dashboards in DashBuilder. When it comes to building dashboards, you can configure the dashboards to consume your own datasets in DashBuilder from a variety of sources like Bean, CSV, SQL, Prometheus, Elastic Search, Kafka, and Execution server. In this post, you will learn how to add and configure a SQL dataset for your dashboards. ABOUT SQL SQL or Structured Query Language is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e. data incorporating relations among entities and variables. ADD AND CONFIGURE SQL DATASETS ON DASHBUILDER 1. To get started, you will have to configure DashBuilder to run on the WildFly server since it is impossible to get the SQL data set configured in Dev mode as in Dev mode all WildFly configurations are lost when we rebuild the project. For this example, I have used WildFly 19.x. and the to deploy in the WildFly server. Unzip the WildFly zip and open a terminal inside the WildFly unzipped folder(let’s call this WILDFLY_HOME). Go to /bin and run standalone.sh using ./standalone.sh or sudo sh standalone.sh. You can now find the WildFly server running in localhost:9990 and see the following screen which prompts to add a user. WildFly page that you can see after typing localhost:9990 on your browser 2. Open another terminal and run ./add-user.sh and add a ManagementRealm user by selecting option “a” and adding a username and password, followed by adding an “admin” group following the instructions flashing on the terminal. Here is what the terminal should look like after you add a user. Add ManagementRealm user Note: You have to add a ManagementRealm user to login into WildFly, the applicationRealm users are the users you will need to login into the apps deployed in WildFly. 3. Click on the “Try again” link on the browser and login the same credentials that you configured in the terminal. You will now be able to see the HAL Management Console screen. Management Console 4. Now click on the Start link under Deployments and deploy the DashBuilder WAR you downloaded earlier. Refer to the GIF below for reference. You can click on the link against “Context Root” in order to access DashBuilder. Deploy DashBuilder WAR on WildFly 5. Now add a ApplicationRealm user in order to access DashBuilder by running ./add-user.sh -a -u ‘admin’ -p ‘admin’ -g ‘admin’ in a terminal window inside the bin folder of WILDFLY_HOME. After the user is successfully added, you will now be able to use the above credentials to login into DashBuilder. 6. Time to add a MySQL JDBC driver to our WildFly server. Download the JDBC driver for MySQL(Connector/J) from the . I’m using MySQL Connector/J 8.0.17. MySQL Connector/J 8.0 is compatible with all MySQL versions starting with MySQL 5.5. Download MySQL Connector/J 8.0 at ‘ /opt’ directory using below commands: sudo cd /opt sudo wget Extract the tarball using below command: sudo tar -xvzf mysql-connector-java-8.0.17.tar.gz Inside the /opt/mysql-connector-java-8.0.17/ directory that we extracted, there is a jar file by the name mysql-connector-java-8.0.17.jar. This jar file contains the required classes for the MySQL JDBC driver. 7. Try creating the Module itself using the ./jboss-cli.sh command inside the bin folder of WILDFLY_HOME rather than manually writing the module.xml file. This is because when we use some text editors, they might append some hidden chars to our files. (Especially when we do a copy &amp;amp; paste in such editors). Run connect when prompted(Note:provided the WildFly server must be running on another terminal tab). [standalone@localhost:9990 /] module add — name=com.mysql.driver — dependencies=javax.api,javax.transaction.api — resources=/PATH/TO/mysql-connector-java-5.1.35.jar [standalone@localhost:9990 /] :reload {“outcome” =&gt; “success”,“result” =&gt; undefined} After running above command you should see the module.xml generated in the following location: wildfly-19.0.0.Final/modules/com/mysql/driver/main/module.xml Now create DataSource: [standalone@localhost:9990 /] /subsystem=datasources/jdbc-driver=mysql/:add(driver-module-name=com.mysql.driver,driver-name=mysql,jdbc-compliant=false,driver-class-name=com.mysql.jdbc.Driver) {“outcome” =&gt; “success”} OR You can also choose to manually add the driver by downloading and putting it inside WILDFLY_HOME/modules/system/layers/base/com/mysql/main. Create a module.xml inside the same folder where you have the JAR. Using any kind of text editor, create file inside your WildFly path, WILDFLY_HOME/modules/system/layers/base/com/mysql/main, and this is the XML file contents of it: &lt;module name=”com.mysql.driver” xmlns=”urn:jboss:module:1.5"&gt; &lt;resources&gt; &lt;resource-root path=”mysql-connector-java-8.0.17.jar”&gt; &lt;/resource-root&gt; &lt;/resources&gt; &lt;dependencies&gt; &lt;module name=”javax.api”&gt; &lt;module name=”javax.transaction.api”&gt; &lt;/module&gt; &lt;/module&gt; &lt;/dependencies&gt; &lt;/module&gt; Add MySQL connector to the driver list Open WILDFLY_HOME/standalone/configuration/standalone.xml, and then find &lt;drivers&gt; tag, inside that tag, put these lines to add MySQL driver: &lt;driver name=”mysql” module=”com.mysql.driver”&gt; &lt;driver-class&gt;com.mysql.cj.jdbc.Driver&lt;/driver-class&gt; &lt;/driver&gt; Now you can restart WildFly and expect that the new driver will be inside the available list driver. Click on the Start button under Configuration in the Homepage of the Management Console. Now click on Subsystems -&gt; Datasources &amp;amp;Drivers -&gt; JDBC Drivers and you can see “mysql” added. 8. Time to have the MySQL configured in our local system. if you haven’t yet. I’m using MySQL 8.0.18 for this example.Run mysql -u root -p and enter the password. Create a database called “testdb” and create a table that you will use for your dashboards using the appropriate SQL queries. I have added an example for your reference. Example to create a table 9. Now you will have to create a user and grant all privileges to the user. Run CREATE USER ‘username’@’localhost’ IDENTIFIED BY ‘password’followed by GRANT ALL PRIVILEGES ON testdb.* TO ‘username’@’localhost’; You will now see Query OK, 0 rows affected (0.01 sec) on your terminal after running both the commands. Note: Just replace the username and password with your own, don’t remove the quotes. We are good with the local MySQL dataset configuration now. 10. Now, let’s add the datasource on the management console. Click on “+” in the Datasource section. Select MySQL. Click on Next and change the JNDI to java:jboss/datasources/MySqlDS. Don’t change anything in the JDBC driver screen. In the Connection screen, change the database name in the Connection URL from “mysqldb” to “testdb” and add the username and password. Click on test connection in the Next screen and if successful, click on Finish. Add SQL datasource Troubleshooting: * The JNDI name cannot be changed once configured. If you want to change that go to standalone.xml inside standalone/configuration in WILDFLY_HOME. Search for the &lt;datasource&gt; tag and change the JNDI name corresponding to the required data source there. You will have to restart the server after you change this file. Make sure the JNDI name contains jboss/datasources similar to the ExampleDS datasource configuration in standalone.xml otherwise the datasource can’t be detected. * Don’t add “@localhost” while adding username and the password in the Connection tab, else you may get an wrong/username and password error. Error while configuring driver * You may see the above error in the JDBC tab. This only shows if the driver hasn’t been configured properly. Make sure it is configured correctly. You can always access the live logs in server.log inside standalone/configuration/log in WILDFLY_HOME to know the exact issue. 11. You can now access the Configured datasource inside DashBuilder. Go to Deployments tab and click on the link against “Context Root”. Log in to DashBuilder using the ApplicationRealm user credentials you created in the beginning. You will see the homepage which resembles the screen below. DashBuilder Home page 12. In order to add a dataset, select Datasets from the menu. Alternatively, you can also click on the “Menu” dropdown on the top left corner beside the DashBuilder logo on the navigation bar. You will now see two headings, “DashBuilder” and “Administration”. Click on “Datasets” under Administration. You will see the Dataset Authoring home page with instructions to add datasets, create displayers and new dashboards. Data Set Authoring Home page 13. Now, you can either click on the “New Data Set” on the top left corner beside “Data Set Explorer” or click on the “new data set” hyperlink in Step 1. You will now see a Data Set creation wizard screen that shows all dataset providers like Bean, CSV, SQL, Elastic Search, and so on. Data Set Creation Wizard Page 14. Select SQL from the list of provider types. You will now see the following screen to add your SQL dataset. Add a name. In the Data Source dropdown, select MySqlDS(the name that you used while configuring data source in the Management console) and add source as shown in the screenshot below. SQL Configuration page If you are confused about the role of the fields, please hover on the question mark icons beside the fields or the text boxes adjacent to them. Click on Test to preview your dataset. 15. You are now on the Preview tab. You can now have a look at the data columns and add filters in the tabs above the dataset. You can also edit the types or delete the columns that you don’t require by unchecking the checkbox beside the columns provided on the left side. If the preview isn’t what you expected, you can switch back to the Configuration tab by clicking on it and make changes. If you are satisfied with the preview, click on the “Next” button. 16. Enter required comments in the Save modal and click on the Save button. Your dataset has been added and configured. You will now be directed back to the Data Set Authoring Home page. You can now see a dataset on the left pane. When you add multiple datasets, you can see the list of all of them on the left pane. Here is a screen recording of the full flow. Adding and configuring SQL datasets on DashBuilder You can now click on the Menu dropdown on the navigation bar and select Content Manager to create pages and dashboards. Ensure that the columns are well configured with proper types in the “Data” section of the “Displayer Editor” after dragging the component. CONCLUSION With the help of this post, you will be able to add and configure data from a SQL dataset to be consumed by your dashboards. Feel free to add your comments regarding where you got stuck, so that we can improve and update this guide going further. In the upcoming posts, we will add walkthroughs of the remaining dataset providers, so stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cAw05RDXDds" height="1" width="1" alt=""/&gt;</content><dc:creator>Manaswini Das</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/add-sql-datasource-for-authoring-dashboards.html</feedburner:origLink></entry></feed>
