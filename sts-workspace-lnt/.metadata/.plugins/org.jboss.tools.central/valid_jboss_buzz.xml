<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">How to use Long Running Actions between microservices</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6KP9A9oleNg/how-to-use-long-running-actions-between.html" /><author><name>Michael Musgrove</name></author><id>https://jbossts.blogspot.com/2021/07/how-to-use-long-running-actions-between.html</id><updated>2021-07-20T20:31:00Z</updated><content type="html">INTRODUCTION In my I showed how to run a Long Running Action (LRA) within a single JAX-RS resource method using quarkus features to build and run the application. I showed how to create and start an LRA coordinator and then generated a basic hello application, showing how to modify the application to run with a long running action (by adding dependencies on the org.eclipse.microprofile.lra:microprofile-lra-api and org.jboss.narayana.rts:narayana-lra artifacts, which together provide annotations for controlling the lifecycle of LRAs). That post also includes links to the and to the . In this follow up post I will indicate how to include a second resource in the LRA. To keep things interesting I’ll deploy the second resource to another microservice and use quarkus’s MicroProfile Rest Client support to implement the remote service invocations. The main difference between this example and the one I developed in the earlier post, apart from the technicalities of using Rest Client, is that we will set the attribute to false in the remote service so that the LRA will remain active when the call returns. In this way the initiating service method has the option of calling other microservices before ending the LRA. CREATING AND STARTING AN LRA COORDINATOR LRA relies on a coordinator to manage the lifecycle of LRAs so you will need one to be running for this demo to work successfully. The showed how to build and run coordinators. Alternatively, which execute all of the steps required in the current post and it includes a shell script called which will build a runnable coordinator jar (it’s fairly simple and short so you can just read it and create your own jar or just run it as is). GENERATE A PROJECT FOR BOOKING TICKETS Since the example will be REST based, include the resteasy and rest-client extensions (on line 6 next): 1: mvn io.quarkus:quarkus-maven-plugin:2.0.1.Final:create \ 2: -DprojectGroupId=org.acme \ 3: -DprojectArtifactId=ticket \ 4: -DclassName="org.acme.ticket.TicketResource" \ 5: -Dpath="/tickets" \ 6: -Dextensions="resteasy,rest-client" 7: cd ticket You will need the mvn program to run the plugin (but the generated projects will include the mvnw maven wrapper). Modify the generated TicketResource.java source file to add Microprofile LRA support. The changes that you will need for LRA are on lines 26 and 27. Line 26 says that the bookTicket method must run with an LRA (if one is not present when the method is invoked then one will be automatically created). Note that we have set the end attribute to false to stop the LRA from being automatically closed when the method finishes. By keeping the LRA active when the ticket is booked, the caller can invoke other services in the context of the same LRA. Most services will require the LRA context for tracking updates which typically will be useful for knowing which actions to compensate for if the LRA is later cancelled: the context is injected as a JAX-RS method parameter on line 27. You will also need to include callbacks for when the LRA is later closed or cancelled (the methods are defined on lines 37 and line 46, respectively). 1: package org.acme.ticket; 2: 3: import static javax.ws.rs.core.MediaType.APPLICATION_JSON; 4: 5: // import annotation definitions 6: import org.eclipse.microprofile.lra.annotation.ws.rs.LRA; 7: import org.eclipse.microprofile.lra.annotation.Compensate; 8: import org.eclipse.microprofile.lra.annotation.Complete; 9: // import the definition of the LRA context header 10: import static org.eclipse.microprofile.lra.annotation.ws.rs.LRA.LRA_HTTP_CONTEXT_HEADER; 11: 12: // import some JAX-RS types 13: import javax.ws.rs.GET; 14: import javax.ws.rs.PUT; 15: import javax.ws.rs.Path; 16: import javax.ws.rs.Produces; 17: import javax.ws.rs.core.Response; 18: import javax.ws.rs.HeaderParam; 19: 20: @Path("/tickets") 21: @Produces(APPLICATION_JSON) 22: public class TicketResource { 23: 24: @GET 25: @Path("/book") 26: @LRA(value = LRA.Type.REQUIRED, end = false) // an LRA will be started before method execution if none exists and will not be ended after method execution 27: public Response bookTicket(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 28: System.out.printf("TicketResource.bookTicket: %s%n", lraId); 29: String ticket = "1234" 30: return Response.ok(ticket).build(); 31: } 32: 33: // ask to be notified if the LRA closes: 34: @PUT // must be PUT 35: @Path("/complete") 36: @Complete 37: public Response completeWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 38: System.out.printf("TicketResource.completeWork: %s%n", lraId); 39: return Response.ok().build(); 40: } 41: 42: // ask to be notified if the LRA cancels: 43: @PUT // must be PUT 44: @Path("/compensate") 45: @Compensate 46: public Response compensateWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 47: System.out.printf("TicketResource.compensateWork: %s%n", lraId); 48: return Response.ok().build(); 49: } 50: } Skip the tests: rm src/test/java/org/acme/ticket/* Add dependencies on microprofile-lra-api and narayana-lra to the pom to include the MicroProfile LRA annotations and the narayana implementation of them so that the LRA context will be propagated during interservice communications: &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.microprofile.lra&lt;/groupId&gt; &lt;artifactId&gt;microprofile-lra-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.narayana.rts&lt;/groupId&gt; &lt;artifactId&gt;narayana-lra&lt;\/artifactId&gt; &lt;version&gt;5.12.0.Final&lt;/version&gt; &lt;/dependency&gt; We are creating ticket and trip microservices so they need to listen on different ports, configure the ticket service to run on port 8081: 1: quarkus.arc.exclude-types=io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantRegistry,io.narayana.lra.filter.ServerLRAFilter,io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantResource 2: quarkus.http.port=8081 3: quarkus.http.test-port=8081 The excludes are pulled in by the org.jboss.narayana.rts:narayana-lra maven dependency. As mentioned in my previous post this step will not be necessary when the pull request for the io.quarkus:quarkus-narayana-lra extension is approved. Now build and test the ticket service, making sure that you have already started a coordinator as described in the previous blog (or you can use the shell scripts ): ./mvnw clean package -DskipTests # skip tests java -jar target/quarkus-app/quarkus-run.jar &amp;amp; # run the application in the background curl http://localhost:8081/tickets/book TicketResource.bookTicket: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_2 1234 The bookTicket() method prints the method name and the id of the active LRA followed by the hard-coded booking id 1234. GENERATE A PROJECT FOR BOOKING TRIPS Now create a second microservice which will be used for booking trips. It will invoke other microservices to complete trip bookings. In order to simplify the example there is just the single remote ticket service involved in the booking process. First generate the project. Like the ticket service, the example will be REST based so include the resteasy and rest-client extensions: mvn io.quarkus:quarkus-maven-plugin:2.0.1.Final:create \ -DprojectGroupId=org.acme \ -DprojectArtifactId=trip \ -DclassName="org.acme.trip.TripResource" \ -Dpath="/trips" \ -Dextensions="resteasy,rest-client" cd trip The rest-client extension includes support for MicroProfile REST Client which we shall use to perform the remote REST invocations from the trip to the ticket service. For REST Client we need a TicketService and we need to register it as shown on line 12 of the following listing: 1: package org.acme.trip; 2: 3: import org.eclipse.microprofile.rest.client.inject.RegisterRestClient; 4: 5: import javax.ws.rs.GET; 6: import javax.ws.rs.Path; 7: import javax.ws.rs.Produces; 8: import javax.ws.rs.core.MediaType; 9: 10: @Path("/tickets") 11: @Produces(MediaType.APPLICATION_JSON) 12: @RegisterRestClient 13: public interface TicketService { 14: 15: @GET 16: @Path("/book") 17: String bookTicket(); 18: } Let’s also create a TripService and inject an instance of the TicketService into it, marking it with the @RestClient annotation on line 11. The quarkus rest client support will configure this injected instance such that it will perform remote REST calls to the ticket service (the remote endpoint for the ticket service will be configured below in the application.properties file): 1: package org.acme.trip; 2: 3: import org.eclipse.microprofile.rest.client.inject.RestClient; 4: import javax.enterprise.context.ApplicationScoped; 5: import javax.inject.Inject; 6: 7: @ApplicationScoped 8: public class TripService { 9: 10: @Inject 11: @RestClient 12: TicketService ticketService; 13: 14: String bookTrip() { 15: return ticketService.bookTicket(); // only one service will be used for the trip booking 16: 17: // if other services need to be part of the trip they would be called here 18: // and the TripService would associate each step of the booking with the id of the LRA 19: // (although I've not shown it being passed in this example) and that would form the 20: // basis of the ability to compensate or clean up depending upon the outcome. 21: // We may include a more comprehensive/realistic example in a later blog. 22: } 23: } And now we can inject an instance of this service into the generated TripResource (src/main/java/org/acme/trip/TripResource.java) on line 26. I have also annotated the bookTrip() method with an LRA annotation so that a new LRA will be started before the method is started (if one wasn’t already present) and I have added @Complete and @Compensate callback methods (these will be called when the LRA closes or cancels, respectively): 1: package org.acme.trip; 2: 3: import javax.inject.Inject; 4: import javax.ws.rs.GET; 5: import javax.ws.rs.Path; 6: import javax.ws.rs.Produces; 7: import javax.ws.rs.core.Response; 8: 9: import static javax.ws.rs.core.MediaType.APPLICATION_JSON; 10: 11: // import annotation definitions 12: import org.eclipse.microprofile.lra.annotation.ws.rs.LRA; 13: import org.eclipse.microprofile.lra.annotation.Compensate; 14: import org.eclipse.microprofile.lra.annotation.Complete; 15: // import the definition of the LRA context header 16: import static org.eclipse.microprofile.lra.annotation.ws.rs.LRA.LRA_HTTP_CONTEXT_HEADER; 17: 18: // import some JAX-RS types 19: import javax.ws.rs.PUT; 20: import javax.ws.rs.HeaderParam; 21: 22: @Path("/trips") 23: @Produces(APPLICATION_JSON) 24: public class TripResource { 25: 26: @Inject 27: TripService service; 28: 29: // annotate the hello method so that it will run in an LRA: 30: @GET 31: @LRA(LRA.Type.REQUIRED) // an LRA will be started before method execution and ended after method execution 32: @Path("/book") 33: public Response bookTrip(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 34: System.out.printf("TripResource.bookTrip: %s%n", lraId); 35: String ticket = service.bookTrip(); 36: return Response.ok(ticket).build(); 37: } 38: 39: // ask to be notified if the LRA closes: 40: @PUT // must be PUT 41: @Path("/complete") 42: @Complete 43: public Response completeWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 44: System.out.printf("TripResource.completeWork: %s%n", lraId); 45: return Response.ok().build(); 46: } 47: 48: // ask to be notified if the LRA cancels: 49: @PUT // must be PUT 50: @Path("/compensate") 51: @Compensate 52: public Response compensateWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 53: System.out.printf("TripResource.compensateWork: %s%n", lraId); 54: return Response.ok().build(); 55: } 56: } For the blog we can skip the tests: rm src/test/java/org/acme/trip/* Configure the trip service to listen on port 8082 (line 2). Also configure the remote ticket endpoint as required by the MicroProfile REST Client specification (line 5): 1: quarkus.arc.exclude-types=io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantRegistry,io.narayana.lra.filter.ServerLRAFilter,io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantResource 2: quarkus.http.port=8082 3: quarkus.http.test-port=8082 4: 5: org.acme.trip.TicketService/mp-rest/url=http://localhost:8081 6: org.acme.trip.TicketService/mp-rest/scope=javax.inject.Singleton Add dependencies on microprofile-lra-api and narayana-lra to the pom to include the MicroProfile LRA annotations and the narayana implementation of them so that the application can request that the LRA context be propagated during interservice communications: &lt;dependency&gt; &lt;groupId&gt;org.eclipse.microprofile.lra&lt;/groupId&gt; &lt;artifactId&gt;microprofile-lra-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.narayana.rts&lt;/groupId&gt; &lt;artifactId&gt;narayana-lra&lt;/artifactId&gt; &lt;version&gt;5.12.0.Final&lt;/version&gt; &lt;/dependency&gt; and finally, build and run the microservice: ./mvnw clean package -DskipTests java -jar target/quarkus-app/quarkus-run.jar &amp;amp; Use curl to book a trip. The HTTP GET request to the trips/book endpoint is handled by the trip service bookTrip() method and it then invokes the ticket service to book a ticket. When the bookTrip() method finishes the LRA will be closed (since the default value for the LRA.end attribute is true), triggering calls to the service @Complete methods of the two services: curl http://localhost:8082/trips/book TripResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TripResource.completeWork: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.completeWork: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c 1234 Notice the output shows the bookTrip and bookTicket methods being called and also notice that the @Complete methods of both services (completeWork()) were called. The id of the LRA on all calls should be the same value as shown in the example output, this is worthwhile noting since the completion and compensation methods will typically use it in order to determine which actions it should clean up for or compensate for when the LRA closes or cancels. Not shown here, but if there was a problem booking the ticket then the ticket service should return a JAX-RS status code (4xx and 5xx HTTP codes by default) that triggers the cancellation of the LRA, and this would then cause the @Compensate methods of all services involved in the LRA to be invoked. -------------------------------------------------------------------------------- Last updated 2021-07-20 21:25:50 BST&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6KP9A9oleNg" height="1" width="1" alt=""/&gt;</content><dc:creator>Michael Musgrove</dc:creator><feedburner:origLink>https://jbossts.blogspot.com/2021/07/how-to-use-long-running-actions-between.html</feedburner:origLink></entry><entry><title type="html">Error handling in Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JoZkCTo7ztw/error-handling-in-kogito.html" /><author><name>Tiago Dolphine</name></author><id>https://blog.kie.org/2021/07/error-handling-in-kogito.html</id><updated>2021-07-20T16:33:30Z</updated><content type="html">WHAT ARE ERRORS WITHIN PROCESSES? First, we need to define the most usual type of errors, they can be divided into two main categories: technical and business errors. Technical Errors could be defined as general errors that are not explicitly handled by the process, for instance, a database communication or constraint issue, a network connection that is down, or even an external service that is not responding. These errors are hidden and harmless for the process itself, which means the system and internal logic might have capabilities to handle them, like retries upon certain failures, or a more critical technical error may cause the transaction to fail, rolling back changes and keeping the process in a consistent state. Business Errors, on the other hand, are related to the domain of a given process and are handled in the process declaration with a specific business meaning, that triggers an alternative flow to properly handle that situation, for instance, a credit limit exceeded, an item out of stock or even fraud that was detected in the payment process. In this post, let’s focus on the business errors and the error events supported in Kogito after the ERROR EVENTS Error events are events that are triggered by a defined known error. In BPMN, an error is meant for business errors and allows us to explicitly model errors with an alternative flow for them; they are represented by a flash symbol. START ERROR EVENT A start error event indicates the point to start a flow to handle a situation where an error has happened, and can only be used to trigger an Event Sub-Process, it could not be used to start a new process instance, for example. The Event subprocess is the way to declare the error handling flow and has access to the parent process or subprocess variables. Process using Start and End Error Events with Event Sub-process END ERROR EVENT The end error events are used to stop the process execution flow throwing an error when it is reached. This error can be caught by an error boundary event or triggering a start event that matches the error by its error code. In case there is no matching, the execution is stopped and the process follows to an error state (semantics defaults to the none end event semantics). BOUNDARY ERROR EVENT A boundary error event is placed in the boundary of an activity, for instance, an embedded subprocess or a service task, and indicates a specific error defined by the error code, can be caught within the scope of the activity in which it is hooked to. When an end error event is achieved, its referred error is thrown and it will be propagated upwards to its parent until the point an error boundary event is found, as already mentioned, where it matches the given error code from the thrown error. The activity where a boundary error is defined and all the nested ones, like any other subprocess, are stopped. The process continues from the sequence flow attached to the boundary error event. Another point of observation might be that an event subprocess if added to an embedded subprocess, could become an alternative to the use of a boundary event. Process using a Boundary Error Event EXCEPTIONS AS ERRORS In Kogito it is provided a way to use Java Exceptions to be mapped to Errors, to use this feature, the FQN of the exception class should be used as the error code in the process, but keep in mind this is just an alternative in the case where the error could not be properly thrown as an Error Event in the process. The exception mapped to error works in some specific locations like in a Service Task implementation, in a custom work item handler, or even in a Script Task.  In general, this feature should only be used in some edge cases where you need to turn a technical error (exception in your code) into a business error, that is not the standard behavior, but it could be useful in some situations and in this case the process takes control of the alternative flow when this error happens. Setting the error code as the Java Exception FQN Seat reservation process using exception as an error in a service task RETRY USING ERROR EVENTS When designing a process it might be useful to explicitly declare a retry logic when some unexpected situation occurs, this is useful for long-running actions, and it is not recommended for short-lived ones that in general are technical errors like an HTTP request timeout, that could be handled by the HTTP client library instead of the process itself.  Error events could be an alternative to do retries inside the process flow. For example, we could consider a process as follows, where we retry to execute an action, declared in a sub-process, upon a condition where a certain error event is thrown,  in this situation the error is caught and a timer could be used to wait for some time before executing the same operation again. Retry process example CONCLUSIONS The business error handling in the process has advantages compared to the exception handling in the code itself, where we could consider technical errors. So this clear separation of errors types is important to simplify the complexity of a system. It is possible to see and easily monitor what happens in the process when unexpected situations happen, it allows us to easily measure unexpected situations and it makes the process easy to evolve and to be maintained like changing an alternative flow to another flow upon an error. Error handling capability provided by Kogito is important to be considered when designing processes and it brings many benefits, that’s why creating the business logic to handle business errors and isolating them from internal technical errors should be taken into account in the whole system architecture. SHOW ME THE CODE! In the next blog post, let’s use the Error Events that were covered here in concrete examples where you be able to run and test. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JoZkCTo7ztw" height="1" width="1" alt=""/&gt;</content><dc:creator>Tiago Dolphine</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/error-handling-in-kogito.html</feedburner:origLink></entry><entry><title type="html">Integrating Red Hat Process Automation Manager and Red Hat AMQ Streams on OpenShift in 4 steps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/kne3WIvD6EM/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/07/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html</id><updated>2021-07-20T13:44:25Z</updated><content type="html">An event-driven architecture is a model that allows communication between services in a decoupled fashion. This pattern has evolved into a powerful software paradigm and covers a wide array of use cases. Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. With this industry evolution, we can now create event-driven business processes which can work seamlessly in a microservices-based environment. With the latest release of Red Hat Process Automation Manager (7.11), you can work with business processes capable of interacting with external services via events, either by emitting or consuming them. Earlier this year,  wrote a detailed  about the integration between Red Hat Process Automation Manager and Kafka. In this article, we will look at the integration of Red Hat Process Automation with Red Hat AMQ Streams on OpenShift. Red Hat AMQ Streams is an enterprise grade Kubernetes-native Kafka solution. In IT today, it can be both challenging and time-consuming for operations and development teams to be experts in many different technologies knowing how to use them, while also knowing how to install, configure, and maintain them. Kubernetes operators help streamline the installation, configuration, and maintenance complexities. We will be using the AMQ Streams Operator and the Business Automation Operator to help simplify the process. In this article you’ll see how to deliver and test an event-driven process application on OpenShift in four steps: 1. Kafka deployment on OpenShift 2. Red Hat Process Automation Manager deployment on OpenShift 3. Creation and deployment of the business application 4. Test of the business application using events To learn more check with a detailed step-by-step guide. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/kne3WIvD6EM" height="1" width="1" alt=""/&gt;</content><dc:creator>Sadhana Nandakumar</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html</feedburner:origLink></entry><entry><title>Deploy Node.js applications to Red Hat OpenShift with Helm</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/EN_yHQH-Iz8/deploy-nodejs-applications-red-hat-openshift-helm" /><author><name>Ash Cripps</name></author><id>20cbf5cf-69e0-4305-9cd5-16cb7dfb1e46</id><updated>2021-07-20T07:00:00Z</updated><published>2021-07-20T07:00:00Z</published><summary type="html">&lt;p&gt;There are many different ways to deploy your &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; applications to &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. This article shows you how to deploy a Node.js application using Helm, along with some recent additions to OpenShift.&lt;/p&gt; &lt;h2&gt;What is Helm?&lt;/h2&gt; &lt;p&gt;&lt;a&gt;Helm&lt;/a&gt; is a package manager for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, which you can use to define, install, and upgrade all types of Kubernetes applications. You can think of Helm as an operating system packager (such as &lt;code&gt;apt&lt;/code&gt; or &lt;strong&gt;yum&lt;/strong&gt;) but for Kubernetes. With Helm, you package your Kubernetes application into a &lt;em&gt;chart&lt;/em&gt;, which is a series of files that define the Kubernetes resources for your deployment. You can use Helm for a variety of scenarios—from very simple applications to complex ones with many dependencies.&lt;/p&gt; &lt;p&gt;Helm offers a fast and effective way for you and your customers to automate Node.js application deployments. Helm also supports Go, which allows for greater chart customization depending on user-specified values. (You can turn certain features on or off depending on the values.) For more information, see the &lt;a href="https://helm.sh/docs/"&gt;Helm documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can use Helm to deploy applications to any Kubernetes environment through the command line. It’s often as easy as &lt;code&gt;helm install XYZ&lt;/code&gt;. However, in OpenShift we’ve worked to make it even easier. There are now two ways to deploy applications with Helm using the OpenShift user interface (UI).&lt;/p&gt; &lt;p&gt;We'll start with a Helm chart template that was recently made available on OpenShift. You can use the template to deploy your Node.js application to OpenShift via Helm as a starter, and then customize it to create your own Helm chart. While you can also use this template to deploy to Kubernetes, it includes OpenShift extensions that make deployments easier in that environment.&lt;/p&gt; &lt;p&gt;In the next sections, I will show you how to use the Helm chart template to deploy a Node.js application to OpenShift with just a few clicks. After that, we'll talk through the chart's implementation, and I'll show you how to package up your own Helm chart and add it to the OpenShift developer catalog.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can also add your own Helm charts to OpenShift by using a custom resource (CR) to &lt;a href="https://docs.openshift.com/container-platform/4.6/cli_reference/helm_cli/configuring-custom-helm-chart-repositories.html"&gt;create a new Helm chart repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Setting up the demonstration&lt;/h2&gt; &lt;p&gt;To follow this demonstration, you will need access to an OpenShift cluster. I'm using &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt;, which allows me to run a single-node OpenShift cluster locally. It doesn’t have all the features of an OpenShift cluster, but it has everything we need for this article. Alternatively, You could use the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which requires a Red Hat account.&lt;/p&gt; &lt;p&gt;You will also need a Node.js application that can be containerized. If you don't have one, you can use the sample program &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy a simple application with Helm on OpenShift&lt;/h2&gt; &lt;p&gt;The Helm chart that I added allows you to easily deploy a simple application with Helm through the OpenShift user interface, without having to write your own chart. Let's go through the steps together.&lt;/p&gt; &lt;h3&gt;Step 1: Select Helm Chart from the project's topology view&lt;/h3&gt; &lt;p&gt;First, you want to be inside the OpenShift console's developer view. From there, click on the topology view in the left-hand menu. You will be presented with the developer catalog, featuring a variety of deployment options. Go ahead and click on Helm Chart, as shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-1.png?itok=ztMJgYH2" width="600" height="316" alt="The OpenShift developer catalog with the Helm Charts option highlighted." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Select Helm Chart as your deployment option. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 2: Select the Node.js chart&lt;/h3&gt; &lt;p&gt;From within the Helm Chart section of the developer catalog, select the Node.js chart, which is highlighted in Figure 2. Then, click &lt;strong&gt;Install Helm Chart&lt;/strong&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-2.png?itok=vAU5HzGH" width="600" height="325" alt="The Helm Charts section of the developer catalog with the Node.js option highlighted." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Select the Node.js Helm chart from the developer catalog. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 3: Configure the Helm release&lt;/h3&gt; &lt;p&gt;Now, you can configure the values that will be implanted in your Helm release. OpenShift gives you two ways to input values, using either the user-friendly form view or the YAML view. For the purpose of this demonstration, we'll use the form view.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Not all of the available values are presented in the form view, so the YAML view gives you more control.&lt;/p&gt; &lt;p&gt;In the form shown in Figure 3, I name my release &lt;code&gt;nodejs-example&lt;/code&gt;, set my image name as &lt;code&gt;my-node-image&lt;/code&gt;, and input the URL of my source code’s Git repository. For this example, I'm using the &lt;code&gt;nodejs-http-rest&lt;/code&gt; example from &lt;code&gt;nodeshift-starters&lt;/code&gt;, but feel free to use your own repository if you wish.&lt;/p&gt; &lt;p&gt;I'll leave the rest of the options set to default (blank) for now, but you can change them. For example, you might want to pull from a different Node source image for the &lt;a href="https://docs.openshift.com/container-platform/4.7/openshift_images/using_images/using-s21-images.html"&gt;S2I&lt;/a&gt; builder. Figure 3 shows my completed form.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-3.png?itok=I9FbUWAF" width="600" height="326" alt="The completed Helm Chart form." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Example of how to fill out the Node.js Helm chart form. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 4: Install and track the build&lt;/h3&gt; &lt;p&gt;Once you have filled in the required values, go ahead and click &lt;strong&gt;Install&lt;/strong&gt;. At first, you will notice the pod within your deployment is reporting &lt;code&gt;ErrImagePull&lt;/code&gt;. This is &lt;em&gt;normal&lt;/em&gt;. The pod is unable to pull down your image because it hasn't been built yet! You can track your image's progress under the &lt;strong&gt;Builds&lt;/strong&gt; section of your deployments submenu, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-4.png?itok=bPujM4w2" width="600" height="312" alt="Pods spinning up from the Helm chart." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The OpenShift topology overview, showing the pods spinning up from the Helm chart deployment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Congratulations—you've just deployed your Node.js application to OpenShift via Helm! To view your application, click the &lt;strong&gt;open url&lt;/strong&gt; button in the top right corner of the deployment in the topology view.&lt;/p&gt; &lt;h2&gt;Customize your Helm chart&lt;/h2&gt; &lt;p&gt;For this section, I assume you already have a basic understanding of how Helm charts are structured, so we won't dig deep into the base files. Instead, we will explore the OpenShift-specific files you wouldn't normally find in a standard &lt;a href="https://helm.sh/docs/chart_template_guide/getting_started"&gt;Helm chart&lt;/a&gt;. We'll explore the following files, which you can use to develop a custom Helm chart:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;values.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;buildconfig.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;imagestream.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;route.yaml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Note that the &lt;code&gt;buildconfig.yaml&lt;/code&gt;, &lt;code&gt;imagestream.yaml&lt;/code&gt;, and &lt;code&gt;route.yaml&lt;/code&gt; files are all specific to OpenShift.&lt;/p&gt; &lt;h3&gt;values.yaml&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;values.yaml&lt;/code&gt; file is very important as it is the one your user will interact with, either directly or through the UI. So, you need to make sure it is easy to follow and that you have enough values to cover all the customization use cases you wish your user to have.&lt;/p&gt; &lt;h3&gt;buildconfig.yaml&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;buildconfig.yaml&lt;/code&gt; is the first OpenShift-specific file in the Helm chart. Let's have a look at the file for the sample Node.js chart:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;{{- if .Values.build.enabled }} apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: {{ include "nodejs.name" . }} labels: {{- include "nodejs.labels" . | nindent 4 }} spec: source: type: Git git: uri: {{ .Values.build.uri }} ref: {{ .Values.build.ref }} {{- if .Values.build.contextDir }} contextDir: {{ .Values.build.contextDir }} {{- end }} strategy: type: Source sourceStrategy: from: kind: ImageStreamTag namespace: {{ .Values.build.source.namespace }} name: {{ .Values.build.source.name }} {{- if .Values.build.pullSecret }} pullSecret: name: {{ .Values.build.pullSecret }} {{- end }} {{- if .Values.build.env }} env: {{- tpl (toYaml .Values.build.env) . | nindent 8 }} {{- end }} output: to: kind: {{ .Values.build.output.kind }} name: {{ include "nodejs.imageName" . }} {{- if and (eq .Values.build.output.kind "DockerImage") .Values.build.output.pushSecret }} pushSecret: name: {{ .Values.build.output.pushSecret }} {{- end }} {{- if .Values.build.resources }} resources: {{- toYaml .Values.build.resources | nindent 4 }} {{- end }} triggers: - type: ConfigChange {{- end }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing you'll notice is &lt;code&gt;{{- if .Values.build.enabled }}&lt;/code&gt;, which is an &lt;code&gt;if&lt;/code&gt; conditional in Golang templating. It allows the user to specify if they wish to build the image themselves using this config or use an image already in the OpenShift instance. You can also use this option to turn the OpenShift-specific features on or off. This option is useful if you want to be able to deploy your Helm chart to a vanilla Kubernetes environment.&lt;/p&gt; &lt;p&gt;The next line to look at is &lt;code&gt;apiVersion: build.openshift.io/v1&lt;/code&gt;. This line specifies that the file is OpenShift-specific and uses the OpenShift API to build a deployable image.&lt;/p&gt; &lt;p&gt;The next key section is the &lt;code&gt;source&lt;/code&gt; section under &lt;code&gt;spec&lt;/code&gt;. This section, as the name hints, is where you specify the source of the program: What Git repo and which reference should be checked out.&lt;/p&gt; &lt;p&gt;Next up, we specify what &lt;a href="https://docs.openshift.com/container-platform/4.7/cicd/builds/build-strategies.html"&gt;strategy&lt;/a&gt; we will use to build the image. For this Helm chart, I used the source-to-image (S2I) build strategy but you could choose to use a Docker strategy or a custom build. Inside the &lt;code&gt;strategy&lt;/code&gt; block, I specify that I want to build from an &lt;code&gt;ImageStreamTag&lt;/code&gt; and then have variables for the namespace, name, and if the source has a pull secret or not. The user can also use the &lt;code&gt;strategy&lt;/code&gt; block to specify if they have environment variables for the build.&lt;/p&gt; &lt;p&gt;Finally, we see the &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;resources&lt;/code&gt; blocks. The &lt;code&gt;resources&lt;/code&gt; block is where the user can specify if they want to limit the resources (such as CPU and memory) to be available to the pod once it is built. The &lt;code&gt;output&lt;/code&gt; block is where the user specifies what type of output they would like (I defaulted to &lt;code&gt;ImageStreamTag&lt;/code&gt; for this example), the name of the output, and the push secret (if needed) to upload the image.&lt;/p&gt; &lt;h3&gt;imagestream.yaml&lt;/h3&gt; &lt;p&gt;Now, let's have a look at the &lt;code&gt;imagestream.yaml&lt;/code&gt; file, which is another OpenShift-specific file. It's a fairly simple file where you just specify the kind of image stream and its name and labels. For more information about image streams please refer to the &lt;a href="https://docs.openshift.com/container-platform/4.7/openshift_images/image-streams-manage.html"&gt;OpenShift documentation for image streams&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;route.yaml&lt;/h3&gt; &lt;p&gt;Lastly, let's take a look at the &lt;code&gt;route.yaml&lt;/code&gt; file. This file is used to set up the routes for your application within OpenShift. Here is where you will do things like establishing a TLS connection, and specifying ports and certificates. You can use this file to expose your application in OpenShift without having to mess around with port forwarding like you would in standard Kubernetes.&lt;/p&gt; &lt;h2&gt;Package and deploy the Helm chart&lt;/h2&gt; &lt;p&gt;Once you have your Helm chart completed, you'll need to package it into a .tar file and upload it to your repository of choice. You can then deploy it to OpenShift via a custom resource definition (CRD), like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOF | oc apply -f - apiVersion: helm.openshift.io/v1beta1 kind: HelmChartRepository metadata: name: $name spec: name: $name connectionConfig: url: https://raw.githubusercontent.com/$org/$repo/$ref EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The CRD will show up under &lt;strong&gt;Custom Resource Definitions&lt;/strong&gt; in the OpenShift administrator view, and the chart itself will show up in the OpenShift developer catalog. You will be able to select the chart from the catalog to deploy your application.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has been an introduction to deploying Node.js applications to OpenShift via Helm. You saw how to use an example Helm chart or build your own chart to deploy a Node.js application.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/20/deploy-nodejs-applications-red-hat-openshift-helm" title="Deploy Node.js applications to Red Hat OpenShift with Helm"&gt;Deploy Node.js applications to Red Hat OpenShift with Helm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/EN_yHQH-Iz8" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ash Cripps</dc:creator><dc:date>2021-07-20T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/20/deploy-nodejs-applications-red-hat-openshift-helm</feedburner:origLink></entry><entry><title type="html">Eclipse Vert.x 4.1.2 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jrlC_Sar5RE/eclipse-vert-x-4-1-2" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-1-2</id><updated>2021-07-20T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.1.2 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jrlC_Sar5RE" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/eclipse-vert-x-4-1-2</feedburner:origLink></entry><entry><title type="html">Add SQL datasource for authoring dashboards</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cAw05RDXDds/add-sql-datasource-for-authoring-dashboards.html" /><author><name>Manaswini Das</name></author><id>https://blog.kie.org/2021/07/add-sql-datasource-for-authoring-dashboards.html</id><updated>2021-07-19T14:37:29Z</updated><content type="html">Dashboards are quintessential ways of gaining precise and accurate at-a-glance insights into complex data indicating key performance indicators(KPIs), metrics, and other key data points related to business or specific processes. is a one such standalone tool that is also integrated into Business Central and is used by the Datasets editor and Content Manager page to facilitate creating dashboards and reporting. You can get started by referring to the , if you are a first-time user. Refer to for configuring CSV datasets for authoring dashboards on DashBuilder. In the , we walked you through the process of adding Prometheus datasets for authoring dashboards in DashBuilder. When it comes to building dashboards, you can configure the dashboards to consume your own datasets in DashBuilder from a variety of sources like Bean, CSV, SQL, Prometheus, Elastic Search, Kafka, and Execution server. In this post, you will learn how to add and configure a SQL dataset for your dashboards. ABOUT SQL SQL or Structured Query Language is a domain-specific language used in programming and designed for managing data held in a relational database management system (RDBMS), or for stream processing in a relational data stream management system (RDSMS). It is particularly useful in handling structured data, i.e. data incorporating relations among entities and variables. ADD AND CONFIGURE SQL DATASETS ON DASHBUILDER 1. To get started, you will have to configure DashBuilder to run on the WildFly server since it is impossible to get the SQL data set configured in Dev mode as in Dev mode all WildFly configurations are lost when we rebuild the project. For this example, I have used WildFly 19.x. and the to deploy in the WildFly server. Unzip the WildFly zip and open a terminal inside the WildFly unzipped folder(let’s call this WILDFLY_HOME). Go to /bin and run standalone.sh using ./standalone.sh or sudo sh standalone.sh. You can now find the WildFly server running in localhost:9990 and see the following screen which prompts to add a user. WildFly page that you can see after typing localhost:9990 on your browser 2. Open another terminal and run ./add-user.sh and add a ManagementRealm user by selecting option “a” and adding a username and password, followed by adding an “admin” group following the instructions flashing on the terminal. Here is what the terminal should look like after you add a user. Add ManagementRealm user Note: You have to add a ManagementRealm user to login into WildFly, the applicationRealm users are the users you will need to login into the apps deployed in WildFly. 3. Click on the “Try again” link on the browser and login the same credentials that you configured in the terminal. You will now be able to see the HAL Management Console screen. Management Console 4. Now click on the Start link under Deployments and deploy the DashBuilder WAR you downloaded earlier. Refer to the GIF below for reference. You can click on the link against “Context Root” in order to access DashBuilder. Deploy DashBuilder WAR on WildFly 5. Now add a ApplicationRealm user in order to access DashBuilder by running ./add-user.sh -a -u ‘admin’ -p ‘admin’ -g ‘admin’ in a terminal window inside the bin folder of WILDFLY_HOME. After the user is successfully added, you will now be able to use the above credentials to login into DashBuilder. 6. Time to add a MySQL JDBC driver to our WildFly server. Download the JDBC driver for MySQL(Connector/J) from the . I’m using MySQL Connector/J 8.0.17. MySQL Connector/J 8.0 is compatible with all MySQL versions starting with MySQL 5.5. Download MySQL Connector/J 8.0 at ‘ /opt’ directory using below commands: sudo cd /opt sudo wget Extract the tarball using below command: sudo tar -xvzf mysql-connector-java-8.0.17.tar.gz Inside the /opt/mysql-connector-java-8.0.17/ directory that we extracted, there is a jar file by the name mysql-connector-java-8.0.17.jar. This jar file contains the required classes for the MySQL JDBC driver. 7. Try creating the Module itself using the ./jboss-cli.sh command inside the bin folder of WILDFLY_HOME rather than manually writing the module.xml file. This is because when we use some text editors, they might append some hidden chars to our files. (Especially when we do a copy &amp;amp; paste in such editors). Run connect when prompted(Note:provided the WildFly server must be running on another terminal tab). [standalone@localhost:9990 /] module add — name=com.mysql.driver — dependencies=javax.api,javax.transaction.api — resources=/PATH/TO/mysql-connector-java-5.1.35.jar [standalone@localhost:9990 /] :reload {“outcome” =&gt; “success”,“result” =&gt; undefined} After running above command you should see the module.xml generated in the following location: wildfly-19.0.0.Final/modules/com/mysql/driver/main/module.xml Now create DataSource: [standalone@localhost:9990 /] /subsystem=datasources/jdbc-driver=mysql/:add(driver-module-name=com.mysql.driver,driver-name=mysql,jdbc-compliant=false,driver-class-name=com.mysql.jdbc.Driver) {“outcome” =&gt; “success”} OR You can also choose to manually add the driver by downloading and putting it inside WILDFLY_HOME/modules/system/layers/base/com/mysql/main. Create a module.xml inside the same folder where you have the JAR. Using any kind of text editor, create file inside your WildFly path, WILDFLY_HOME/modules/system/layers/base/com/mysql/main, and this is the XML file contents of it: &lt;module name=”com.mysql.driver” xmlns=”urn:jboss:module:1.5"&gt; &lt;resources&gt; &lt;resource-root path=”mysql-connector-java-8.0.17.jar”&gt; &lt;/resource-root&gt; &lt;/resources&gt; &lt;dependencies&gt; &lt;module name=”javax.api”&gt; &lt;module name=”javax.transaction.api”&gt; &lt;/module&gt; &lt;/module&gt; &lt;/dependencies&gt; &lt;/module&gt; Add MySQL connector to the driver list Open WILDFLY_HOME/standalone/configuration/standalone.xml, and then find &lt;drivers&gt; tag, inside that tag, put these lines to add MySQL driver: &lt;driver name=”mysql” module=”com.mysql.driver”&gt; &lt;driver-class&gt;com.mysql.cj.jdbc.Driver&lt;/driver-class&gt; &lt;/driver&gt; Now you can restart WildFly and expect that the new driver will be inside the available list driver. Click on the Start button under Configuration in the Homepage of the Management Console. Now click on Subsystems -&gt; Datasources &amp;amp;Drivers -&gt; JDBC Drivers and you can see “mysql” added. 8. Time to have the MySQL configured in our local system. if you haven’t yet. I’m using MySQL 8.0.18 for this example.Run mysql -u root -p and enter the password. Create a database called “testdb” and create a table that you will use for your dashboards using the appropriate SQL queries. I have added an example for your reference. Example to create a table 9. Now you will have to create a user and grant all privileges to the user. Run CREATE USER ‘username’@’localhost’ IDENTIFIED BY ‘password’followed by GRANT ALL PRIVILEGES ON testdb.* TO ‘username’@’localhost’; You will now see Query OK, 0 rows affected (0.01 sec) on your terminal after running both the commands. Note: Just replace the username and password with your own, don’t remove the quotes. We are good with the local MySQL dataset configuration now. 10. Now, let’s add the datasource on the management console. Click on “+” in the Datasource section. Select MySQL. Click on Next and change the JNDI to java:jboss/datasources/MySqlDS. Don’t change anything in the JDBC driver screen. In the Connection screen, change the database name in the Connection URL from “mysqldb” to “testdb” and add the username and password. Click on test connection in the Next screen and if successful, click on Finish. Add SQL datasource Troubleshooting: * The JNDI name cannot be changed once configured. If you want to change that go to standalone.xml inside standalone/configuration in WILDFLY_HOME. Search for the &lt;datasource&gt; tag and change the JNDI name corresponding to the required data source there. You will have to restart the server after you change this file. Make sure the JNDI name contains jboss/datasources similar to the ExampleDS datasource configuration in standalone.xml otherwise the datasource can’t be detected. * Don’t add “@localhost” while adding username and the password in the Connection tab, else you may get an wrong/username and password error. Error while configuring driver * You may see the above error in the JDBC tab. This only shows if the driver hasn’t been configured properly. Make sure it is configured correctly. You can always access the live logs in server.log inside standalone/configuration/log in WILDFLY_HOME to know the exact issue. 11. You can now access the Configured datasource inside DashBuilder. Go to Deployments tab and click on the link against “Context Root”. Log in to DashBuilder using the ApplicationRealm user credentials you created in the beginning. You will see the homepage which resembles the screen below. DashBuilder Home page 12. In order to add a dataset, select Datasets from the menu. Alternatively, you can also click on the “Menu” dropdown on the top left corner beside the DashBuilder logo on the navigation bar. You will now see two headings, “DashBuilder” and “Administration”. Click on “Datasets” under Administration. You will see the Dataset Authoring home page with instructions to add datasets, create displayers and new dashboards. Data Set Authoring Home page 13. Now, you can either click on the “New Data Set” on the top left corner beside “Data Set Explorer” or click on the “new data set” hyperlink in Step 1. You will now see a Data Set creation wizard screen that shows all dataset providers like Bean, CSV, SQL, Elastic Search, and so on. Data Set Creation Wizard Page 14. Select SQL from the list of provider types. You will now see the following screen to add your SQL dataset. Add a name. In the Data Source dropdown, select MySqlDS(the name that you used while configuring data source in the Management console) and add source as shown in the screenshot below. SQL Configuration page If you are confused about the role of the fields, please hover on the question mark icons beside the fields or the text boxes adjacent to them. Click on Test to preview your dataset. 15. You are now on the Preview tab. You can now have a look at the data columns and add filters in the tabs above the dataset. You can also edit the types or delete the columns that you don’t require by unchecking the checkbox beside the columns provided on the left side. If the preview isn’t what you expected, you can switch back to the Configuration tab by clicking on it and make changes. If you are satisfied with the preview, click on the “Next” button. 16. Enter required comments in the Save modal and click on the Save button. Your dataset has been added and configured. You will now be directed back to the Data Set Authoring Home page. You can now see a dataset on the left pane. When you add multiple datasets, you can see the list of all of them on the left pane. Here is a screen recording of the full flow. Adding and configuring SQL datasets on DashBuilder You can now click on the Menu dropdown on the navigation bar and select Content Manager to create pages and dashboards. Ensure that the columns are well configured with proper types in the “Data” section of the “Displayer Editor” after dragging the component. CONCLUSION With the help of this post, you will be able to add and configure data from a SQL dataset to be consumed by your dashboards. Feel free to add your comments regarding where you got stuck, so that we can improve and update this guide going further. In the upcoming posts, we will add walkthroughs of the remaining dataset providers, so stay tuned! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cAw05RDXDds" height="1" width="1" alt=""/&gt;</content><dc:creator>Manaswini Das</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/add-sql-datasource-for-authoring-dashboards.html</feedburner:origLink></entry><entry><title type="html">Kogito 1.8.0 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/iO17c297Fb4/kogito-1-8-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>http://feeds.athico.com/~r/droolsatom/~3/f9UPat8eHJM/kogito-1-8-0-released.html</id><updated>2021-07-19T14:07:32Z</updated><content type="html">We are glad to announce that the Kogito 1.8.0 release is now available! This goes hand in hand with and. From a feature point of view, we included a series of new features and bug fixes, including: * Infinispan required version has been upgrade to 12.1.4, including Data Index and Jobs services * New runtime persistence addon using generic JDBC Driver. * Jobs Service support for persistence using MongoDB  * Data Index Service support for persistence using PostgreSQL  * We now have a dedicated extension for Kogito Processes (kogito-quarkus-processes) on Quarkus Runtimes in addition to the platform extension (kogito-quarkus) * It is now possible to use application.properties to provide Drools knowledge builder properties * Monitoring dashboards now report the version of Kogito runtime so that it is possible to filter by version * Support for Boundary Error Events BREAKING CHANGES * The add-ons have been restructured! Please to know more about it. You will need to update your dependencies KNOWN ISSUES * Our team have identified an issue ) that currently prevents the usage of the different persistent addons on Quarkus native images. For more details head to the complete. All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found. * Kogito images are available on. * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.11.0 artifacts are available at the. A detailed changelog for 1.8.0 can be found in. New to Kogito? Check out our website. Click the "Get Started" button. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/iO17c297Fb4" height="1" width="1" alt=""/&gt;</content><dc:creator>Cristiano Nicolai</dc:creator><feedburner:origLink>http://feeds.athico.com/~r/droolsatom/~3/f9UPat8eHJM/kogito-1-8-0-released.html</feedburner:origLink></entry><entry><title>Benchmarking Kafka producer throughput with Quarkus</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/uCN7rSLmt0A/benchmarking-kafka-producer-throughput-quarkus" /><author><name>Berker Agir</name></author><id>63bec0af-e65a-44f6-9082-752ea7c560e0</id><updated>2021-07-19T07:00:00Z</updated><published>2021-07-19T07:00:00Z</published><summary type="html">&lt;p&gt;The interest in &lt;a href="https://developers.redhat.com/topics/event-driven"&gt;event-driven architecture&lt;/a&gt; has sped up within the last couple of years, with a great level of adoption and modernization effort across all enterprises. &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt;, one of the most pervasive streaming middleware technologies, is being tried and tested by many development teams. High performance is a critical goal for these teams.&lt;/p&gt; &lt;p&gt;There are numerous resources for configuring and benchmarking your Kafka cluster size. Such guides and benchmarks naturally involve producers and consumers, but their primary aim is the performance of the Kafka cluster itself. Recently, I was asked to give pointers regarding how to fine-tune Kafka producers for high throughput. Many guides explain the most important Kafka producer configurations and their relationship to performance, as well as the trade-offs. But there isn’t much benchmark data showcasing how different configuration combinations can impact producer message throughput.&lt;/p&gt; &lt;p&gt;In this article, I show the throughput outcomes resulting from various producer configurations I employed in a recent test setup. My hope is to help other developers and architects better understand the relationship between producer configurations and message throughput. You can use this information to make educated guesses while configuring your own Kafka clusters.&lt;/p&gt; &lt;h2&gt;The test environment&lt;/h2&gt; &lt;p&gt;You will need the following technologies if you want to reproduce my test environment:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/products/quarkus/"&gt;Quarkus&lt;/a&gt; 1.11.1 (compiled as Java application with &lt;a href="https://developers.redhat.com/products/openjdk/overview"&gt;OpenJDK 11&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/openshift"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; 4.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/amq/download"&gt;Red Hat AMQ Streams&lt;/a&gt; 7.7 on &lt;a href="https://developers.redhat.com/products/openshift"&gt;Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; 2.6&lt;/li&gt; &lt;li&gt;&lt;a href="https://prometheus.io/download/"&gt;Prometheus&lt;/a&gt; 2.16&lt;/li&gt; &lt;li&gt;&lt;a href="https://grafana.com/grafana/download"&gt;Grafana&lt;/a&gt; 7.1.1&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Note that I used the Red Hat-supported operators to deploy AMQ Streams, Prometheus, and Grafana. Additionally, you cannot replicate the work explained in this article on OpenShift Container Platform versions earlier than 4.5 or &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; 1.18.&lt;/p&gt; &lt;h3&gt;Setting up the test environment on OpenShift&lt;/h3&gt; &lt;p&gt;I conducted my tests with a simple Quarkus application. In a nutshell, I deployed a Kafka cluster using the Red Hat AMQ Streams 7.7 distribution on an OpenShift Container Platform 4.5 cluster. I also deployed a Prometheus instance in order to collect metrics from both the Quarkus (&lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;) application and the Kafka cluster, and a Grafana instance.&lt;/p&gt; &lt;p&gt;In the tests, the application sends bursts of messages to the Kafka cluster for a period of time. I initiated the requests with different combinations of producer configurations on the application side and observed the metrics on a Grafana dashboard configured to show data from Kafka producers.&lt;/p&gt; &lt;p&gt;My test setup consists of three projects, or namespaces, in the OpenShift cluster:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;kafka-cluster&lt;/strong&gt;: Where AMQ Streams is deployed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;application-monitoring&lt;/strong&gt;: Where the Prometheus and Grafana instances are deployed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;kafka-performance-test&lt;/strong&gt;: Where the Kafka client is deployed (on one pod).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;I built my Quarkus application using OpenJDK 11 and deployed it using the &lt;a href="https://github.com/redhat-developer/redhat-helm-charts/tree/master/alpha/quarkus-chart"&gt;Quarkus helm chart&lt;/a&gt; and Helm chart values found &lt;a href="https://github.com/berkeragir/quarkus-kafka-demo/blob/main/deploy/quarkus-helm-values.yaml"&gt;here&lt;/a&gt;. You will need an image pull secret in order to use the same &lt;a href="https://github.com/berkeragir/quarkus-kafka-demo/blob/main/deploy/openjdk11-imagestream.yaml"&gt;OpenJDK image stream&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Setting up the Kafka cluster&lt;/h3&gt; &lt;p&gt;There are two ways client applications can connect to the Kafka brokers. One way is to set up AMQ Streams to create OpenShift routes for external access to the Kafka brokers. This approach is best if the clients are deployed outside the OpenShift cluster, or if they are deployed in different network zones and you want the network traffic to flow through a firewall.&lt;/p&gt; &lt;p&gt;The other way is to enable the network from the &lt;code&gt;kafka-performance-test&lt;/code&gt; project to the &lt;code&gt;kafka-cluster&lt;/code&gt; project. This is achieved with the &lt;code&gt;NetworkPolicy&lt;/code&gt; type of resources. By default, the AMQ Streams Operator creates network policies to allow incoming traffic throughout the OpenShift cluster on certain ports. I relied on this default behavior in my current setup. (Note that you might need to have the right roles to perform these deployments.)&lt;/p&gt; &lt;p&gt;My Kafka cluster consists of three brokers, each with two CPU limits and 6 GB of memory. I deployed them with persistent storage. &lt;a href="https://github.com/berkeragir/quarkus-kafka-demo/blob/main/deploy/kafka-cluster.yaml"&gt;Here&lt;/a&gt; is the custom resource (CR) for deploying the Kafka cluster with Strimzi.&lt;/p&gt; &lt;p&gt;I created a &lt;a href="https://github.com/berkeragir/quarkus-kafka-demo/blob/main/deploy/kafkatopic.yaml"&gt;topic&lt;/a&gt; using the &lt;code&gt;KafkaTopic&lt;/code&gt; custom resource provided with the AMQ Streams Operator. The topic configuration is three partitions with three replicas.&lt;/p&gt; &lt;h3&gt;Prometheus and Grafana&lt;/h3&gt; &lt;p&gt;Finally, I deployed Prometheus and a Grafana instance in order to collect metrics from my application and analyze the data. You can find all the resources I created related to monitoring &lt;a href="https://github.com/berkeragir/quarkus-kafka-demo/tree/main/deploy/monitoring"&gt;here&lt;/a&gt;. All of them are created in the &lt;code&gt;application-monitoring&lt;/code&gt; namespace. I labeled my application with “&lt;code&gt;monitor: 'true'&lt;/code&gt;” as I configured the related PodMonitor to target pods with that label.&lt;/p&gt; &lt;h2&gt;Benchmark configurations&lt;/h2&gt; &lt;p&gt;The first test I performed was with the default producer configurations. Subsequently, I focused on a few fundamental configurations and tested the client application with combinations of different values for these. The application exposes an API endpoint to receive a payload to send to Kafka. The application then commits the same message, in the desired number, to a Kafka emitter serially. In each test scenario, I sent parallel requests with different payloads and the desired number of messages to this endpoint in a loop.&lt;/p&gt; &lt;p&gt;Here are the configurations I played with for my producer application:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;batch.size in bytes&lt;/strong&gt;: Does not take effect unless &lt;code&gt;linger.ms&lt;/code&gt; is non-zero. This lets the producer package messages and send them together, which reduces the number of requests to the cluster.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;linger.ms in milliseconds&lt;/strong&gt;: Determines how long a message will be buffered in the current batch until the batch is sent. In other words, the producer will send the current batch either when &lt;code&gt;batch.size&lt;/code&gt; is reached or the &lt;code&gt;linger.ms&lt;/code&gt; amount of time has passed since the batch started to be filled.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;compression.type&lt;/strong&gt;: For applications that produce messages of big sizes, compression can help improve the throughput.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;acks&lt;/strong&gt;: As a business requirement, you might need to replicate messages across your Kafka cluster. In some cases, you might need to acknowledge all replicas; in others, it might be enough to get acknowledgment only from the original node. I also looked at the impact of this configuration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU limit&lt;/strong&gt;: The computational power of the client application considerably impacts the throughput from the producer’s perspective. Though I won't focus on this parameter here, I tried to give some insight.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Other noteworthy parameters are &lt;code&gt;buffer.memory&lt;/code&gt; and &lt;code&gt;max.block.ms&lt;/code&gt;. Producers take both into account regarding the send buffer. The &lt;code&gt;buffer.memory&lt;/code&gt; parameter is by default 32 MB (or 33,554,432 bytes). I will leave analyzing the effect of these parameters to future work. For the curious, the Kafka Producers Grafana dashboard I provided has the metric of how the producers use the buffer.&lt;/p&gt; &lt;p&gt;I used default values for all the remaining configurations.&lt;/p&gt; &lt;p&gt;For different test scenarios, I generated random JSON dictionaries to use as payloads. I used a combination of these in looped requests to my Kafka producer in order to achieve different average payload sizes, which serve as an additional test parameter in my experiments. I generated different average payload sizes of 1.1 KB and 6 KB. The former is achieved with a mixture of small and medium payload sizes, and the latter with a mixture of medium and large payloads.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: My benchmarks do not necessarily include the best possible combination for a certain scenario, so don’t pay attention to the numbers, but rather to the trends.&lt;/p&gt; &lt;h2&gt;Analyzing producer message throughput&lt;/h2&gt; &lt;p&gt;Let's start by considering an overview of what the producer message throughput looks like with the default configuration, shown in Table 1.&lt;/p&gt; &lt;div&gt; &lt;table border="1" cellpadding="0" cellspacing="0" width="500"&gt;&lt;caption&gt;Table 1: Kafka producer message throughput with default configurations.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th scope="col"&gt; &lt;p&gt;&lt;strong&gt;Default values&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;th scope="col"&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;batch.size&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;16,384&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;linger.ms&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;0&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;acks&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;1&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;compression.type&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;none&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;&lt;strong&gt;Average throughput&lt;/strong&gt; (with 1.1 KB average payload and one CPU)&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;~14,700&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;&lt;strong&gt;Average throughput&lt;/strong&gt; (with 1.1 KB average payload and 0.5 CPU)&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;~6,700&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;&lt;strong&gt;Average throughput&lt;/strong&gt; (with 6 KB average payload and one CPU)&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;~6,000&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;&lt;strong&gt;Average throughput&lt;/strong&gt; (with 6 KB average payload and 0.5 CPU)&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;~4,000&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;p&gt;The producer message throughput is around 14,700 on average for an average payload size of 1.1 KB, when running on one core. For a larger average payload size of 6 KB, the throughput is unsurprisingly lower at approximately 6,000. When I decreased the number of cores allocated to my application to 0.5, I observed drastically lower throughput for each payload size. Consequently, we can see that CPU alone dramatically impacts the producer’s message throughput.&lt;/p&gt; &lt;h3&gt;Producer message throughput over batch size&lt;/h3&gt; &lt;p&gt;Now, let’s see how the numbers change when we start playing with the aforementioned parameters. Figure 1 consists of four graphs that show throughput over various batch sizes under different scenarios. The top-row plots are detailed by &lt;code&gt;linger.ms&lt;/code&gt;, and the bottom row by average payload size. The left column shows the results without any compression employed, and the right column shows the results with compression of the type “snappy."&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Figure 1: Message throughput over batch size with different details, and compression types (left column: none, right column: snappy)." data-entity-type="file" data-entity-uuid="1c4b6baf-485f-4bf5-b7fa-79540b2a0e1a" src="https://developers.redhat.com/sites/default/files/inline-images/plot-throughput-by-comp-by-payload_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1: Message throughput over batch size with different details, and compression types.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;It is obvious at first glance that in this particular test, the default configuration does not provide the best throughput. By setting &lt;code&gt;linger.ms&lt;/code&gt; to a positive value, and thus enabling batching, we immediately gain an increase in throughput of 10% to 20%. However, as a reminder, this outcome is for this particular experiment, with average payload sizes of 1.1 KB and 6 KB in different test scenarios. When the compression type is none, increasing the batch size, together with setting &lt;code&gt;linger.ms&lt;/code&gt; to a non-zero value, improves the throughput with respect to the default configuration scenario. All the 1.1 KB payload scenarios demonstrate approximately 15,500 to 17,500 message throughput: A considerable improvement from the default configuration.&lt;/p&gt; &lt;p&gt;When compression is none, &lt;code&gt;linger.ms&lt;/code&gt; has no significant impact. The differences between results with employed &lt;code&gt;linger.ms&lt;/code&gt; values in these experiments might be circumstantial because there does not seem to be a pattern. This aspect would require further research and experiments. Remember that &lt;code&gt;linger.ms&lt;/code&gt; sets how much time a producer will wait till the producer batch is filled before transmitting the batch to the Kafka brokers. Setting this parameter to a very high number should not be an option to consider unless the application architecture and use case fit it. Nevertheless, the values I used—10, 20, and 40—are quite typical, and you can always start with such settings for &lt;code&gt;linger.ms&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;What happens when message compression is enabled?&lt;/h3&gt; &lt;p&gt;Kafka supports compressing messages with &lt;a href="https://kafka.apache.org/26/documentation.html#compression.type"&gt;standard compression algorithms&lt;/a&gt;. Compression is done on batches of data, so it is not necessarily a matter of gaining performance in the case of large messages. Nevertheless, let’s see if there is any difference when the average message size increases considerably.&lt;/p&gt; &lt;p&gt;Since I'm not focusing on comparing different compression types for this benchmark, I only run tests with one compression type: snappy. You can find numerous works on comparing the compression types in Kafka on the Internet. One benefit that’s worth mentioning is that compressed messages mean lower disk usage.&lt;/p&gt; &lt;p&gt;An interesting detail that impacts not only the throughput, but especially storage, is the average batch size. I observed keenly in my Kafka Producers Grafana dashboard that the average batch size reported in the metrics is significantly lower than the &lt;code&gt;batch.size&lt;/code&gt; setting. This makes sense as the batches are first filled up to the &lt;code&gt;batch.size&lt;/code&gt; and &lt;em&gt;then&lt;/em&gt; compressed, if compression is enabled. This results in faster transmission of the network packets, but also in lower storage requirements on the broker side. When considering using a compression mechanism for your producers, this is one of the dimensions to keep in mind.&lt;/p&gt; &lt;p&gt;Enabling compression adds computational overhead on the producer side. When compression is enabled, increasing &lt;code&gt;batch.size&lt;/code&gt; alone does not yield a higher throughput, as one can observe in the case of no compression. The throughput seems to be negatively affected by higher &lt;code&gt;batch.size&lt;/code&gt;, but especially when the payload size is relatively small. You can see this clearly in the plot showing the throughout over batch size with average payload size. When the average payload size is approximately 6 KB, increasing the batch size does not seem to impact the throughput with compression enabled. When the compression is not employed, a higher batch size results in slightly better throughput. Also, notice the trade-off between &lt;code&gt;batch.size&lt;/code&gt; and &lt;code&gt;linger.ms&lt;/code&gt; with the compression enabled. High &lt;code&gt;linger.ms&lt;/code&gt; results in higher throughput for 64 KB (65,536 bytes) of &lt;code&gt;batch.size&lt;/code&gt;, while the result clearly reverses for a &lt;code&gt;batch.size&lt;/code&gt; of 256 KB (262,144 bytes). However, this does not seem to indicate a specific pattern. As with small &lt;code&gt;batch.size&lt;/code&gt; (16 KB), &lt;code&gt;linger.ms&lt;/code&gt;'s impact is similar to the case of &lt;code&gt;batch.size=256&lt;/code&gt; KB.&lt;/p&gt; &lt;p&gt;Finally, it is clear in these experiments that throughput improves with compression enabled when the average payload size is large.&lt;/p&gt; &lt;h3&gt;How acknowledgments impact message throughput&lt;/h3&gt; &lt;p&gt;Unsurprisingly, when I set the &lt;code&gt;acks&lt;/code&gt; to &lt;code&gt;all&lt;/code&gt;, the message throughput decreases considerably. In this case, higher &lt;code&gt;batch.size&lt;/code&gt; values seem to help achieve relatively satisfactory message throughput. In other words, the default &lt;code&gt;batch.size&lt;/code&gt; does not result in optimal message throughput when &lt;code&gt;acks&lt;/code&gt; is set to &lt;code&gt;all&lt;/code&gt;. This parameter can also be set to zero, meaning that a producer would not wait for any acknowledgments from the Kafka brokers. In certain streaming scenarios, this option could be desirable, but many enterprise applications require stronger data resilience. Therefore, I did not include this option in the tests. Still, it is safe to guess that throughput would be better than, or at least as good as &lt;code&gt;acks=1&lt;/code&gt;, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Figure 2: Message throughput over batch size for an average record size of 1.1 KB. By acks." data-entity-type="file" data-entity-uuid="ab1ba7a8-2b55-4e9c-9f9f-ff74904979e8" src="https://developers.redhat.com/sites/default/files/inline-images/plot-by-acks-by-payload_0.png" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 2: Message throughput over batch size for an average record size of 1.1 KB. By acks.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Cluster resources&lt;/h3&gt; &lt;p&gt;Kafka brokers were observed to reach a max CPU usage of 0.8 and around 5.55 GB memory usage during the tests. Consequently, the performance of the Kafka cluster was not a limiting factor on the throughput.&lt;/p&gt; &lt;h2&gt;Further experiments&lt;/h2&gt; &lt;p&gt;Many applications don’t produce a high volume of messages nonstop. In many scenarios, a high volume of message production might occur sporadically. If your application matches this scenario, you might anticipate it by setting proper limits and requests for your deployments, and also by setting automatic pod scaling with low requests. This would optimize the use of your computational resources, as opposed to my setup of one pod for my application.&lt;/p&gt; &lt;p&gt;One follow-up to this experiment would be a larger overview of performance with both producers and consumers. My Kafka cluster served only a single producer application and was not bogged down by consumers. Consumers might decrease the throughput due to the Kafka cluster’s higher workload. The rate at which a consumer can consume messages from a Kafka topic would then be another subject for experimentation.&lt;/p&gt; &lt;p&gt;Another area for investigation is the impact of additional producer parameters of interest, such as &lt;code&gt;buffer.memory&lt;/code&gt;. Its relationship to &lt;code&gt;batch.size&lt;/code&gt; and &lt;code&gt;linger.ms&lt;/code&gt; should reveal interesting directions for fine-tuning.&lt;/p&gt; &lt;p&gt;Last but not least, I focused this article solely on message throughput. In many enterprises, disk space might be a limiting criteria. Analyzing the trade-off between throughout, producer parameters, and disk usage would nicely complement this work.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article demonstrated how setting various parameters on a Kafka producer leads to different message throughput, with some trade-offs between them. I hope these results help you fine-tune your producers, as you can decide where to start in a more informed manner. There are many valuable guides on this subject, in addition to the empirical results presented here. &lt;em&gt;&lt;a href="https://strimzi.io/blog/2020/10/15/producer-tuning/"&gt;Optimizing Kafka producers&lt;/a&gt;&lt;/em&gt; is one such guide, which explains in-depth important parameters and how they might impact the performance of your producers. For further resources, consider the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Download the resources used in this experiment from my &lt;a href="https://github.com/berkeragir/quarkus-kafka-demo"&gt;Quarkus-Kafka GitHub repository&lt;/a&gt;. The Quarkus application as well as the resources to create and deploy the Kafka cluster, Prometheus, and Grafana (including the dashboards for producers and Kafka clusters) are all in the repository. Remember that you need to install the operators for Kafka, Prometheus, and Grafana on your Kubernetes or OpenShift cluster.&lt;/li&gt; &lt;li&gt;Get instructions for installing and configuring Prometheus on an OpenShift cluster &lt;a href="https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md"&gt;using the Prometheus Operator&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;See some &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/master/examples/metrics/"&gt;Strimzi metrics examples&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Check out the &lt;a href="https://kafka.apache.org/26/documentation.html"&gt;Kafka 2.6 documentation&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;I recommend reading &lt;a href="https://strimzi.io/blog/2020/10/15/producer-tuning/"&gt;&lt;em&gt;Optimizing Kafka producers&lt;/em&gt;&lt;/a&gt; (Paul Mello, 2020).&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/19/benchmarking-kafka-producer-throughput-quarkus" title="Benchmarking Kafka producer throughput with Quarkus"&gt;Benchmarking Kafka producer throughput with Quarkus&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/uCN7rSLmt0A" height="1" width="1" alt=""/&gt;</summary><dc:creator>Berker Agir</dc:creator><dc:date>2021-07-19T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/19/benchmarking-kafka-producer-throughput-quarkus</feedburner:origLink></entry><entry><title>What's new in fabric8 Kubernetes client version 5.5.0</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XAHWbFVnMnA/whats-new-fabric8-kubernetes-client-version-550" /><author><name>Rohan Kumar</name></author><id>dec57063-7521-42ee-8d17-651322721a72</id><updated>2021-07-16T07:00:00Z</updated><published>2021-07-16T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://github.com/fabric8io/kubernetes-client"&gt;fabric8 Kubernetes client&lt;/a&gt; has been simplifying Java developers' use of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; for several years. Although the parent &lt;a href="https://fabric8.io/"&gt;fabric8 project&lt;/a&gt; has &lt;a href="https://developers.redhat.com/blog/2020/01/28/introduction-to-eclipse-jkube-java-tooling-for-kubernetes-and-red-hat-openshift"&gt;ended&lt;/a&gt;, the Kubernetes client continues to be popular, and the recent 5.5.0 release includes many new features and bug fixes.&lt;/p&gt; &lt;p&gt;This article takes a look at new features in fabric8 Kubernetes client, focusing on:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Dynamic clients&lt;/li&gt; &lt;li&gt;Dynamic informers&lt;/li&gt; &lt;li&gt;TLS certificate management&lt;/li&gt; &lt;li&gt;HTTP retry options&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; resources in the &lt;a href="https://docs.openshift.com/container-platform/3.7/dev_guide/openshift_pipeline.html#pipeline-openshift-dsl"&gt;OpenShift client DSL&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Knowing about these changes will help you avoid problems when you upgrade to the latest version of fabric8's &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; client for Kubernetes or Red Hat OpenShift.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The fabric8 development team mostly consists of Java developers, so the client's design is heavily influenced by a Java developer's perspective. I will demonstrate just a few of fabric8's features for using Kubernetes APIs in a Java environment.&lt;/p&gt; &lt;h2&gt;How to get the new fabric8 Java client&lt;/h2&gt; &lt;p&gt;You can find the most current fabric8 Java client release on &lt;a href="https://search.maven.org/artifact/io.fabric8/kubernetes-client/5.5.0/jar"&gt;Maven Central&lt;/a&gt;. To start using the new client, add it as a dependency in your Maven &lt;code&gt;pom.xml&lt;/code&gt; file. For Kubernetes, the dependency is:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.fabric8&lt;/groupId&gt; &lt;artifactId&gt;kubernetes-client&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For OpenShift, it's:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.fabric8&lt;/groupId&gt; &lt;artifactId&gt;openshift-client&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt; &lt;/dependency&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;New features in fabric8 Kubernetes client 5.5.0&lt;/h2&gt; &lt;p&gt;In many areas, we've been watching developments in Kubernetes and OpenShift and listening to the needs of developers. I will cover the most important new features in the following sections.&lt;/p&gt; &lt;h3&gt;Dynamic clients: Unstructured and GenericKubernetesResource&lt;/h3&gt; &lt;p&gt;The Kubernetes Go language client has the concept of a &lt;em&gt;dynamic&lt;/em&gt; client and a generic Kubernetes type called &lt;code&gt;Unstructured&lt;/code&gt;. The fabric8 Kubernetes client already provided support for &lt;code&gt;Unstructured&lt;/code&gt; in its &lt;code&gt;CustomResource&lt;/code&gt; API using raw &lt;code&gt;HashMap&lt;/code&gt;s. In 5.5.0, we added a new type, &lt;code&gt;GenericKubernetesResource&lt;/code&gt;, which you can use for deserializing unknown types. Here is an example of using it to get a list of &lt;code&gt;CronTab&lt;/code&gt; resources (mentioned in the &lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#create-a-customresourcedefinition"&gt;Kubernetes CustomResourceDefinition guide&lt;/a&gt;) in a specified namespace:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;try (KubernetesClient client = new DefaultKubernetesClient()) { CustomResourceDefinitionContext context = new CustomResourceDefinitionContext.Builder() .withVersion("v1") .withGroup("stable.example.com") .withScope("Namespaced") .withPlural("crontabs") .build(); System.out.println("CronTab resources in default namespace: "); client.genericKubernetesResources(context) .inNamespace("default") .list().getItems().stream() .map(GenericKubernetesResource::getMetadata) .map(ObjectMeta::getName) .forEach(System.out::println); } catch (KubernetesClientException e) { System.out.println("Exception received: " + e.getMessage()); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the future, we will provide additional methods to make using &lt;code&gt;GenericKubernetesResource&lt;/code&gt; even easier. We also plan to add support for applying the list of Kubernetes resources (right now the class works only for primitive Kubernetes resource lists), which can also contain custom resources&lt;/p&gt; &lt;h3&gt;Dynamic informers&lt;/h3&gt; &lt;p&gt;With the introduction of &lt;code&gt;GenericKubernetesResource&lt;/code&gt;, you can use the &lt;a href="https://javadoc.io/static/io.fabric8/kubernetes-client/4.6.1/io/fabric8/kubernetes/client/informers/SharedInformer.html"&gt;SharedInformer API&lt;/a&gt; for a &lt;code&gt;CustomResource&lt;/code&gt; without providing any type. The earlier &lt;code&gt;KubernetesClient&lt;/code&gt; raw API was missing support for &lt;code&gt;SharedInformer&lt;/code&gt;s, but now you can use informers from both &lt;code&gt;SharedInformerFactory&lt;/code&gt; and the DSL &lt;code&gt;inform()&lt;/code&gt; method.&lt;/p&gt; &lt;p&gt;Here is an example of using dynamic informers from &lt;code&gt;SharedInformerFactory&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;try (KubernetesClient client = new DefaultKubernetesClient()) { SharedInformerFactory informerFactory = client.informers(); CustomResourceDefinitionContext context = new CustomResourceDefinitionContext.Builder() .withGroup("stable.example.com") .withVersion("v1") .withPlural("crontabs") .withScope("Namespaced") .build(); SharedIndexInformer&lt;GenericKubernetesResource&gt; informer = informerFactory.sharedIndexInformerForCustomResource(context, 60 * 1000L); informer.addEventHandler(new ResourceEventHandler&lt;&gt;() { @Override public void onAdd(GenericKubernetesResource genericKubernetesResource) { System.out.printf("ADD %s\n", genericKubernetesResource.getMetadata().getName()); } @Override public void onUpdate(GenericKubernetesResource genericKubernetesResource, GenericKubernetesResource t1) { System.out.printf("UPDATE %s\n", genericKubernetesResource.getMetadata().getName()); } @Override public void onDelete(GenericKubernetesResource genericKubernetesResource, boolean b) { System.out.printf("DELETE %s\n", genericKubernetesResource.getMetadata().getName()); } }); informerFactory.startAllRegisteredInformers(); TimeUnit.MINUTES.sleep(10); } catch (InterruptedException e) { Thread.currentThread().interrupt(); e.printStackTrace(); } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We’ve also added a new method to the DSL:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;inform(ResourceEventHandle eventHandler)&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This method can be used from within the DSL without having to create a &lt;code&gt;SharedInformerFactory&lt;/code&gt;. A &lt;code&gt;SharedIndexInformer&lt;/code&gt; will be started automatically, so there is no need to invoke the informer's &lt;code&gt;run()&lt;/code&gt; method. Here is an example:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package io.fabric8.demos; import io.fabric8.kubernetes.api.model.Pod; import io.fabric8.kubernetes.client.DefaultKubernetesClient; import io.fabric8.kubernetes.client.KubernetesClient; import io.fabric8.kubernetes.client.informers.ResourceEventHandler; import java.util.concurrent.TimeUnit; public class DSLInformMethod { public static void main(String[] args) { try (KubernetesClient client = new DefaultKubernetesClient()) { client.pods().inNamespace("default").inform(new ResourceEventHandler&lt;&gt;() { @Override public void onAdd(Pod pod) { System.out.println(pod.getMetadata().getName() + " ADD "); } @Override public void onUpdate(Pod pod, Pod t1) { System.out.println(pod.getMetadata().getName() + " UPDATE "); } @Override public void onDelete(Pod pod, boolean b) { System.out.println(pod.getMetadata().getName() + " DELETE "); } }); TimeUnit.SECONDS.sleep(10 * 1000); } catch (InterruptedException interruptedException) { Thread.currentThread().interrupt(); interruptedException.printStackTrace(); } } } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Certification management&lt;/h3&gt; &lt;p&gt;A new extension was contributed to this release: The &lt;a href="https://github.com/jetstack/cert-manager"&gt;JetStack&lt;/a&gt; &lt;code&gt;cert-manager&lt;/code&gt; extension. With this extension, you can manage TLS web certificates through &lt;code&gt;cert-manager&lt;/code&gt;-based CRDs from the Kubernetes API server in Java, using the &lt;code&gt;cert-manager&lt;/code&gt; extension.&lt;/p&gt; &lt;p&gt;Add this dependency to use the extension in your projects:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.fabric8&lt;/groupId&gt; &lt;artifactId&gt;certmanager-client&lt;/artifactId&gt; &lt;version&gt;5.5.0&lt;/version&gt; &lt;/dependency&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here is a simple example of a &lt;code&gt;CerificateRequest&lt;/code&gt; using the extension:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package io.fabric8.demo; import io.fabric8.certmanager.api.model.meta.v1.ObjectReferenceBuilder; import io.fabric8.certmanager.api.model.v1.CertificateRequest; import io.fabric8.certmanager.api.model.v1.CertificateRequestBuilder; import io.fabric8.certmanager.client.CertManagerClient; import io.fabric8.certmanager.client.DefaultCertManagerClient; import io.fabric8.kubernetes.api.model.Duration; import java.text.ParseException; public class CertificateRequestExample { public static void main(String[] args) { try (CertManagerClient certManagerClient = new DefaultCertManagerClient()) { CertificateRequest certificateRequest = new CertificateRequestBuilder() .withNewMetadata().withName("my-ca-cr").endMetadata() .withNewSpec() .withRequest("base64encodedcert=") .withIsCA(false) .addToUsages("signing", "digital signature", "server auth") .withDuration(Duration.parse("90d")) .withIssuerRef(new ObjectReferenceBuilder() .withName("ca-issuer") .withKind("Issuer") .withGroup("cert-manager.io") .build()) .endSpec() .build(); certManagerClient.v1().certificateRequests().inNamespace("default").create(certificateRequest); } catch (ParseException e) { e.printStackTrace(); } } } &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;HTTP operation retry configuration options with exponential backoff&lt;/h3&gt; &lt;p&gt;According to &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#http-status-code"&gt;Kubernetes API conventions&lt;/a&gt;, when the HTTP status code is 500 (Status Internal Server Error), 503 (Status Service Unavailable), or 504 (Status Server Timeout), the suggested client recovery behavior is "retry with exponential backoff." In this release, we introduced additional configuration properties for this retry mechanism, shown in Table 1.&lt;/p&gt; &lt;div&gt; &lt;table&gt;&lt;caption&gt;Table 1: New properties to control HTTP retries in fabric8 Kubernetes client 5.5.0.&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt; &lt;p&gt;&lt;strong&gt;Property name&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;th&gt; &lt;p&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;kubernetes.request.retry.backoffLimit&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Number of retry attempts on status codes &gt;= 500. Defaults to 0.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;kubernetes.request.retry.backoffInterval&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;Retry initial backoff interval, in milliseconds. Defaults to 1000.&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;h3&gt;OpenShift client DSL improvements&lt;/h3&gt; &lt;p&gt;The previous release provided support for all Kubernetes resources, as described in &lt;a href="https://github.com/fabric8io/kubernetes-client/issues/2912"&gt;this issue&lt;/a&gt; in the fabric8 Kubernetes client GitHub repository. We also had good coverage for OpenShift 3.x resources. But in OpenShift 4, newer resources were introduced by the installation of additional operators. Now we’re covering most of the resources offered by OpenShift 4. You can find more details in &lt;a href="https://github.com/fabric8io/kubernetes-client/issues/2949"&gt;this GitHub issue&lt;/a&gt;. Table 2 shows the newly added DSL methods.&lt;/p&gt; &lt;div&gt; &lt;table&gt;&lt;caption&gt;Table 2: New resource DSLs in fabric8 Kubernetes client 5.5.0&lt;/caption&gt; &lt;thead&gt;&lt;tr&gt;&lt;th&gt; &lt;p&gt;&lt;strong&gt;OpenShift resource&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;th&gt; &lt;p&gt;&lt;strong&gt;OpenShift client DSL&lt;/strong&gt;&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 ClusterRole&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.clusterRoles()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 ResourceAccessReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.resourceAccessReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 LocalSubjectAccessReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.localSubjectAccessReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 LocalResourceAccessReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.localResourceAccessReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 SubjectRulesReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.subjectRulesReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 SelfSubjectRulesReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.selfSubjectRulesReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;authorization.openshift.io/v1 RoleBindingRestrictions&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.roleBindingRestrictions()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;autoscaling.openshift.io/v1 ClusterAutoscaler&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.clusterAutoscaling().v1().clusterAutoscalers()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;autoscaling.openshift.io/v1beta1 MachineAutoscaler&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.clusterAutoscaling().v1beta1().machineAutoscalers()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;cloudcredential.openshift.io/v1 CredentialsRequest&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.credentialsRequests()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;config.openshift.io/v1 Authentication&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.config().authentications()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;config.openshift.io/v1 Console&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.config().consoles()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;config.openshift.io/v1 DNS&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.config().dnses()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;config.openshift.io/v1 Network&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.config().networks()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;console.openshift.io/v1alpha1 ConsolePlugin&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.console().consolePlugins()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;console.openshift.io/v1 ConsoleQuickStart&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.console().consoleQuickStarts()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;controlplane.operator.openshift.io/v1alpha1 PodNetworkConnectivityCheck&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operator().podNetworkConnectivityChecks()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;helm.openshift.io/v1beta1 HelmChartRepository&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.helmChartRepositories()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;image.openshift.io/v1 ImageSignature&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.imageSignatures()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;image.openshift.io/v1 ImageStreamImage&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.imageStreamImages()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;image.openshift.io/v1 ImageStreamImport&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.imageStreamImports()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;image.openshift.io/v1 ImageStreamMapping&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.imageStreamMappings()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machine.openshift.io/v1beta1 MachineHealthCheck&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machine().machineHealthChecks()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machine.openshift.io/v1beta1 MachineSet&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machine().machineSets()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 ContainerRuntimeConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().containerRuntimeConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 ControllerConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().controllerConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 KubeletConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().kubeletConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 MachineConfigPool&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().machineConfigPools()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;machineconfiguration.openshift.io/v1 MachineConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.machineConfigurations().machineConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;metal3.io/v1alpha1 BareMetalHost&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.bareMetalHosts()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;monitoring.coreos.com/v1alpha1 AlertmanagerConfig&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.monitoring().alertmanagerConfigs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;monitoring.coreos.com/v1 Probe&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.monitoring().probes()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;monitoring.coreos.com/v1 ThanosRuler&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.monitoring().thanosRulers()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;network.openshift.io/v1 HostSubnet&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.hostSubnets()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;network.operator.openshift.io/v1 OperatorPKI&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operatorPKIs()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;oauth.openshift.io/v1 OAuthClientAuthorization&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.oAuthClientAuthorizations()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;oauth.openshift.io/v1 UserOAuthAccessToken&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.userOAuthAccessTokens()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operator.openshift.io/v1 CloudCredential&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operator().cloudCredentials()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operator.openshift.io/v1 ClusterCSIDriver&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operator().clusterCSIDrivers()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operator.openshift.io/v1 Storage&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operator().storages()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operators.coreos.com/v1 OperatorCondition&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operatorHub().operatorConditions()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;operators.coreos.com/v1 Operator&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operatorHub().operators()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;packages.operators.coreos.com/v1 PackageManifest&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.operatorHub().packageManifests()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;security.openshift.io/v1 PodSecurityPolicyReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.podSecurityPolicyReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;security.openshift.io/v1 PodSecurityPolicySelfSubjectReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.podSecurityPolicySelfSubjectReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;security.openshift.io/v1 PodSecurityPolicySubjectReview&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.podSecurityPolicySubjectReviews()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;template.openshift.io/v1 BrokerTemplateInstance&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.brokerTemplateInstances()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;template.openshift.io/v1 TemplateInstance&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.templateInstances()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;tuned.openshift.io/v1 Profile&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.tuned().profiles()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;tuned.openshift.io/v1 Tuned&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.tuned().tuneds()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;user.openshift.io/v1 Identity&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.identities()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;&lt;td&gt; &lt;p&gt;user.openshift.io/v1 UserIdentityMapping&lt;/p&gt; &lt;/td&gt; &lt;td&gt; &lt;p&gt;openShiftClient.userIdentityMappings()&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/div&gt; &lt;h3&gt;Other improvements&lt;/h3&gt; &lt;p&gt;Other notable improvements to the Kubernetes client in this release include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;fabric8 Knative extension: Knative model updated to v0.23.0.&lt;/li&gt; &lt;li&gt;fabric8 Tekton extension: Tekton pipeline model updated to v0.24.1.&lt;/li&gt; &lt;li&gt;fabric8 Tekton extension: Tekton triggers model updated to v0.13.0.&lt;/li&gt; &lt;li&gt;fabric8 Kubernetes mock server: Bug fixes related to ignoring the local kubeconfig, CRUD-mode fixes such as status subresource handling, apiVersion awareness, etc.&lt;/li&gt; &lt;li&gt;Introduction of &lt;code&gt;withNewFilter()&lt;/code&gt; in the &lt;code&gt;KubernetesClient&lt;/code&gt; DSL, offering better options for Kubernetes resource filtering.&lt;/li&gt; &lt;/ul&gt;&lt;h2 id="learn_more_about_fabric8-h2"&gt;Learn more about fabric8&lt;/h2&gt; &lt;p&gt;This article has demonstrated just a few of fabric8's features for using Kubernetes APIs in a Java environment. For more examples, see the &lt;a href="https://github.com/fabric8io/kubernetes-client/tree/master/kubernetes-examples/src/main/java/io/fabric8/kubernetes/examples"&gt;Kubernetes Java client examples repository&lt;/a&gt;. For a deep dive into using fabric8, visit the &lt;em&gt;&lt;a href="https://github.com/fabric8io/kubernetes-client/blob/master/doc/CHEATSHEET.md"&gt;fabric8 Kubernetes Java Client Cheat Sheet&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/16/whats-new-fabric8-kubernetes-client-version-550" title="What's new in fabric8 Kubernetes client version 5.5.0"&gt;What's new in fabric8 Kubernetes client version 5.5.0&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XAHWbFVnMnA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Rohan Kumar</dc:creator><dc:date>2021-07-16T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/16/whats-new-fabric8-kubernetes-client-version-550</feedburner:origLink></entry><entry><title type="html">Quarkus Newsletter #10</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/OxWpae7K68I/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-newsletter-10/</id><updated>2021-07-16T00:00:00Z</updated><content type="html">Ok… it’s been a while since our last newsletter but we’ve been busy working on a new format. We’re moved to an email subscription model starting with Issue #10. Our goal is to allow folks to sign up and get the cream of the crop articles delivered to their inbox...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/OxWpae7K68I" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-newsletter-10/</feedburner:origLink></entry></feed>
