<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>JDK Flight Recorder support for GraalVM Native Image: The journey so far</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/i4bViePVGGM/jdk-flight-recorder-support-graalvm-native-image-journey-so-far" /><author><name>Jie Kang</name></author><id>3700dbb8-83dc-4f19-8c79-8a021cd812d4</id><updated>2021-07-23T07:00:00Z</updated><published>2021-07-23T07:00:00Z</published><summary type="html">&lt;p&gt;Over the past year, Oracle and Red Hat engineers have worked together to bring &lt;a href="https://developers.redhat.com/blog/2020/08/25/get-started-with-jdk-flight-recorder-in-openjdk-8u" target="_blank"&gt;JDK Flight Recorder&lt;/a&gt; (JFR) support to &lt;a href="https://www.graalvm.org/reference-manual/native-image/" target="_blank"&gt;GraalVM Native Image&lt;/a&gt;. Prototype pull requests were first introduced by &lt;a href="https://github.com/oracle/graal/pull/3070" target="_blank"&gt;Red Hat&lt;/a&gt; and &lt;a href="https://github.com/oracle/graal/pull/3155" target="_blank"&gt;Oracle&lt;/a&gt; in late 2020 and early 2021. Work then continued in a &lt;a href="https://github.com/native-image-jfr/graal/tree/jfr" target="_blank"&gt;shared repository&lt;/a&gt;, with plans to contribute a new pull request incorporating work from all parties. In June 2021, we merged the &lt;a href="https://github.com/oracle/graal/pull/3444" target="_blank"&gt;first pull request&lt;/a&gt; that introduces the infrastructure for supporting JDK Flight Recorder on GraalVM with OpenJDK 11 upstream. It is available in GraalVM's 21.2 release. This article shares some of the details behind that story: What native images are, why we worked to add JDK Flight Recorder, some of the technical challenges we faced, and what we're looking to do next.&lt;/p&gt; &lt;h2&gt;Performance profiling for GraalVM Native Image&lt;/h2&gt; &lt;p&gt;GraalVM is a high-performance runtime that includes support for producing &lt;a href="https://www.graalvm.org/reference-manual/native-image/" target="_blank"&gt;native images&lt;/a&gt; of &lt;a href="https://developers.redhat.com/topics/enterprise-java" target="_blank"&gt;Java&lt;/a&gt; applications. This feature takes Java class files and produces a binary executable for a target platform. The resulting native executable incorporates a runtime system written in Java, called SubstrateVM, which includes components such as memory management and thread scheduling. This is similar to how &lt;a href="https://developers.redhat.com/products/openjdk" target="_blank"&gt;OpenJDK&lt;/a&gt; uses the HotSpot virtual machine (VM) to execute Java applications. The native executable is generated via iterative points-to-analysis and heap population, followed by &lt;a href="https://dl.acm.org/doi/10.1145/3360610" target="_blank"&gt;ahead-of-time compilation&lt;/a&gt;. The native executables produced by GraalVM ideally have a significantly faster startup time compared to a Java application running on a traditional JVM like HotSpot.&lt;/p&gt; &lt;p&gt;Binary executables promising better performance still need an ecosystem of supporting tools for monitoring and performance profiling. At this time, however, most of the monitoring features typically used for Java applications on OpenJDK are not available for GraalVM Native Image on GraalVM. Diagnosing application performance problems is difficult without having tools to examine the application execution and runtime execution layers. This omission is critical for applications deployed in production, whether they are in &lt;a href="https://developers.redhat.com/topics/containers" target="_blank"&gt;containers&lt;/a&gt; or on bare metal.&lt;/p&gt; &lt;h2&gt;Why JDK Flight Recorder?&lt;/h2&gt; &lt;p&gt;JDK Flight Recorder is a profiling system in the Hotspot VM that delivers JVM internal data and custom, application-specific data through an events-based logging system. JDK Flight Recorder is designed to collect critical data in production environments with minimal performance overhead. Traditionally, the output is a JFR file (.jfr) that can be read by tools like &lt;a href="https://github.com/openjdk/jmc" target="_blank"&gt;JDK Mission Control&lt;/a&gt;. With JDK 14 and later, the data can also be streamed through a Java API.&lt;/p&gt; &lt;p&gt;Having JDK Flight Recorder support for GraalVM Native Image will provide users a strong performance profiling tool that is similar to the HotSpot VM experience. Ideally, developers who use JDK Flight Recorder to profile their Java applications on OpenJDK would have the same experience using it for native executables. JFR for native images will be a profiling system in SubstrateVM that delivers SubstrateVM internal data, and custom, application-specific data with the same JFR file and streaming capabilities found in HotSpot.&lt;/p&gt; &lt;p&gt;JFR for GraalVM Native Image would provide internal data on SubstrateVM systems, mimicking much of the kinds of data seen in HotSpot. This includes information about garbage collection operations, thread state, monitor state, exceptions, safepoints, object allocations, file or socket I/O events, and more.&lt;/p&gt; &lt;h3&gt;Profiling with internal and custom events&lt;/h3&gt; &lt;p&gt;JDK Flight Recorder and other profiling tools in HotSpot help us answer a wide variety of questions. JDK Flight Recorder in GraalVM Native Image aims to do the same. Some of these questions include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;How much execution time is consumed by the garbage collector?&lt;/li&gt; &lt;li&gt;How much time is spent in safepoint, stop-the-world operations?&lt;/li&gt; &lt;li&gt;Are there bottlenecks in the execution flow and where are they?&lt;/li&gt; &lt;li&gt;What are the hot methods in my application?&lt;/li&gt; &lt;li&gt;How much time is spent on I/O processing?&lt;/li&gt; &lt;li&gt;What about synchronization issues like deadlocks and operations waiting on a lock?&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Along with internal events, JDK Flight Recorder has an API for developers to add their own events to the system. This lets users add even more data for analysis that benefits from the existing low-overhead, production-oriented JFR infrastructure. Custom events can take advantage of the common pool for runtime constants (class names, method names, strings, and so on) for overall reduced output cost compared to separate, custom, event or metric emission agents.&lt;/p&gt; &lt;h3&gt;Solving performance problems&lt;/h3&gt; &lt;p&gt;Events by themselves are not enough to diagnose performance issues. Fortunately, the existing JFR format lets us quickly integrate with visualization and analysis tools like JDK Mission Control. We can use these tools together to solve performance problems in our native image applications. Any of the events in SubstrateVM that are one-to-one matches to events in HotSpot will already be understood without any changes to the existing analysis tools.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: JDK Flight Recorder for HotSpot has a plethora of resources, some of which I’ve listed at the end of the article. Please take a look to learn more about JDK Flight Recorder in general and the benefits it brings to solving performance problems.&lt;/p&gt; &lt;h2&gt;Implementing JDK Flight Recorder support for GraalVM Native Image&lt;/h2&gt; &lt;p&gt;Prior to our work, there was no JDK Flight Recorder support for GraalVM Native Image, and we've encountered many challenges in implementing that support. While the Java API is accessible, there is no mechanism to manage the JDK Flight Recorder system. Additionally, existing code that uses the JDK Flight Recorder Java API directly, &lt;a href="https://github.com/jiekang/sandbox/blob/java-jfr-app/src/main/java/com/redhat/jkang/Main.java" target="_blank"&gt;for example, to start a recording&lt;/a&gt;, fails with an &lt;a href="https://gist.github.com/jiekang/ae928a7bbe4dafeb05b5e7a97d0174d1" target="_blank"&gt;unsatisfied link error&lt;/a&gt;. The failure happens because the JDK Flight Recorder native API is not present. This particular error occurs because the native JDK Flight Recorder code found in the HotSpot library, &lt;code&gt;libjvm.so&lt;/code&gt;, is not included in the SubstrateVM system. Unfortunately, the native code isn’t suitable for inclusion because of its deep ties to HotSpot internals.&lt;/p&gt; &lt;p&gt;To add JDK Flight Recorder to GraalVM Native Image, we reimplemented the native infrastructure for JDK Flight Recorder in the HotSpot VM entirely in Java code in SubstrateVM. However, we were not able to perform a one-to-one translation of the HotSpot code, written in C++, to a SubstrateVM equivalent in Java. This section explains some of the reasons why.&lt;/p&gt; &lt;h3&gt;Class transformation&lt;/h3&gt; &lt;p&gt;First, the event class in HotSpot is &lt;a href="https://github.com/openjdk/jdk/blob/1aa653957619acfdb5f08ce0f3a1ad1a17cfa127/src/jdk.jfr/share/classes/jdk/jfr/Event.java#L100" target="_blank"&gt;nominally empty&lt;/a&gt; and class transformation is done lazily at runtime to add implementations to the methods for enabled events. The methods have empty bodies and are marked final, preventing extends. See the code in &lt;a href="https://github.com/openjdk/jdk/blob/1aa653957619acfdb5f08ce0f3a1ad1a17cfa127/src/jdk.jfr/share/classes/jdk/jfr/internal/EventInstrumentation.java#L328" target="_blank"&gt;EventInstrumentation&lt;/a&gt; to see what the implementations look like when the event is transformed.&lt;/p&gt; &lt;p&gt;The transformed event class is needed in SubstrateVM to emit events but the class transformation cannot be done at runtime for native images. OpenJDK JDK Flight Recorder has the boolean flag &lt;a href="https://github.com/openjdk/jdk/blob/1aa653957619acfdb5f08ce0f3a1ad1a17cfa127/src/hotspot/share/jfr/recorder/service/jfrOptionSet.cpp#L249" target="_blank"&gt;retransform&lt;/a&gt; to instrument classes via the Java Virtual Machine Tool Interface (JVMTI). The default value is true. To get around this, we can set the value to false for the JVM being run for SubstrateVM analysis, for example via&lt;/p&gt; &lt;pre&gt; &lt;code&gt; $ mx native-image -J-XX:FlightRecorderOptions=retransform=false ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The resulting event classes that SubstrateVM sees will contain the transformed implementations as if events were enabled and JDK Flight Recorder was running.&lt;/p&gt; &lt;p&gt;Another solution is to &lt;a href="https://gist.github.com/jiekang/d3c0a7e11b689f57950ec009c5f8fff4#file-jfreventsubstitution-java-L80" target="_blank"&gt;transform the event classes before compilation&lt;/a&gt; by using the internal API &lt;code&gt;retransformClasses&lt;/code&gt; in &lt;a href="https://github.com/openjdk/jdk/blob/1aa653957619acfdb5f08ce0f3a1ad1a17cfa127/src/jdk.jfr/share/classes/jdk/jfr/internal/JVM.java#L206" target="_blank"&gt;jdk.jfr.internal.JVM&lt;/a&gt;. This workaround also relies on implementation details of HotSpot JDK Flight Recorder, but it allows for more fine-grained control over exactly which classes to transform.&lt;/p&gt; &lt;h3&gt;Uninterruptible safepoints&lt;/h3&gt; &lt;p&gt;By default, native code (HotSpot) cannot be interrupted for a safepoint, whereas Java code (SubstrateVM) can. Fortunately, SubstrateVM has the &lt;a href="https://github.com/oracle/graal/blob/master/substratevm/src/com.oracle.svm.core/src/com/oracle/svm/core/annotate/Uninterruptible.java" target="_blank"&gt;uninterruptible annotation&lt;/a&gt; that can be used to mark method blocks that will not be interrupted for a safepoint. It’s not ideal to mark all the "native" portions as uninterruptible, as performance will be impacted by these sections. As well, some JDK Flight Recorder code paths in HotSpot are commented with “&lt;a href="https://github.com/openjdk/jdk/blob/1aa653957619acfdb5f08ce0f3a1ad1a17cfa127/src/hotspot/share/jfr/recorder/checkpoint/jfrCheckpointManager.cpp#L403" target="_blank"&gt;can safepoint here&lt;/a&gt;” (and marked appropriately to allow safepointing), so we still need to pay careful attention to the SubstrateVM implementation to ensure consistency is maintained.&lt;/p&gt; &lt;p&gt;Using the &lt;code&gt;Uninterruptible&lt;/code&gt; annotation also comes with a restriction: No object allocation is allowed. This proves quite problematic when implementing familiar code patterns in Java, an object-oriented programming language. Even logging can be difficult as any &lt;code&gt;toString&lt;/code&gt; method that allocates is not usable in an uninterruptible method call. This has a follow-on in making debugging a bit more painful than normal, especially when dealing with concurrency issues where simple debugger strategies don’t see the same execution flow as the issue being hunted.&lt;/p&gt; &lt;p&gt;Additionally, some critical parts of SubstrateVM, such as the garbage collector, have methods that are marked uninterruptible. We must be able to trigger the event emission infrastructure in these locations, and so its components also need to be uninterruptible. This requires that they do not allocate Java objects. Fortunately, SubstrateVM provides methods to &lt;code&gt;malloc&lt;/code&gt;, or free memory, that are &lt;a href="https://github.com/oracle/graal/blob/master/sdk/src/org.graalvm.nativeimage/src/org/graalvm/nativeimage/UnmanagedMemory.java#L57" target="_blank"&gt;not managed by garbage collection&lt;/a&gt;. These instances do not extend &lt;code&gt;Object&lt;/code&gt;, though, and so any existing Java API, like &lt;code&gt;ArrayList&lt;/code&gt;, cannot be used to manipulate the data. These data structures must be reimplemented if they are needed.&lt;/p&gt; &lt;h3&gt;The JFR Recorder thread&lt;/h3&gt; &lt;p&gt;Some critical work for writing data is given to a single thread in HotSpot, the JFR &lt;code&gt;Recorder&lt;/code&gt; thread, which is itself excluded from the JDK Flight Recorder system. This exclusion cannot be mimicked in SubstrateVM because any thread can be appropriated to perform a virtual machine operation such as garbage collection. The garbage collector could emit relevant JDK Flight Recorder events and exclusion would result in missing event data from such operations.&lt;/p&gt; &lt;p&gt;It is even possible for the closing of a chunk—where events are written to disk, the epoch is transitioned, and metadata and constant pools are emitted—to be paused for a garbage collection operation. This action could then write garbage collection events to the buffer. The tasks here need to be analyzed and marked appropriately as uninterruptible to keep consistency for these garbage collection events and their constant pool data. The write operation to close the chunk, which I described earlier, needs to be carefully written as a safepoint operation with uninterruptible portions to account for the possibility of appropriation for garbage collection.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;JDK Flight Recorder is an important feature of the HotSpot VM, giving users an impressive amount of data about the JVM’s execution with low overhead suitable for continuously running in production environments. It will be useful for developers to have access to the same tool when running native image applications with SubstrateVM.&lt;/p&gt; &lt;p&gt;The initial merge for JFR infrastructure is complete but there is a long road ahead before the system can provide a view into native executables produced by GraalVM that is similar to what is possible for HotSpot. Up next is the work to add events for garbage collection, threads, exceptions, and other useful locations in SubstrateVM. The &lt;a href="https://docs.oracle.com/cd/E15289_01/JRJVD/com/jrockit/mc/rjmx/package-summary.html" target="_blank"&gt;RJMX API&lt;/a&gt; is not currently implemented for SubstrateVM, so there is no API to remotely manage JDK Flight Recorder. Alongside this, the JFR system is still being significantly improved and enhanced in OpenJDK and HotSpot. We will need to account for major changes that affect the API in the SubstrateVM implementation to properly support the different underlying OpenJDK versions. At this time, the codebase only targets OpenJDK 11.&lt;/p&gt; &lt;p&gt;For now, you can try out the latest GraalVM and test JDK Flight Recorder by passing the flag &lt;code&gt;-H:+AllowVMInspection&lt;/code&gt; to the &lt;code&gt;native-image&lt;/code&gt; process at build time. After you've done that, you may add flags such as &lt;code&gt;-XX:+FlightRecorder -XX:StartFlightRecording="filename=recording.jfr"&lt;/code&gt; to the application binary at runtime.&lt;/p&gt; &lt;p&gt;Alongside GraalVM, &lt;a href="https://developers.redhat.com/blog/2021/04/14/mandrel-a-specialized-distribution-of-graalvm-for-quarkus" target="_blank"&gt;Mandrel&lt;/a&gt;, the community distribution for &lt;a href="https://developers.redhat.com/products/quarkus/" target="_blank"&gt;Quarkus&lt;/a&gt;, will naturally inherit JFR support. As we continue to improve and enhance JDK Flight Recorder for GraalVM Native Image, please look for more announcements and articles about using JFR with native images in Quarkus and Mandrel environments.&lt;/p&gt; &lt;h2&gt;Learn more about JDK Flight Recorder&lt;/h2&gt; &lt;p&gt;The following resources are available for learning more about JDK Flight Recorder and using it to solve performance problems:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://archive.fosdem.org/2019/schedule/event/imc/"&gt;An introduction to Middleware Application Monitoring with Java Mission Control and Flight Recorder&lt;/a&gt; (Mario Torre and Marcus Hirt, FOSDEM 2019)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/08/25/get-started-with-jdk-flight-recorder-in-openjdk-8u"&gt;Get started with JDK Flight Recorder in OpenJDK 8u&lt;/a&gt; (Mario Torre, Red Hat Developer)&lt;/li&gt; &lt;li&gt;&lt;a href="https://dzone.com/articles/using-java-flight-recorder-with-openjdk-11-1"&gt;Using Java Flight Recorder with JDK 11&lt;/a&gt; (Laszlo Csontos, DZone)&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.oracle.com/javase/10/troubleshoot/troubleshoot-performance-issues-using-jfr.htm#JSTGD299"&gt;Troubleshoot Performance Issues Using JFR&lt;/a&gt; (&lt;em&gt;Java Platform, Standard Edition Troubleshooting Guide&lt;/em&gt;, Oracle)&lt;/li&gt; &lt;li&gt;&lt;a href="https://bell-sw.com/announcements/2020/06/24/Java-Flight-Recorder-a-gem-hidden-in-OpenJDK/"&gt;JDK Flight Recorder—a gem hidden in OpenJDK&lt;/a&gt; (BellSoft)&lt;/li&gt; &lt;li&gt;&lt;a href="https://youtu.be/7z_R2Aq-Fl8"&gt;JDK11—Introduction to JDK Flight Recorder&lt;/a&gt; (Markus Grönlund, Oracle YouTube)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.morling.dev/blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/"&gt;Monitoring REST APIs with Custom JDK Flight Recorder Events&lt;/a&gt; (Gunnar Morling)&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/25/introduction-to-containerjfr-jdk-flight-recorder-for-containers"&gt;Introduction to ContainerJFR: JDK Flight Recorder for containers&lt;/a&gt; (Andrew Azores, Red Hat Developer)&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Acknowledgment&lt;/h2&gt; &lt;p&gt;The author thanks Alina Yurenko and Oracle for their time in reviewing this article.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/23/jdk-flight-recorder-support-graalvm-native-image-journey-so-far" title="JDK Flight Recorder support for GraalVM Native Image: The journey so far"&gt;JDK Flight Recorder support for GraalVM Native Image: The journey so far&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/i4bViePVGGM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Jie Kang</dc:creator><dc:date>2021-07-23T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/23/jdk-flight-recorder-support-graalvm-native-image-journey-so-far</feedburner:origLink></entry><entry><title>Troubleshooting application performance with Red Hat OpenShift metrics, Part 3: Collecting runtime metrics</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ncoqSuhXI4A/troubleshooting-application-performance-red-hat-openshift-metrics-part-3" /><author><name>Pavel Macik</name></author><id>1144a785-8b85-4a04-b813-02b2ca9fc72b</id><updated>2021-07-22T07:00:00Z</updated><published>2021-07-22T07:00:00Z</published><summary type="html">&lt;p&gt;This is the third article in series showing how to use metrics from &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; to reveal application performance problems. In &lt;a href="https://developers.redhat.com/articles/2021/07/08/troubleshooting-application-performance-red-hat-openshift-metrics-part-1"&gt;Part 1&lt;/a&gt;, I explained the environment and requirements for our application, the &lt;a href="https://developers.redhat.com/blog/2019/12/19/introducing-the-service-binding-operator"&gt;Service Binding Operator&lt;/a&gt;. In &lt;a href="https://developers.redhat.com/articles/2021/07/15/troubleshooting-application-performance-red-hat-openshift-metrics-part-2-test"&gt;Part 2&lt;/a&gt;, I showed you how to set up a test environment in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; and introduced the test scenarios. Now, we can start to focus on the metrics themselves.&lt;/p&gt; &lt;h2&gt;Collecting runtime metrics with the OpenShift monitoring tool&lt;/h2&gt; &lt;p&gt;To see what is going on with Service Binding Operator and the OpenShift cluster under heavy load, it is important to collect metrics for the duration of the test. OpenShift's &lt;a href="https://docs.openshift.com/container-platform/4.7/monitoring/understanding-the-monitoring-stack.html" target="_blank"&gt;monitoring tool&lt;/a&gt;, a combination of &lt;a href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; and &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; with predefined, out-of-the-box metrics, can collect the necessary data for the lifespan of the cluster. At first, it seems an ideal solution to simply use that feature, let the monitoring stack gather the data normally, and collect it after the test is done. The problem is that all of the monitoring tools—the Prometheus and Graphana instances—are deployed on the same cluster as the application they're monitoring, including the OpenShift cluster's own resources. So, if the cluster goes down (which we expect to happen while stress testing), the monitoring subsystem goes down along with it, and all the gathered data is lost. Keep in mind that we are using a temporary development cluster that is terminated after about 10 hours anyway.&lt;/p&gt; &lt;p&gt;To ensure results are preserved on a node that won't crash, I've created the following collector script. It uses the &lt;a href="https://mirror.openshift.com/pub/openshift-v4/clients/ocp/" target="_blank"&gt;OpenShift Client tool&lt;/a&gt; (&lt;code&gt;oc&lt;/code&gt;) to pull the runtime metrics every 30 seconds or so for the duration of the test and stores it in a set of simple CSV files, one for nodes and one for each monitored pod. I start the script in the background before the user provisioning starts in order to catch the "before" state, and leave the script running for some time after the load generation ends to see the long-term behavior of the watched resources:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;PERIOD="${1:-30}" RESULTS=${2:-metrics-$(date "+%F_%T")} mkdir -p "$RESULTS" strip_unit(){ echo -n $1 | sed -e 's,\([0-9]\+\)m,\1,g' | sed -e 's,\([0-9]\+\)Mi,\1,g' | sed -e 's,\([0-9]\+\)%,\1,g' } # Nodes oc get nodes &gt; $RESULTS/nodes.yaml oc describe nodes &gt; $RESULTS/nodes.info node_info_file(){ readlink -m "$RESULTS/node-info.$1.csv" } node_line(){ node=$1 node_json="$(oc get node $node -o json)" echo -n "$(echo "$node_json" | jq -rc '.status.conditions[] | select(.type=="MemoryPressure").status');" echo -n "$(echo "$node_json" | jq -rc '.status.conditions[] | select(.type=="DiskPressure").status');" echo -n "$(echo "$node_json" | jq -rc '.status.conditions[] | select(.type=="PIDPressure").status');" echo -n "$(echo "$node_json" | jq -rc '.status.conditions[] | select(.type=="Ready").status');" node_info=($(oc adm top node $node --no-headers)) echo -n "$(strip_unit ${node_info[1]});" echo -n "$(strip_unit ${node_info[2]});" echo -n "$(strip_unit ${node_info[3]});" echo "$(strip_unit ${node_info[4]})" } NODES=($(oc get nodes -o json | jq -rc '.items[].metadata.name' | sort)) for node in "${NODES[@]}"; do echo "Time;MemoryPressure;DiskPressure;PIDPressure;Ready;CPU_millicores;CPU_percent;Memory_MiB;Memory_percent" &gt; $(node_info_file $node) done # Operator pods pod_info_file(){ readlink -m "$RESULTS/pod-info.$1.csv" } pod_line(){ pod=$1 ns=$2 pod_info=($(oc adm top pod $pod -n $ns --no-headers)) echo -n "$(strip_unit ${pod_info[1]});" echo "$(strip_unit ${pod_info[2]})" } for namespace in openshift-operators openshift-monitoring openshift-apiserver openshift-kube-apiserver openshift-sdn openshift-operator-lifecycle-manager service-binding-operator; do PODS=($(oc get pods -n $namespace -o json | jq -rc '.items[].metadata.name' | grep -E 'operator|prometheus|apiserver|sdn|ovs|olm|packageserver' | sort)) for pod in "${PODS[@]}"; do echo "Time;CPU_millicores;Memory_MiB" &gt; $(pod_info_file $pod) done done echo "Collecting metrics" # Periodical collection while true; do echo -n "." for namespace in openshift-operators openshift-monitoring openshift-apiserver openshift-kube-apiserver openshift-sdn openshift-operator-lifecycle-manager service-binding-operator; do PODS=($(oc get pods -n $namespace -o json | jq -rc '.items[].metadata.name' | grep -E 'operator|prometheus|apiserver|sdn|ovs|olm|packageserver' | sort)) for pod in "${PODS[@]}"; do pod_file=$(pod_info_file $pod) echo -n "$(date -u '+%F %T.%N');" &gt;&gt; $pod_file pod_line $pod $namespace &gt;&gt; $pod_file done done for node in ${NODES[@]}; do node_file=$(node_info_file $node) echo -n "$(date -u '+%F %T.%N');" &gt;&gt; $node_file node_line $node &gt;&gt; $node_file done sleep ${PERIOD}s done &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If the cluster survives the entire duration of the stress test, we can use Grafana to download the collected data from Prometheus in a form of similar CSV files.&lt;/p&gt; &lt;h2&gt;Compiling a performance report&lt;/h2&gt; &lt;p&gt;Since these tests were arranged pretty quickly and the metrics were collected in the raw form of CSV files, I had to manually convert the data into charts to put them into perspective. I used &lt;a href="https://www.google.com/sheets/about/"&gt;Google Sheets&lt;/a&gt; for that purpose.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;Finally, we have all the infrastructure we need for performance testing. In Part 4, we will take our first look at the actual metrics.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3" title="Troubleshooting application performance with Red Hat OpenShift metrics, Part 3: Collecting runtime metrics"&gt;Troubleshooting application performance with Red Hat OpenShift metrics, Part 3: Collecting runtime metrics&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ncoqSuhXI4A" height="1" width="1" alt=""/&gt;</summary><dc:creator>Pavel Macik</dc:creator><dc:date>2021-07-22T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/22/troubleshooting-application-performance-red-hat-openshift-metrics-part-3</feedburner:origLink></entry><entry><title type="html">Quarkus 2.0.3.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/XyM3cPhK_3w/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-0-3-final-released/</id><updated>2021-07-22T00:00:00Z</updated><content type="html">We just released Quarkus 2.0.3.Final with a new round of bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.0. If you are not using 2.0 already, please refer to the 2.0 migration guide. Full changelog You can get the full changelog of 2.0.3.Final on GitHub....&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/XyM3cPhK_3w" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-0-3-final-released/</feedburner:origLink></entry><entry><title>Red Hat JBoss Enterprise Application Platform 7.4 brings new developer and operations capabilities</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/yEw64ms3fXw/red-hat-jboss-enterprise-application-platform-74-brings-new-developer-and" /><author><name>James Falkner</name></author><id>21cdd3d4-e045-4d3a-9126-26349ea57f80</id><updated>2021-07-21T19:00:00Z</updated><published>2021-07-21T19:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/eap/overview"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) 7.4 is now in general availability (GA). JBoss EAP is an open source, &lt;a href="https://jakarta.ee/compatibility/"&gt;Jakarta Enterprise Edition (Jakarta EE) 8-compliant&lt;/a&gt; application server that enables organizations to deploy and manage enterprise &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; applications across hybrid IT environments, including bare-metal, virtualized, private, and public clouds. This release provides enhancements to operations on &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; as well as several new improvements in &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt;, management, and developer productivity.&lt;/p&gt; &lt;p&gt;This article covers what's new in the JBoss EAP 7.4 GA. With this release, Red Hat continues its commitment to Jakarta EE support and enabling developers to extend existing application investments as they transition to emerging architectures and programming paradigms that require a lightweight, highly modular, cloud-native platform.&lt;/p&gt; &lt;h2&gt;Self-signed certificates and runtime data&lt;/h2&gt; &lt;p&gt;This release features the ability to automatically generate self-signed certificates, which makes it easier for developers to verify the security capabilities of their applications before moving to production.&lt;/p&gt; &lt;p&gt;JBoss EAP 7.4 also publishes runtime metrics for managed executor services for developers using managed threads that want to observe the state of the service and threads it is managing. Developers using Jakarta Enterprise Beans (stateful session beans, stateless session beans, and singleton beans) can now access runtime data about their beans using the JBoss CLI. These beans can also be dynamically discovered over HTTP as part of this release. They can optionally use globally-defined compression on both clients (using the &lt;code&gt;default.compression&lt;/code&gt; property) and servers (using the &lt;code&gt;default-compression&lt;/code&gt; attribute in the server configuration).&lt;/p&gt; &lt;h2&gt;Elytron enhancements and TLS support&lt;/h2&gt; &lt;p&gt;Security is perhaps the most important consideration for any organization. The Elytron subsystem, &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.1/html/migration_guide/migrating_to_elytron"&gt;introduced&lt;/a&gt; in JBoss EAP 7.1, has been enhanced in this release with support for auto-updating of credentials, reducing the number of steps to add new credentials to existing stores. In addition, administrators can now use regular expressions to translate security role names (avoiding the creation and maintenance of custom translators).&lt;/p&gt; &lt;p&gt;JBoss EAP 7.4 also now supports Transport Layer Security (TLS) 1.3. It is disabled by default in this release, but easily enabled via the &lt;code&gt;cipher-suite-names&lt;/code&gt; attribute. For websites that use cookies, the Undertow subsystem now supports the use of &lt;a href="https://datatracker.ietf.org/doc/html/draft-ietf-httpbis-cookie-same-site-00"&gt;SameSite cookie&lt;/a&gt; handling via the &lt;code&gt;samesite-cookie&lt;/code&gt; handler declaration.&lt;/p&gt; &lt;p&gt;Finally, administrators can now use Git repositories to store and manage their server configuration, properties files, and deployments.&lt;/p&gt; &lt;p&gt;These are just a few of the many new security features of JBoss EAP 7.4. For more details, consult the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/"&gt;documentation&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Management capabilities&lt;/h2&gt; &lt;p&gt;JBoss EAP solves many developers' hardest challenges, in large part due to its very flexible configuration and management capabilities. In this new release, administrators can now use global directories to distribute shared libraries, saving many steps when deploying new libraries across multiple applications. To control configuration drift, administrators can now declare the configuration directory as read-only to prevent unintended updates, which is especially useful in immutable &lt;a href="https://developers.redhat.com/topics/containers/"&gt;container&lt;/a&gt; deployments.&lt;/p&gt; &lt;p&gt;Finally, the management console (and the JBoss command-line interface) gains the ability to add new role decoders to the Elytron subsystem. Using this decoder, remote clients making calls to JBoss EAP can be mapped to Elytron roles based on the source address or regular expression and subsequently used during authorization decisions.&lt;/p&gt; &lt;h2&gt;ActiveMQ Artemis messaging capabilities&lt;/h2&gt; &lt;p&gt;Developers using the &lt;a href="https://activemq.apache.org/components/artemis/"&gt;Apache ActiveMQ Artemis&lt;/a&gt; messaging capabilities included in JBoss EAP 7.4 can now pause message topics (in addition to queues). This allows messages to be received into the queue or topic but held for delivery until the topic or queue is resumed, which is useful for testing failure scenarios or other maintenance tasks. JBoss EAP 7.4 also features a configurable list of hosts to periodically ping to detect broker network isolation, improving the resiliency of messaging applications.&lt;/p&gt; &lt;h2&gt;Support for Red Hat OpenShift and Galleon feature packs&lt;/h2&gt; &lt;p&gt;JBoss EAP 7.4 container images are available via the &lt;a href="https://catalog.redhat.com/"&gt;Red Hat Ecosystem Catalog&lt;/a&gt;, which can be used to create and deploy JBoss EAP 7.4 applications on Red Hat OpenShift. With this new release, the Source-to-Image (S2I) builder images now support custom &lt;a href="https://developers.redhat.com/blog/2020/04/10/jboss-eap-7-3-brings-new-packaging-capabilities"&gt;Galleon&lt;/a&gt; feature pack configurations to specify the location of custom content and which feature packs to use when building custom images. A new Galleon layer, &lt;code&gt;web-passivation&lt;/code&gt;, is included to automatically include a local cache (based on &lt;a href="https://infinispan.org/"&gt;Infinispan&lt;/a&gt;) for data session handling.&lt;/p&gt; &lt;p&gt;Additionally, Red Hat OpenShift users can use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html/getting_started_with_jboss_eap_for_openshift_online/eap-operator-for-automating-application-deployment-on-openshift_default"&gt;JBoss EAP Operator&lt;/a&gt; to deploy and manage JBoss EAP applications using operator semantics.&lt;/p&gt; &lt;h2&gt;MicroProfile 4.0 support with JBoss EAP expansion pack&lt;/h2&gt; &lt;p&gt;With the &lt;a href="https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/"&gt;JBoss Enterprise Application Platform expansion pack&lt;/a&gt; (EAP XP) 3.0 release, developers can use &lt;a href="https://microprofile.io/"&gt;Eclipse MicroProfile&lt;/a&gt; APIs to build and deploy microservices-based applications. This new JBoss EAP XP release brings support for the MicroProfile 4.0 specification, including bulk configuration property support, the ability to define cross-invocation life cycles for circuit breakers and bulkheads, and support for adding tags to individual metrics, which enhances the observability of JBoss EAP XP applications. JBoss EAP XP 3 also lets you specify runtime configuration via the &lt;code&gt;--cli-script&lt;/code&gt; argument when launching applications, which runs a custom script at startup.&lt;/p&gt; &lt;p&gt;The new release also includes a developer preview of &lt;a href="https://github.com/eclipse/microprofile-reactive-messaging"&gt; MicroProfile Reactive Messaging&lt;/a&gt;, allowing developers to integrate with &lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt; Red Hat AMQ Streams&lt;/a&gt; and work as a message relayer to consume, process, and produce messages. Note that the MicroProfile APIs that were included in earlier versions of JBoss EAP 7 have been moved to EAP XP. To use these MicroProfile APIs, developers should use JBoss EAP XP (and be aware of its &lt;a href="https://access.redhat.com/support/policy/updates/jboss_eap_xp_notes"&gt;different support life cycle&lt;/a&gt;). The JBoss EAP XP 3 release is forthcoming.&lt;/p&gt; &lt;h2&gt;Migration tooling updates&lt;/h2&gt; &lt;p&gt;As with any new JBoss EAP release, the popular &lt;a href="https://developers.redhat.com/products/mta/overview"&gt;Migration Toolkit for Applications&lt;/a&gt; (based on the &lt;a href="https://github.com/windup"&gt;Windup project&lt;/a&gt;) gets new updates in its upcoming 5.2 release, including new rules for JBoss EAP 7.3 to 7.4 and JBoss EAP XP 2 to XP 3, a new IntelliJ IDEA extension, and assistance for &lt;a href="https://access.redhat.com/products/thorntail"&gt;Thorntail&lt;/a&gt; users looking to migrate to JBoss EAP XP.&lt;/p&gt; &lt;h2&gt;Supported platforms&lt;/h2&gt; &lt;p&gt;As always, the list of supported platforms continues to evolve. In this new 7.4 release, several platforms and features have been deprecated in favor of better alternatives. For the full list of new, changed, or deprecated platforms and features, please refer to the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/"&gt;release notes&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;JBoss EAP on Microsoft Azure&lt;/h2&gt; &lt;p&gt;Microsoft and Red Hat &lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-brings-jboss-enterprise-application-platform-microsoft-azure-easing-shift-cloud-traditional-java-applications"&gt;recently announced&lt;/a&gt; the availability of Red Hat JBoss Enterprise Application Platform as a native offering in Microsoft Azure, both for traditional VM-based deployments and a &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/application-platform/azure"&gt;managed offering through Azure App Service&lt;/a&gt;. Both of these offerings currently offer JBoss EAP 7.3, and work is underway to update these to use the new 7.4 release.&lt;/p&gt; &lt;h2&gt;Red Hat Runtimes&lt;/h2&gt; &lt;p&gt;JBoss EAP is included in &lt;a href="https://www.redhat.com/en/products/runtimes"&gt;Red Hat Runtimes&lt;/a&gt;, a Red Hat Application Services product that includes a set of cloud-native runtimes and capabilities such as the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Red Hat build of Quarkus&lt;/a&gt;, Red Hat support of &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring Boot&lt;/a&gt;, Red Hat build of &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, Red Hat's single sign-on technology, &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; broker, and &lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt;. These are all integrated and optimized for Red Hat OpenShift, offering a coherent hybrid cloud application platform on which they can optimize their existing Java applications while innovating with enterprise Java and non-Java &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt;, DevOps, &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt;, and advanced deployment techniques. Check out the &lt;a href="https://access.redhat.com/support/policy/updates/jboss_notes#p_eap"&gt;JBoss EAP support life cycle&lt;/a&gt; for details on support levels and dates.&lt;/p&gt; &lt;h2&gt;JBoss EAP resources&lt;/h2&gt; &lt;p&gt;You can find the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/"&gt;JBoss Enterprise Application Platform 7.4 documentation&lt;/a&gt;, including release notes, the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.4/html/getting_started_guide/"&gt;Getting Started Guide&lt;/a&gt;, and several other guides on &lt;a href="https://docs.redhat.com"&gt; docs.redhat.com&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/21/red-hat-jboss-enterprise-application-platform-74-brings-new-developer-and" title="Red Hat JBoss Enterprise Application Platform 7.4 brings new developer and operations capabilities"&gt;Red Hat JBoss Enterprise Application Platform 7.4 brings new developer and operations capabilities&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/yEw64ms3fXw" height="1" width="1" alt=""/&gt;</summary><dc:creator>James Falkner</dc:creator><dc:date>2021-07-21T19:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/21/red-hat-jboss-enterprise-application-platform-74-brings-new-developer-and</feedburner:origLink></entry><entry><title>Bootstrap GitOps with Red Hat OpenShift Pipelines and kam CLI</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/cjmU1pPrHqI/bootstrap-gitops-red-hat-openshift-pipelines-and-kam-cli" /><author><name>Ishita Sequeira, William Tam</name></author><id>3060b156-f201-4d00-816f-6cf5753afe71</id><updated>2021-07-21T07:00:00Z</updated><published>2021-07-21T07:00:00Z</published><summary type="html">&lt;p&gt;The GitOps Application Manager command-line interface (CLI), &lt;code&gt;kam&lt;/code&gt;, simplifies &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; adoption by bootstrapping Git repositories with opinionated layouts for &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous delivery&lt;/a&gt;. It also configures Argo CD to sync configurations across multiple &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; environments. In this article, we show you how to generate a GitOps repository using the &lt;code&gt;kam&lt;/code&gt; CLI to streamline your application delivery.&lt;/p&gt; &lt;h2&gt;A GitOps approach to application deployment&lt;/h2&gt; &lt;p&gt;Developers can adopt a GitOps approach to managing deployments into one or more environments. An &lt;em&gt;environment&lt;/em&gt; is a namespace in a Kubernetes cluster that is used to perform quality, &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt;, and performance checks. To ensure quality and expected behavior, teams deploy applications across various environments like development, testing, and staging, before running on the production cluster.&lt;/p&gt; &lt;p&gt;Currently, the &lt;code&gt;kam&lt;/code&gt; CLI bootstrap command supports three fixed environments: &lt;code&gt;dev&lt;/code&gt;, &lt;code&gt;stage&lt;/code&gt;, and &lt;code&gt;cicd&lt;/code&gt;. The &lt;code&gt;cicd&lt;/code&gt; environment is where we keep our &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton&lt;/a&gt; pipelines and tasks. The commands enable the user to deploy the application to only two environments: &lt;code&gt;dev&lt;/code&gt; and &lt;code&gt;stage&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;One or more applications can be deployed into a given environment. An &lt;em&gt;application&lt;/em&gt; is an aggregation of one or more services. The source code for each service (or &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservice&lt;/a&gt;) is contained within a single Git repository. To minimize the risk of change after deployment, whether intended or not, we must maintain a reproducible, reliable, and auditable deployment process. The ultimate value of GitOps lies in its ability to simplify the way you manage infrastructure and application deployment, control change, and gain visibility into your environments.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/redhat-developer/kam"&gt;&lt;code&gt;kam&lt;/code&gt; CLI&lt;/a&gt; generates a GitOps repository that contains the Kubernetes YAML configuration files required to deploy one or more applications into an environment. We use &lt;a href="https://github.com/kubernetes-sigs/kustomize"&gt;Kustomize&lt;/a&gt; to customize and configure resources based on their dependencies.&lt;/p&gt; &lt;p&gt;In this tutorial, we'll show you how to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Bootstrap a GitOps repository.&lt;/li&gt; &lt;li&gt;Create two services in an environment and deploy them in a single application.&lt;/li&gt; &lt;li&gt;Trigger the pipeline by making a small change in the service repository.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;To generate a GitOps repository using the &lt;code&gt;kam&lt;/code&gt; CLI, you need to have the &lt;a href="https://github.com/redhat-developer/kam/blob/4f3be956fadecec2367840d0079bf9be6d384483/docs/journey/day1/prerequisites/gitops_operator.md"&gt;OpenShift GitOps Operator&lt;/a&gt; and &lt;a href="https://github.com/redhat-developer/kam/blob/4f3be956fadecec2367840d0079bf9be6d384483/docs/journey/day1/prerequisites/pipelines_operator.md"&gt;OpenShift Pipelines Operator&lt;/a&gt; preinstalled on your OpenShift cluster. This tutorial also requires &lt;code&gt;kam&lt;/code&gt; version 0.0.36 or later. Download the &lt;code&gt;kam&lt;/code&gt; binary from &lt;a href="https://github.com/redhat-developer/kam/releases"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Create the GitOps repository with 'kam bootstrap'&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;kam bootstrap&lt;/code&gt; command generates the &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton and Kubernetes resources&lt;/a&gt; required to deploy an application across different environments. The command also generates event listeners that can trigger the pipelines on pull requests. You can apply these generated resources to a Kubernetes cluster as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kam bootstrap --service-repo-url https://github.com/ishitasequeira/taxi.git --gitops-repo-url https://github.com/ishitasequeira/gitops.git --image-repo quay.io/isequeir/image-repo --dockercfgjson ~/Downloads/isequeir-robot-auth.json --git-host-access-token $TOKEN &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let's break this down:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;--gitops-repo-url&lt;/code&gt; provides a Git repository containing the Kubernetes YAML configuration files required to deploy one or more applications into an environment.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--image-repo&lt;/code&gt; should specify either &lt;code&gt;registry/username/repository&lt;/code&gt; to use Red Hat Quay or Docker Hub, or &lt;code&gt;project/app&lt;/code&gt; to use the Red Hat OpenShift internal image registry.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--service-git-repo&lt;/code&gt; contains the service source code and the deployment path.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The command generates the repository layout and creates the required pipeline resources. The &lt;code&gt;cicd&lt;/code&gt; environment is also created within the layout. The &lt;code&gt;cicd&lt;/code&gt; environment is where we keep our &lt;a href="https://github.com/tektoncd/pipeline"&gt;Tekton&lt;/a&gt; pipelines and tasks. Two pipelines are created in the &lt;code&gt;cicd&lt;/code&gt; environment: one for the GitOps repository and another for the applications that can be deployed in various environments.&lt;/p&gt; &lt;p&gt;An unencrypted secret is generated into the &lt;code&gt;secrets&lt;/code&gt; folder, which is a sibling of the folder that contains the generated repository layout. Deploying this GitOps configuration without encrypting the secrets is insecure and is not recommended. One way to encrypt the secrets is by using the &lt;a href="https://github.com/bitnami-labs/sealed-secrets"&gt;Sealed Secrets&lt;/a&gt; tool from Bitnami Labs. To encrypt and apply the generated secrets, follow the instructions linked &lt;a href="https://github.com/redhat-developer/kam/blob/4f3be956fadecec2367840d0079bf9be6d384483/docs/journey/day1/README.md#sealed-secrets"&gt;here&lt;/a&gt;. Make sure to apply the secrets (encrypted or unencrypted) to the cluster every time a secret is generated (when the &lt;code&gt;bootstrap&lt;/code&gt; or &lt;code&gt;service add&lt;/code&gt; command is executed).&lt;/p&gt; &lt;p&gt;Create a new GitHub or GitLab project to host the GitOps repository (for example, &lt;code&gt;https://github.com/ishitasequeira/gitops.git&lt;/code&gt;). Push the generated directory structure to the repository using the following commands (the directory structure is created in the current working directory):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git init . $ git add . $ git commit -m "Initial Commit" $ git remote add origin &lt;insert gitops repo&gt; $ git push -u origin main&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This initializes the GitOps repository and starts your journey to deploying applications via Git.&lt;/p&gt; &lt;p&gt;We'll now bring up our deployment infrastructure. You only need to do this once at the beginning; the configuration will be self-hosted after that:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc apply -k config/argocd/&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Add services to the GitOps repository&lt;/h2&gt; &lt;p&gt;For this demo, we will use a &lt;a href="https://github.com/ishitasequeira/frontend-app"&gt;front-end service&lt;/a&gt; that deploys an Angular web application and a &lt;a href="https://github.com/ishitasequeira/backend-service"&gt;back-end service&lt;/a&gt; that deploys a &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; REST API. The front-end service tries to connect to the back-end service via the REST APIs to fetch or update the data.&lt;/p&gt; &lt;p&gt;Let's add the services to our GitOps repository using the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kam service add --git-repo-url https://github.com/ishitasequeira/frontend-service.git --app-name full-stack --env-name stage --image-repo quay.io/isequeir/frontend-service --service-name frontend-service $ kam service add --git-repo-url https://github.com/ishitasequeira/backend-service.git --app-name full-stack --env-name stage --image-repo quay.io/isequeir/backend-service --service-name backend-service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To break it down:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;--git-repo-url&lt;/code&gt; contains the service source code and the deployment path.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--app-name&lt;/code&gt; is the name of the application folder that utilizes one or more services.&lt;/li&gt; &lt;li&gt;&lt;code&gt;--env-name&lt;/code&gt; is the environment to which the service is being deployed.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For each service, an unencrypted secret will be generated into the &lt;code&gt;secrets&lt;/code&gt; folder. Make sure to apply these secrets to the cluster. To encrypt this secret, follow the same steps &lt;a href="https://github.com/redhat-developer/kam/tree/master/docs/journey/day1#secrets"&gt;here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt; Next, push the newly created services to the GitOps repository and sync the applications on the Argo CD user interface (UI).&lt;/p&gt; &lt;p&gt;At this point, the applications in Argo CD should be synced and healthy. You may need to manually &lt;a href="https://github.com/argoproj/argo-cd/blob/master/docs/getting_started.md#7-sync-deploy-the-application"&gt;sync applications&lt;/a&gt; from the Argo CD web UI if any of the applications are out of sync.&lt;/p&gt; &lt;p&gt;You can log into the Argo CD instance from the application launcher in the OpenShift console, as shown in Figure 1.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%203.39.28%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%203.39.28%20PM.png?itok=4U5m50Ql" width="1440" height="809" alt="A screenshot of the Applications menu showing where to log into the Argo CD instance." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Select &lt;strong&gt;Cluster ArgoCD&lt;/strong&gt; from the application launcher to log in to the Argo CD instance.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For the precreated Argo CD instance under the &lt;code&gt;openshift-gitops&lt;/code&gt; project, you'll find the password to log into the Argo CD UI by navigating to &lt;strong&gt;Workloads → Secrets&lt;/strong&gt; and selecting &lt;code&gt;openshift-gitops-cluster&lt;/code&gt;, as shown in Figure 2.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-15%20at%2012.40.01%20AM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-15%20at%2012.40.01%20AM.png?itok=tLBAH9gW" width="1440" height="857" alt="A screenshot showing how to access the password to log into the Argo CD UI as described in the article." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Access the password to log into the Argo CD UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Copy the secret at the bottom of the &lt;strong&gt;Secrets details&lt;/strong&gt; page to your clipboard (see Figure 3) and paste it into the password field on the Argo CD login page. Use &lt;code&gt;admin&lt;/code&gt; as the username.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-15%20at%2012.43.35%20AM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-15%20at%2012.43.35%20AM.png?itok=_O4ihX8j" width="1440" height="853" alt="Screenshot of the Secret details page that shows where the user can copy the password." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Copying the password from the Secrets details page.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Alternatively, you can fetch this password via the command line by running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl get secret openshift-gitops-cluster -n openshift-gitops -ojsonpath='{.data.admin\.password}' | base64 -d&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see in Figure 4, the applications have been synced.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.02.51%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.02.51%20PM.png?itok=4Lt0lzVs" width="1440" height="814" alt="Screenshot showing the list of apps deployed in the Argo CD UI." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: The list of applications in the Argo CD UI.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;When you click the full-stack application (see Figure 5), there should be two services associated with it: the front-end service and the back-end service.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-15%20at%2012.49.40%20AM_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-15%20at%2012.49.40%20AM_0.png?itok=ou7W-PJ8" width="1440" height="756" alt="Screenshot of the full-stack application with the statuses Healthy and Synced." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Deploying the front-end and back-end services.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can launch the front-end service by navigating to &lt;strong&gt;stage project → Topology&lt;/strong&gt; and clicking the front-end service (see Figure 6).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.04.19%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.04.19%20PM.png?itok=Bt2E4Cg4" width="1440" height="758" alt="Accessing the front-end service from the Topology page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Topology view of services deployed in the project stage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Doing this creates a route, which will serve the default image. The route is automatically created based on the name of your application source repository. When you open the front-end service URL, you will see the web page, as shown in Figure 7.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.04.30%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.04.30%20PM.png?itok=A3nOFZ8D" width="1440" height="799" alt="Screenshot of the example web application home page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: The web application homepage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can access the list of products fetched from the back-end service using the REST API calls by clicking the &lt;strong&gt;Products&lt;/strong&gt; tab in the upper-right corner of the application (see Figure 8).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.04.43%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.04.43%20PM.png?itok=89LFZ8NY" width="1440" height="635" alt="Screenshot of the Product Inventory page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: Accessing the product database from the Products tab.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Add webhooks to the Git repository&lt;/h2&gt; &lt;p&gt;We need to add a webhook to the Git repository once the resources are applied. The webhook's payload URL is the &lt;code&gt;EventListener&lt;/code&gt; external address exposed by the OpenShift route. You can obtain the external address by displaying OpenShift routes in the &lt;code&gt;cicd&lt;/code&gt; namespace. Simply run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get route --namespace cicd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You also need a webhook for your Git hosting service. This is used to trigger continuous integration (CI) pipeline runs automatically on pushes to your repositories. The address (&lt;code&gt;host/port&lt;/code&gt;) retrieved from the command will be further added to the webhook secret of that repository. We can create webhooks for the added services and the GitOps repository.&lt;/p&gt; &lt;p&gt;For this demo, we will add a webhook to the front-end service:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kam webhook create --env-name stage --service-name frontend-service --git-host-access-token $TOKEN&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Testing the pipeline&lt;/h2&gt; &lt;p&gt;After you add the webhook secret, trigger a small change in your front-end service repository to see if &lt;code&gt;app-ci-pipeline&lt;/code&gt; is working. For this example, we changed the content and the color of the homepage navigation bar in a new branch. Doing this created a pull request. To determine if the &lt;code&gt;app-ci-pipeline&lt;/code&gt; has been triggered, go to the Developer view on the OpenShift web console. The &lt;strong&gt;Pipelines&lt;/strong&gt; tab will indicate whether the pipelines are triggered and completed successfully (see Figure 9). Upon successfully completing the pipeline, a new image gets pushed to the Red Hat Quay repository that we specified while adding the service.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.04.54%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.04.54%20PM.png?itok=mWbL9o9T" width="1440" height="630" alt="Checking the status on the Pipelines detail page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: Checking the &lt;code&gt;app-ci-pipeline&lt;/code&gt; status.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For the changes to be reflected on the deployed application, we need to update the image tag for the front-end service in the &lt;code&gt;config&lt;/code&gt; folder. Once the image tag has been updated, we'll push the changes to the GitOps repository and delete the previous front-end service deployment. Doing so automatically updates the deployment with a new image, as shown in Figure 10.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Screen%20Shot%202021-07-13%20at%204.05.04%20PM.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/Screen%20Shot%202021-07-13%20at%204.05.04%20PM.png?itok=lgfzihOT" width="1440" height="695" alt="Screenshot of the updated web application homepage." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: The updated web application homepage.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article provided an overview of the CI/CD experience using the &lt;code&gt;kam&lt;/code&gt; CLI. To learn more about the &lt;code&gt;kam&lt;/code&gt; CLI, visit the &lt;a href="https://github.com/redhat-developer/kam"&gt;GitHub project&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/21/bootstrap-gitops-red-hat-openshift-pipelines-and-kam-cli" title="Bootstrap GitOps with Red Hat OpenShift Pipelines and kam CLI"&gt;Bootstrap GitOps with Red Hat OpenShift Pipelines and kam CLI&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/cjmU1pPrHqI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ishita Sequeira, William Tam</dc:creator><dc:date>2021-07-21T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/21/bootstrap-gitops-red-hat-openshift-pipelines-and-kam-cli</feedburner:origLink></entry><entry><title type="html">How to use Long Running Actions between microservices</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/6KP9A9oleNg/how-to-use-long-running-actions-between.html" /><author><name>Michael Musgrove</name></author><id>https://jbossts.blogspot.com/2021/07/how-to-use-long-running-actions-between.html</id><updated>2021-07-20T20:31:00Z</updated><content type="html">INTRODUCTION In my I showed how to run a Long Running Action (LRA) within a single JAX-RS resource method using quarkus features to build and run the application. I showed how to create and start an LRA coordinator and then generated a basic hello application, showing how to modify the application to run with a long running action (by adding dependencies on the org.eclipse.microprofile.lra:microprofile-lra-api and org.jboss.narayana.rts:narayana-lra artifacts, which together provide annotations for controlling the lifecycle of LRAs). That post also includes links to the and to the . In this follow up post I will indicate how to include a second resource in the LRA. To keep things interesting I’ll deploy the second resource to another microservice and use quarkus’s MicroProfile Rest Client support to implement the remote service invocations. The main difference between this example and the one I developed in the earlier post, apart from the technicalities of using Rest Client, is that we will set the attribute to false in the remote service so that the LRA will remain active when the call returns. In this way the initiating service method has the option of calling other microservices before ending the LRA. CREATING AND STARTING AN LRA COORDINATOR LRA relies on a coordinator to manage the lifecycle of LRAs so you will need one to be running for this demo to work successfully. The showed how to build and run coordinators. Alternatively, which execute all of the steps required in the current post and it includes a shell script called which will build a runnable coordinator jar (it’s fairly simple and short so you can just read it and create your own jar or just run it as is). GENERATE A PROJECT FOR BOOKING TICKETS Since the example will be REST based, include the resteasy and rest-client extensions (on line 6 next): 1: mvn io.quarkus:quarkus-maven-plugin:2.0.1.Final:create \ 2: -DprojectGroupId=org.acme \ 3: -DprojectArtifactId=ticket \ 4: -DclassName="org.acme.ticket.TicketResource" \ 5: -Dpath="/tickets" \ 6: -Dextensions="resteasy,rest-client" 7: cd ticket You will need the mvn program to run the plugin (but the generated projects will include the mvnw maven wrapper). Modify the generated TicketResource.java source file to add Microprofile LRA support. The changes that you will need for LRA are on lines 26 and 27. Line 26 says that the bookTicket method must run with an LRA (if one is not present when the method is invoked then one will be automatically created). Note that we have set the end attribute to false to stop the LRA from being automatically closed when the method finishes. By keeping the LRA active when the ticket is booked, the caller can invoke other services in the context of the same LRA. Most services will require the LRA context for tracking updates which typically will be useful for knowing which actions to compensate for if the LRA is later cancelled: the context is injected as a JAX-RS method parameter on line 27. You will also need to include callbacks for when the LRA is later closed or cancelled (the methods are defined on lines 37 and line 46, respectively). 1: package org.acme.ticket; 2: 3: import static javax.ws.rs.core.MediaType.APPLICATION_JSON; 4: 5: // import annotation definitions 6: import org.eclipse.microprofile.lra.annotation.ws.rs.LRA; 7: import org.eclipse.microprofile.lra.annotation.Compensate; 8: import org.eclipse.microprofile.lra.annotation.Complete; 9: // import the definition of the LRA context header 10: import static org.eclipse.microprofile.lra.annotation.ws.rs.LRA.LRA_HTTP_CONTEXT_HEADER; 11: 12: // import some JAX-RS types 13: import javax.ws.rs.GET; 14: import javax.ws.rs.PUT; 15: import javax.ws.rs.Path; 16: import javax.ws.rs.Produces; 17: import javax.ws.rs.core.Response; 18: import javax.ws.rs.HeaderParam; 19: 20: @Path("/tickets") 21: @Produces(APPLICATION_JSON) 22: public class TicketResource { 23: 24: @GET 25: @Path("/book") 26: @LRA(value = LRA.Type.REQUIRED, end = false) // an LRA will be started before method execution if none exists and will not be ended after method execution 27: public Response bookTicket(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 28: System.out.printf("TicketResource.bookTicket: %s%n", lraId); 29: String ticket = "1234" 30: return Response.ok(ticket).build(); 31: } 32: 33: // ask to be notified if the LRA closes: 34: @PUT // must be PUT 35: @Path("/complete") 36: @Complete 37: public Response completeWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 38: System.out.printf("TicketResource.completeWork: %s%n", lraId); 39: return Response.ok().build(); 40: } 41: 42: // ask to be notified if the LRA cancels: 43: @PUT // must be PUT 44: @Path("/compensate") 45: @Compensate 46: public Response compensateWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 47: System.out.printf("TicketResource.compensateWork: %s%n", lraId); 48: return Response.ok().build(); 49: } 50: } Skip the tests: rm src/test/java/org/acme/ticket/* Add dependencies on microprofile-lra-api and narayana-lra to the pom to include the MicroProfile LRA annotations and the narayana implementation of them so that the LRA context will be propagated during interservice communications: &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.microprofile.lra&lt;/groupId&gt; &lt;artifactId&gt;microprofile-lra-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.narayana.rts&lt;/groupId&gt; &lt;artifactId&gt;narayana-lra&lt;\/artifactId&gt; &lt;version&gt;5.12.0.Final&lt;/version&gt; &lt;/dependency&gt; We are creating ticket and trip microservices so they need to listen on different ports, configure the ticket service to run on port 8081: 1: quarkus.arc.exclude-types=io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantRegistry,io.narayana.lra.filter.ServerLRAFilter,io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantResource 2: quarkus.http.port=8081 3: quarkus.http.test-port=8081 The excludes are pulled in by the org.jboss.narayana.rts:narayana-lra maven dependency. As mentioned in my previous post this step will not be necessary when the pull request for the io.quarkus:quarkus-narayana-lra extension is approved. Now build and test the ticket service, making sure that you have already started a coordinator as described in the previous blog (or you can use the shell scripts ): ./mvnw clean package -DskipTests # skip tests java -jar target/quarkus-app/quarkus-run.jar &amp;amp; # run the application in the background curl http://localhost:8081/tickets/book TicketResource.bookTicket: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_2 1234 The bookTicket() method prints the method name and the id of the active LRA followed by the hard-coded booking id 1234. GENERATE A PROJECT FOR BOOKING TRIPS Now create a second microservice which will be used for booking trips. It will invoke other microservices to complete trip bookings. In order to simplify the example there is just the single remote ticket service involved in the booking process. First generate the project. Like the ticket service, the example will be REST based so include the resteasy and rest-client extensions: mvn io.quarkus:quarkus-maven-plugin:2.0.1.Final:create \ -DprojectGroupId=org.acme \ -DprojectArtifactId=trip \ -DclassName="org.acme.trip.TripResource" \ -Dpath="/trips" \ -Dextensions="resteasy,rest-client" cd trip The rest-client extension includes support for MicroProfile REST Client which we shall use to perform the remote REST invocations from the trip to the ticket service. For REST Client we need a TicketService and we need to register it as shown on line 12 of the following listing: 1: package org.acme.trip; 2: 3: import org.eclipse.microprofile.rest.client.inject.RegisterRestClient; 4: 5: import javax.ws.rs.GET; 6: import javax.ws.rs.Path; 7: import javax.ws.rs.Produces; 8: import javax.ws.rs.core.MediaType; 9: 10: @Path("/tickets") 11: @Produces(MediaType.APPLICATION_JSON) 12: @RegisterRestClient 13: public interface TicketService { 14: 15: @GET 16: @Path("/book") 17: String bookTicket(); 18: } Let’s also create a TripService and inject an instance of the TicketService into it, marking it with the @RestClient annotation on line 11. The quarkus rest client support will configure this injected instance such that it will perform remote REST calls to the ticket service (the remote endpoint for the ticket service will be configured below in the application.properties file): 1: package org.acme.trip; 2: 3: import org.eclipse.microprofile.rest.client.inject.RestClient; 4: import javax.enterprise.context.ApplicationScoped; 5: import javax.inject.Inject; 6: 7: @ApplicationScoped 8: public class TripService { 9: 10: @Inject 11: @RestClient 12: TicketService ticketService; 13: 14: String bookTrip() { 15: return ticketService.bookTicket(); // only one service will be used for the trip booking 16: 17: // if other services need to be part of the trip they would be called here 18: // and the TripService would associate each step of the booking with the id of the LRA 19: // (although I've not shown it being passed in this example) and that would form the 20: // basis of the ability to compensate or clean up depending upon the outcome. 21: // We may include a more comprehensive/realistic example in a later blog. 22: } 23: } And now we can inject an instance of this service into the generated TripResource (src/main/java/org/acme/trip/TripResource.java) on line 26. I have also annotated the bookTrip() method with an LRA annotation so that a new LRA will be started before the method is started (if one wasn’t already present) and I have added @Complete and @Compensate callback methods (these will be called when the LRA closes or cancels, respectively): 1: package org.acme.trip; 2: 3: import javax.inject.Inject; 4: import javax.ws.rs.GET; 5: import javax.ws.rs.Path; 6: import javax.ws.rs.Produces; 7: import javax.ws.rs.core.Response; 8: 9: import static javax.ws.rs.core.MediaType.APPLICATION_JSON; 10: 11: // import annotation definitions 12: import org.eclipse.microprofile.lra.annotation.ws.rs.LRA; 13: import org.eclipse.microprofile.lra.annotation.Compensate; 14: import org.eclipse.microprofile.lra.annotation.Complete; 15: // import the definition of the LRA context header 16: import static org.eclipse.microprofile.lra.annotation.ws.rs.LRA.LRA_HTTP_CONTEXT_HEADER; 17: 18: // import some JAX-RS types 19: import javax.ws.rs.PUT; 20: import javax.ws.rs.HeaderParam; 21: 22: @Path("/trips") 23: @Produces(APPLICATION_JSON) 24: public class TripResource { 25: 26: @Inject 27: TripService service; 28: 29: // annotate the hello method so that it will run in an LRA: 30: @GET 31: @LRA(LRA.Type.REQUIRED) // an LRA will be started before method execution and ended after method execution 32: @Path("/book") 33: public Response bookTrip(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 34: System.out.printf("TripResource.bookTrip: %s%n", lraId); 35: String ticket = service.bookTrip(); 36: return Response.ok(ticket).build(); 37: } 38: 39: // ask to be notified if the LRA closes: 40: @PUT // must be PUT 41: @Path("/complete") 42: @Complete 43: public Response completeWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 44: System.out.printf("TripResource.completeWork: %s%n", lraId); 45: return Response.ok().build(); 46: } 47: 48: // ask to be notified if the LRA cancels: 49: @PUT // must be PUT 50: @Path("/compensate") 51: @Compensate 52: public Response compensateWork(@HeaderParam(LRA_HTTP_CONTEXT_HEADER) String lraId) { 53: System.out.printf("TripResource.compensateWork: %s%n", lraId); 54: return Response.ok().build(); 55: } 56: } For the blog we can skip the tests: rm src/test/java/org/acme/trip/* Configure the trip service to listen on port 8082 (line 2). Also configure the remote ticket endpoint as required by the MicroProfile REST Client specification (line 5): 1: quarkus.arc.exclude-types=io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantRegistry,io.narayana.lra.filter.ServerLRAFilter,io.narayana.lra.client.internal.proxy.nonjaxrs.LRAParticipantResource 2: quarkus.http.port=8082 3: quarkus.http.test-port=8082 4: 5: org.acme.trip.TicketService/mp-rest/url=http://localhost:8081 6: org.acme.trip.TicketService/mp-rest/scope=javax.inject.Singleton Add dependencies on microprofile-lra-api and narayana-lra to the pom to include the MicroProfile LRA annotations and the narayana implementation of them so that the application can request that the LRA context be propagated during interservice communications: &lt;dependency&gt; &lt;groupId&gt;org.eclipse.microprofile.lra&lt;/groupId&gt; &lt;artifactId&gt;microprofile-lra-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.jboss.narayana.rts&lt;/groupId&gt; &lt;artifactId&gt;narayana-lra&lt;/artifactId&gt; &lt;version&gt;5.12.0.Final&lt;/version&gt; &lt;/dependency&gt; and finally, build and run the microservice: ./mvnw clean package -DskipTests java -jar target/quarkus-app/quarkus-run.jar &amp;amp; Use curl to book a trip. The HTTP GET request to the trips/book endpoint is handled by the trip service bookTrip() method and it then invokes the ticket service to book a ticket. When the bookTrip() method finishes the LRA will be closed (since the default value for the LRA.end attribute is true), triggering calls to the service @Complete methods of the two services: curl http://localhost:8082/trips/book TripResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TripResource.completeWork: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.bookTrip: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c TicketResource.completeWork: http://localhost:8080/lra-coordinator/0_ffffc0a8000e_8b2b_60f6a8d4_52c 1234 Notice the output shows the bookTrip and bookTicket methods being called and also notice that the @Complete methods of both services (completeWork()) were called. The id of the LRA on all calls should be the same value as shown in the example output, this is worthwhile noting since the completion and compensation methods will typically use it in order to determine which actions it should clean up for or compensate for when the LRA closes or cancels. Not shown here, but if there was a problem booking the ticket then the ticket service should return a JAX-RS status code (4xx and 5xx HTTP codes by default) that triggers the cancellation of the LRA, and this would then cause the @Compensate methods of all services involved in the LRA to be invoked. -------------------------------------------------------------------------------- Last updated 2021-07-20 21:25:50 BST&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/6KP9A9oleNg" height="1" width="1" alt=""/&gt;</content><dc:creator>Michael Musgrove</dc:creator><feedburner:origLink>https://jbossts.blogspot.com/2021/07/how-to-use-long-running-actions-between.html</feedburner:origLink></entry><entry><title type="html">Error handling in Kogito</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JoZkCTo7ztw/error-handling-in-kogito.html" /><author><name>Tiago Dolphine</name></author><id>https://blog.kie.org/2021/07/error-handling-in-kogito.html</id><updated>2021-07-20T16:33:30Z</updated><content type="html">WHAT ARE ERRORS WITHIN PROCESSES? First, we need to define the most usual type of errors, they can be divided into two main categories: technical and business errors. Technical Errors could be defined as general errors that are not explicitly handled by the process, for instance, a database communication or constraint issue, a network connection that is down, or even an external service that is not responding. These errors are hidden and harmless for the process itself, which means the system and internal logic might have capabilities to handle them, like retries upon certain failures, or a more critical technical error may cause the transaction to fail, rolling back changes and keeping the process in a consistent state. Business Errors, on the other hand, are related to the domain of a given process and are handled in the process declaration with a specific business meaning, that triggers an alternative flow to properly handle that situation, for instance, a credit limit exceeded, an item out of stock or even fraud that was detected in the payment process. In this post, let’s focus on the business errors and the error events supported in Kogito after the ERROR EVENTS Error events are events that are triggered by a defined known error. In BPMN, an error is meant for business errors and allows us to explicitly model errors with an alternative flow for them; they are represented by a flash symbol. START ERROR EVENT A start error event indicates the point to start a flow to handle a situation where an error has happened, and can only be used to trigger an Event Sub-Process, it could not be used to start a new process instance, for example. The Event subprocess is the way to declare the error handling flow and has access to the parent process or subprocess variables. Process using Start and End Error Events with Event Sub-process END ERROR EVENT The end error events are used to stop the process execution flow throwing an error when it is reached. This error can be caught by an error boundary event or triggering a start event that matches the error by its error code. In case there is no matching, the execution is stopped and the process follows to an error state (semantics defaults to the none end event semantics). BOUNDARY ERROR EVENT A boundary error event is placed in the boundary of an activity, for instance, an embedded subprocess or a service task, and indicates a specific error defined by the error code, can be caught within the scope of the activity in which it is hooked to. When an end error event is achieved, its referred error is thrown and it will be propagated upwards to its parent until the point an error boundary event is found, as already mentioned, where it matches the given error code from the thrown error. The activity where a boundary error is defined and all the nested ones, like any other subprocess, are stopped. The process continues from the sequence flow attached to the boundary error event. Another point of observation might be that an event subprocess if added to an embedded subprocess, could become an alternative to the use of a boundary event. Process using a Boundary Error Event EXCEPTIONS AS ERRORS In Kogito it is provided a way to use Java Exceptions to be mapped to Errors, to use this feature, the FQN of the exception class should be used as the error code in the process, but keep in mind this is just an alternative in the case where the error could not be properly thrown as an Error Event in the process. The exception mapped to error works in some specific locations like in a Service Task implementation, in a custom work item handler, or even in a Script Task.  In general, this feature should only be used in some edge cases where you need to turn a technical error (exception in your code) into a business error, that is not the standard behavior, but it could be useful in some situations and in this case the process takes control of the alternative flow when this error happens. Setting the error code as the Java Exception FQN Seat reservation process using exception as an error in a service task RETRY USING ERROR EVENTS When designing a process it might be useful to explicitly declare a retry logic when some unexpected situation occurs, this is useful for long-running actions, and it is not recommended for short-lived ones that in general are technical errors like an HTTP request timeout, that could be handled by the HTTP client library instead of the process itself.  Error events could be an alternative to do retries inside the process flow. For example, we could consider a process as follows, where we retry to execute an action, declared in a sub-process, upon a condition where a certain error event is thrown,  in this situation the error is caught and a timer could be used to wait for some time before executing the same operation again. Retry process example CONCLUSIONS The business error handling in the process has advantages compared to the exception handling in the code itself, where we could consider technical errors. So this clear separation of errors types is important to simplify the complexity of a system. It is possible to see and easily monitor what happens in the process when unexpected situations happen, it allows us to easily measure unexpected situations and it makes the process easy to evolve and to be maintained like changing an alternative flow to another flow upon an error. Error handling capability provided by Kogito is important to be considered when designing processes and it brings many benefits, that’s why creating the business logic to handle business errors and isolating them from internal technical errors should be taken into account in the whole system architecture. SHOW ME THE CODE! In the next blog post, let’s use the Error Events that were covered here in concrete examples where you be able to run and test. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JoZkCTo7ztw" height="1" width="1" alt=""/&gt;</content><dc:creator>Tiago Dolphine</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/error-handling-in-kogito.html</feedburner:origLink></entry><entry><title type="html">Integrating Red Hat Process Automation Manager and Red Hat AMQ Streams on OpenShift in 4 steps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/kne3WIvD6EM/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/07/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html</id><updated>2021-07-20T13:44:25Z</updated><content type="html">An event-driven architecture is a model that allows communication between services in a decoupled fashion. This pattern has evolved into a powerful software paradigm and covers a wide array of use cases. Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. With this industry evolution, we can now create event-driven business processes which can work seamlessly in a microservices-based environment. With the latest release of Red Hat Process Automation Manager (7.11), you can work with business processes capable of interacting with external services via events, either by emitting or consuming them. Earlier this year,  wrote a detailed  about the integration between Red Hat Process Automation Manager and Kafka. In this article, we will look at the integration of Red Hat Process Automation with Red Hat AMQ Streams on OpenShift. Red Hat AMQ Streams is an enterprise grade Kubernetes-native Kafka solution. In IT today, it can be both challenging and time-consuming for operations and development teams to be experts in many different technologies knowing how to use them, while also knowing how to install, configure, and maintain them. Kubernetes operators help streamline the installation, configuration, and maintenance complexities. We will be using the AMQ Streams Operator and the Business Automation Operator to help simplify the process. In this article you’ll see how to deliver and test an event-driven process application on OpenShift in four steps: 1. Kafka deployment on OpenShift 2. Red Hat Process Automation Manager deployment on OpenShift 3. Creation and deployment of the business application 4. Test of the business application using events To learn more check with a detailed step-by-step guide. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/kne3WIvD6EM" height="1" width="1" alt=""/&gt;</content><dc:creator>Sadhana Nandakumar</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/integrating-red-hat-process-automation-manager-and-red-hat-amq-streams-on-openshift-in-4-steps.html</feedburner:origLink></entry><entry><title>Deploy Node.js applications to Red Hat OpenShift with Helm</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/EN_yHQH-Iz8/deploy-nodejs-applications-red-hat-openshift-helm" /><author><name>Ash Cripps</name></author><id>20cbf5cf-69e0-4305-9cd5-16cb7dfb1e46</id><updated>2021-07-20T07:00:00Z</updated><published>2021-07-20T07:00:00Z</published><summary type="html">&lt;p&gt;There are many different ways to deploy your &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; applications to &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. This article shows you how to deploy a Node.js application using Helm, along with some recent additions to OpenShift.&lt;/p&gt; &lt;h2&gt;What is Helm?&lt;/h2&gt; &lt;p&gt;&lt;a&gt;Helm&lt;/a&gt; is a package manager for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, which you can use to define, install, and upgrade all types of Kubernetes applications. You can think of Helm as an operating system packager (such as &lt;code&gt;apt&lt;/code&gt; or &lt;strong&gt;yum&lt;/strong&gt;) but for Kubernetes. With Helm, you package your Kubernetes application into a &lt;em&gt;chart&lt;/em&gt;, which is a series of files that define the Kubernetes resources for your deployment. You can use Helm for a variety of scenarios—from very simple applications to complex ones with many dependencies.&lt;/p&gt; &lt;p&gt;Helm offers a fast and effective way for you and your customers to automate Node.js application deployments. Helm also supports Go, which allows for greater chart customization depending on user-specified values. (You can turn certain features on or off depending on the values.) For more information, see the &lt;a href="https://helm.sh/docs/"&gt;Helm documentation&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can use Helm to deploy applications to any Kubernetes environment through the command line. It’s often as easy as &lt;code&gt;helm install XYZ&lt;/code&gt;. However, in OpenShift we’ve worked to make it even easier. There are now two ways to deploy applications with Helm using the OpenShift user interface (UI).&lt;/p&gt; &lt;p&gt;We'll start with a Helm chart template that was recently made available on OpenShift. You can use the template to deploy your Node.js application to OpenShift via Helm as a starter, and then customize it to create your own Helm chart. While you can also use this template to deploy to Kubernetes, it includes OpenShift extensions that make deployments easier in that environment.&lt;/p&gt; &lt;p&gt;In the next sections, I will show you how to use the Helm chart template to deploy a Node.js application to OpenShift with just a few clicks. After that, we'll talk through the chart's implementation, and I'll show you how to package up your own Helm chart and add it to the OpenShift developer catalog.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can also add your own Helm charts to OpenShift by using a custom resource (CR) to &lt;a href="https://docs.openshift.com/container-platform/4.6/cli_reference/helm_cli/configuring-custom-helm-chart-repositories.html"&gt;create a new Helm chart repository&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Setting up the demonstration&lt;/h2&gt; &lt;p&gt;To follow this demonstration, you will need access to an OpenShift cluster. I'm using &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt;, which allows me to run a single-node OpenShift cluster locally. It doesn’t have all the features of an OpenShift cluster, but it has everything we need for this article. Alternatively, You could use the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which requires a Red Hat account.&lt;/p&gt; &lt;p&gt;You will also need a Node.js application that can be containerized. If you don't have one, you can use the sample program &lt;a href="https://github.com/nodeshift-starters/nodejs-rest-http"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Deploy a simple application with Helm on OpenShift&lt;/h2&gt; &lt;p&gt;The Helm chart that I added allows you to easily deploy a simple application with Helm through the OpenShift user interface, without having to write your own chart. Let's go through the steps together.&lt;/p&gt; &lt;h3&gt;Step 1: Select Helm Chart from the project's topology view&lt;/h3&gt; &lt;p&gt;First, you want to be inside the OpenShift console's developer view. From there, click on the topology view in the left-hand menu. You will be presented with the developer catalog, featuring a variety of deployment options. Go ahead and click on Helm Chart, as shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-1.png?itok=ztMJgYH2" width="600" height="316" alt="The OpenShift developer catalog with the Helm Charts option highlighted." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Select Helm Chart as your deployment option. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 2: Select the Node.js chart&lt;/h3&gt; &lt;p&gt;From within the Helm Chart section of the developer catalog, select the Node.js chart, which is highlighted in Figure 2. Then, click &lt;strong&gt;Install Helm Chart&lt;/strong&gt;.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-2.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-2.png?itok=vAU5HzGH" width="600" height="325" alt="The Helm Charts section of the developer catalog with the Node.js option highlighted." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Select the Node.js Helm chart from the developer catalog. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 3: Configure the Helm release&lt;/h3&gt; &lt;p&gt;Now, you can configure the values that will be implanted in your Helm release. OpenShift gives you two ways to input values, using either the user-friendly form view or the YAML view. For the purpose of this demonstration, we'll use the form view.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Not all of the available values are presented in the form view, so the YAML view gives you more control.&lt;/p&gt; &lt;p&gt;In the form shown in Figure 3, I name my release &lt;code&gt;nodejs-example&lt;/code&gt;, set my image name as &lt;code&gt;my-node-image&lt;/code&gt;, and input the URL of my source code’s Git repository. For this example, I'm using the &lt;code&gt;nodejs-http-rest&lt;/code&gt; example from &lt;code&gt;nodeshift-starters&lt;/code&gt;, but feel free to use your own repository if you wish.&lt;/p&gt; &lt;p&gt;I'll leave the rest of the options set to default (blank) for now, but you can change them. For example, you might want to pull from a different Node source image for the &lt;a href="https://docs.openshift.com/container-platform/4.7/openshift_images/using_images/using-s21-images.html"&gt;S2I&lt;/a&gt; builder. Figure 3 shows my completed form.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-3.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-3.png?itok=I9FbUWAF" width="600" height="326" alt="The completed Helm Chart form." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Example of how to fill out the Node.js Helm chart form. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Step 4: Install and track the build&lt;/h3&gt; &lt;p&gt;Once you have filled in the required values, go ahead and click &lt;strong&gt;Install&lt;/strong&gt;. At first, you will notice the pod within your deployment is reporting &lt;code&gt;ErrImagePull&lt;/code&gt;. This is &lt;em&gt;normal&lt;/em&gt;. The pod is unable to pull down your image because it hasn't been built yet! You can track your image's progress under the &lt;strong&gt;Builds&lt;/strong&gt; section of your deployments submenu, as shown in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rh-helm-4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rh-helm-4.png?itok=bPujM4w2" width="600" height="312" alt="Pods spinning up from the Helm chart." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The OpenShift topology overview, showing the pods spinning up from the Helm chart deployment. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Congratulations—you've just deployed your Node.js application to OpenShift via Helm! To view your application, click the &lt;strong&gt;open url&lt;/strong&gt; button in the top right corner of the deployment in the topology view.&lt;/p&gt; &lt;h2&gt;Customize your Helm chart&lt;/h2&gt; &lt;p&gt;For this section, I assume you already have a basic understanding of how Helm charts are structured, so we won't dig deep into the base files. Instead, we will explore the OpenShift-specific files you wouldn't normally find in a standard &lt;a href="https://helm.sh/docs/chart_template_guide/getting_started"&gt;Helm chart&lt;/a&gt;. We'll explore the following files, which you can use to develop a custom Helm chart:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;values.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;buildconfig.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;imagestream.yaml&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;route.yaml&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Note that the &lt;code&gt;buildconfig.yaml&lt;/code&gt;, &lt;code&gt;imagestream.yaml&lt;/code&gt;, and &lt;code&gt;route.yaml&lt;/code&gt; files are all specific to OpenShift.&lt;/p&gt; &lt;h3&gt;values.yaml&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;values.yaml&lt;/code&gt; file is very important as it is the one your user will interact with, either directly or through the UI. So, you need to make sure it is easy to follow and that you have enough values to cover all the customization use cases you wish your user to have.&lt;/p&gt; &lt;h3&gt;buildconfig.yaml&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;buildconfig.yaml&lt;/code&gt; is the first OpenShift-specific file in the Helm chart. Let's have a look at the file for the sample Node.js chart:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;{{- if .Values.build.enabled }} apiVersion: build.openshift.io/v1 kind: BuildConfig metadata: name: {{ include "nodejs.name" . }} labels: {{- include "nodejs.labels" . | nindent 4 }} spec: source: type: Git git: uri: {{ .Values.build.uri }} ref: {{ .Values.build.ref }} {{- if .Values.build.contextDir }} contextDir: {{ .Values.build.contextDir }} {{- end }} strategy: type: Source sourceStrategy: from: kind: ImageStreamTag namespace: {{ .Values.build.source.namespace }} name: {{ .Values.build.source.name }} {{- if .Values.build.pullSecret }} pullSecret: name: {{ .Values.build.pullSecret }} {{- end }} {{- if .Values.build.env }} env: {{- tpl (toYaml .Values.build.env) . | nindent 8 }} {{- end }} output: to: kind: {{ .Values.build.output.kind }} name: {{ include "nodejs.imageName" . }} {{- if and (eq .Values.build.output.kind "DockerImage") .Values.build.output.pushSecret }} pushSecret: name: {{ .Values.build.output.pushSecret }} {{- end }} {{- if .Values.build.resources }} resources: {{- toYaml .Values.build.resources | nindent 4 }} {{- end }} triggers: - type: ConfigChange {{- end }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first thing you'll notice is &lt;code&gt;{{- if .Values.build.enabled }}&lt;/code&gt;, which is an &lt;code&gt;if&lt;/code&gt; conditional in Golang templating. It allows the user to specify if they wish to build the image themselves using this config or use an image already in the OpenShift instance. You can also use this option to turn the OpenShift-specific features on or off. This option is useful if you want to be able to deploy your Helm chart to a vanilla Kubernetes environment.&lt;/p&gt; &lt;p&gt;The next line to look at is &lt;code&gt;apiVersion: build.openshift.io/v1&lt;/code&gt;. This line specifies that the file is OpenShift-specific and uses the OpenShift API to build a deployable image.&lt;/p&gt; &lt;p&gt;The next key section is the &lt;code&gt;source&lt;/code&gt; section under &lt;code&gt;spec&lt;/code&gt;. This section, as the name hints, is where you specify the source of the program: What Git repo and which reference should be checked out.&lt;/p&gt; &lt;p&gt;Next up, we specify what &lt;a href="https://docs.openshift.com/container-platform/4.7/cicd/builds/build-strategies.html"&gt;strategy&lt;/a&gt; we will use to build the image. For this Helm chart, I used the source-to-image (S2I) build strategy but you could choose to use a Docker strategy or a custom build. Inside the &lt;code&gt;strategy&lt;/code&gt; block, I specify that I want to build from an &lt;code&gt;ImageStreamTag&lt;/code&gt; and then have variables for the namespace, name, and if the source has a pull secret or not. The user can also use the &lt;code&gt;strategy&lt;/code&gt; block to specify if they have environment variables for the build.&lt;/p&gt; &lt;p&gt;Finally, we see the &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;resources&lt;/code&gt; blocks. The &lt;code&gt;resources&lt;/code&gt; block is where the user can specify if they want to limit the resources (such as CPU and memory) to be available to the pod once it is built. The &lt;code&gt;output&lt;/code&gt; block is where the user specifies what type of output they would like (I defaulted to &lt;code&gt;ImageStreamTag&lt;/code&gt; for this example), the name of the output, and the push secret (if needed) to upload the image.&lt;/p&gt; &lt;h3&gt;imagestream.yaml&lt;/h3&gt; &lt;p&gt;Now, let's have a look at the &lt;code&gt;imagestream.yaml&lt;/code&gt; file, which is another OpenShift-specific file. It's a fairly simple file where you just specify the kind of image stream and its name and labels. For more information about image streams please refer to the &lt;a href="https://docs.openshift.com/container-platform/4.7/openshift_images/image-streams-manage.html"&gt;OpenShift documentation for image streams&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;route.yaml&lt;/h3&gt; &lt;p&gt;Lastly, let's take a look at the &lt;code&gt;route.yaml&lt;/code&gt; file. This file is used to set up the routes for your application within OpenShift. Here is where you will do things like establishing a TLS connection, and specifying ports and certificates. You can use this file to expose your application in OpenShift without having to mess around with port forwarding like you would in standard Kubernetes.&lt;/p&gt; &lt;h2&gt;Package and deploy the Helm chart&lt;/h2&gt; &lt;p&gt;Once you have your Helm chart completed, you'll need to package it into a .tar file and upload it to your repository of choice. You can then deploy it to OpenShift via a custom resource definition (CRD), like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOF | oc apply -f - apiVersion: helm.openshift.io/v1beta1 kind: HelmChartRepository metadata: name: $name spec: name: $name connectionConfig: url: https://raw.githubusercontent.com/$org/$repo/$ref EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The CRD will show up under &lt;strong&gt;Custom Resource Definitions&lt;/strong&gt; in the OpenShift administrator view, and the chart itself will show up in the OpenShift developer catalog. You will be able to select the chart from the catalog to deploy your application.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has been an introduction to deploying Node.js applications to OpenShift via Helm. You saw how to use an example Helm chart or build your own chart to deploy a Node.js application.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, check out the &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/20/deploy-nodejs-applications-red-hat-openshift-helm" title="Deploy Node.js applications to Red Hat OpenShift with Helm"&gt;Deploy Node.js applications to Red Hat OpenShift with Helm&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/EN_yHQH-Iz8" height="1" width="1" alt=""/&gt;</summary><dc:creator>Ash Cripps</dc:creator><dc:date>2021-07-20T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/20/deploy-nodejs-applications-red-hat-openshift-helm</feedburner:origLink></entry><entry><title type="html">Eclipse Vert.x 4.1.2 released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/jrlC_Sar5RE/eclipse-vert-x-4-1-2" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-1-2</id><updated>2021-07-20T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.1.2 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/jrlC_Sar5RE" height="1" width="1" alt=""/&gt;</content><dc:creator>Julien Viet</dc:creator><feedburner:origLink>https://vertx.io/blog/eclipse-vert-x-4-1-2</feedburner:origLink></entry></feed>
